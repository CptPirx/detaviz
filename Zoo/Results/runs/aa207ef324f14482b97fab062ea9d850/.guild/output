INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 37/2042 [00:00<00:05, 365.49it/s]Padding data:   4%|▍         | 77/2042 [00:00<00:05, 380.92it/s]Padding data:   6%|▌         | 117/2042 [00:00<00:04, 389.17it/s]Padding data:   8%|▊         | 156/2042 [00:00<00:05, 376.14it/s]Padding data:  10%|▉         | 202/2042 [00:00<00:04, 403.11it/s]Padding data:  12%|█▏        | 245/2042 [00:00<00:04, 410.55it/s]Padding data:  14%|█▍        | 287/2042 [00:00<00:04, 411.81it/s]Padding data:  16%|█▌        | 330/2042 [00:00<00:04, 415.21it/s]Padding data:  18%|█▊        | 373/2042 [00:00<00:03, 418.91it/s]Padding data:  20%|██        | 415/2042 [00:01<00:03, 418.02it/s]Padding data:  22%|██▏       | 457/2042 [00:01<00:03, 404.44it/s]Padding data:  25%|██▍       | 501/2042 [00:01<00:03, 413.15it/s]Padding data:  27%|██▋       | 547/2042 [00:01<00:03, 424.21it/s]Padding data:  29%|██▉       | 592/2042 [00:01<00:03, 431.67it/s]Padding data:  31%|███       | 636/2042 [00:01<00:03, 429.63it/s]Padding data:  33%|███▎      | 681/2042 [00:01<00:03, 434.21it/s]Padding data:  36%|███▌      | 725/2042 [00:01<00:03, 422.16it/s]Padding data:  38%|███▊      | 768/2042 [00:01<00:03, 421.61it/s]Padding data:  40%|███▉      | 811/2042 [00:01<00:02, 417.33it/s]Padding data:  42%|████▏     | 853/2042 [00:02<00:02, 413.58it/s]Padding data:  44%|████▍     | 895/2042 [00:02<00:02, 400.81it/s]Padding data:  46%|████▌     | 936/2042 [00:02<00:02, 393.92it/s]Padding data:  48%|████▊     | 976/2042 [00:02<00:02, 395.08it/s]Padding data:  50%|████▉     | 1016/2042 [00:02<00:02, 390.39it/s]Padding data:  52%|█████▏    | 1056/2042 [00:02<00:02, 386.63it/s]Padding data:  54%|█████▎    | 1097/2042 [00:02<00:02, 391.06it/s]Padding data:  56%|█████▌    | 1138/2042 [00:02<00:02, 396.18it/s]Padding data:  58%|█████▊    | 1178/2042 [00:02<00:02, 394.36it/s]Padding data:  60%|█████▉    | 1220/2042 [00:03<00:02, 399.70it/s]Padding data:  62%|██████▏   | 1263/2042 [00:03<00:01, 407.20it/s]Padding data:  64%|██████▍   | 1306/2042 [00:03<00:01, 413.02it/s]Padding data:  66%|██████▌   | 1348/2042 [00:03<00:01, 412.41it/s]Padding data:  68%|██████▊   | 1390/2042 [00:03<00:01, 413.74it/s]Padding data:  70%|███████   | 1432/2042 [00:03<00:01, 407.98it/s]Padding data:  72%|███████▏  | 1473/2042 [00:03<00:01, 403.86it/s]Padding data:  74%|███████▍  | 1514/2042 [00:03<00:01, 400.07it/s]Padding data:  76%|███████▌  | 1555/2042 [00:03<00:01, 400.69it/s]Padding data:  78%|███████▊  | 1596/2042 [00:03<00:01, 386.71it/s]Padding data:  80%|████████  | 1641/2042 [00:04<00:00, 403.42it/s]Padding data:  83%|████████▎ | 1687/2042 [00:04<00:00, 417.21it/s]Padding data:  85%|████████▍ | 1729/2042 [00:04<00:00, 404.55it/s]Padding data:  87%|████████▋ | 1770/2042 [00:04<00:00, 404.25it/s]Padding data:  89%|████████▊ | 1811/2042 [00:04<00:00, 399.10it/s]Padding data:  91%|█████████ | 1851/2042 [00:04<00:00, 397.73it/s]Padding data:  93%|█████████▎| 1891/2042 [00:04<00:00, 395.24it/s]Padding data:  95%|█████████▍| 1932/2042 [00:04<00:00, 399.26it/s]Padding data:  97%|█████████▋| 1972/2042 [00:04<00:00, 397.62it/s]Padding data:  99%|█████████▊| 2012/2042 [00:04<00:00, 396.55it/s]Padding data: 100%|██████████| 2042/2042 [00:05<00:00, 404.46it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 200, 7)]          0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            24635     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 32,094
Trainable params: 32,094
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4407/4407 - 29s - loss: 0.6061
Epoch 2/100
4407/4407 - 32s - loss: 0.5784
Epoch 3/100
4407/4407 - 32s - loss: 0.5670
Epoch 4/100
4407/4407 - 27s - loss: 0.5564
Epoch 5/100
4407/4407 - 32s - loss: 0.5474
Epoch 6/100
4407/4407 - 32s - loss: 0.5380
Epoch 7/100
4407/4407 - 32s - loss: 0.5340
Epoch 8/100
4407/4407 - 32s - loss: 0.5275
Epoch 9/100
4407/4407 - 32s - loss: 0.5234
Epoch 10/100
4407/4407 - 33s - loss: 0.5209
Epoch 11/100
4407/4407 - 32s - loss: 0.5176
Epoch 12/100
4407/4407 - 33s - loss: 0.5162
Epoch 13/100
4407/4407 - 32s - loss: 0.5144
Epoch 14/100
4407/4407 - 32s - loss: 0.5138
Epoch 15/100
4407/4407 - 32s - loss: 0.5121
Epoch 16/100
4407/4407 - 33s - loss: 0.5111
Epoch 17/100
4407/4407 - 33s - loss: 0.5096
Epoch 18/100
4407/4407 - 33s - loss: 0.5100
Epoch 19/100
4407/4407 - 33s - loss: 0.5102
Epoch 20/100
4407/4407 - 33s - loss: 0.5097
Epoch 21/100
4407/4407 - 33s - loss: 0.5100

Epoch 00021: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 22/100
4407/4407 - 34s - loss: 0.5028
Epoch 23/100
4407/4407 - 34s - loss: 0.5023
Epoch 24/100
4407/4407 - 34s - loss: 0.5026
Epoch 25/100
4407/4407 - 34s - loss: 0.5015
Epoch 26/100
4407/4407 - 34s - loss: 0.5005
Epoch 27/100
4407/4407 - 33s - loss: 0.5009
Epoch 28/100
4407/4407 - 33s - loss: 0.5009
Epoch 29/100
4407/4407 - 33s - loss: 0.5014
Epoch 30/100
4407/4407 - 33s - loss: 0.5003
Epoch 31/100
4407/4407 - 34s - loss: 0.4990
Epoch 32/100
4407/4407 - 34s - loss: 0.5001
Epoch 33/100
4407/4407 - 33s - loss: 0.4999
Epoch 34/100
4407/4407 - 34s - loss: 0.4999
Epoch 35/100
4407/4407 - 33s - loss: 0.4991

Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 36/100
4407/4407 - 34s - loss: 0.4963
Epoch 37/100
4407/4407 - 34s - loss: 0.4960
Epoch 38/100
4407/4407 - 33s - loss: 0.4958
Epoch 39/100
4407/4407 - 34s - loss: 0.4954
Epoch 40/100
4407/4407 - 34s - loss: 0.4952
Epoch 41/100
4407/4407 - 33s - loss: 0.4955
Epoch 42/100
4407/4407 - 33s - loss: 0.4951
Epoch 43/100
4407/4407 - 33s - loss: 0.4942
Epoch 44/100
4407/4407 - 33s - loss: 0.4950
Epoch 45/100
4407/4407 - 34s - loss: 0.4946
Epoch 46/100
4407/4407 - 34s - loss: 0.4949
Epoch 47/100
4407/4407 - 34s - loss: 0.4946

Epoch 00047: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 48/100
4407/4407 - 33s - loss: 0.4926
Epoch 49/100
4407/4407 - 34s - loss: 0.4927
Epoch 50/100
4407/4407 - 33s - loss: 0.4926
Epoch 51/100
4407/4407 - 33s - loss: 0.4923
Epoch 52/100
4407/4407 - 34s - loss: 0.4919
Epoch 53/100
4407/4407 - 34s - loss: 0.4925
Epoch 54/100
4407/4407 - 33s - loss: 0.4924
Epoch 55/100
4407/4407 - 34s - loss: 0.4923
Epoch 56/100
4407/4407 - 33s - loss: 0.4923

Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 57/100
4407/4407 - 34s - loss: 0.4914
Epoch 58/100
4407/4407 - 34s - loss: 0.4918
Epoch 59/100
4407/4407 - 34s - loss: 0.4914
Epoch 60/100
4407/4407 - 34s - loss: 0.4915
Epoch 61/100
4407/4407 - 26s - loss: 0.4911
Epoch 62/100
4407/4407 - 34s - loss: 0.4915
Epoch 63/100
4407/4407 - 34s - loss: 0.4913
Epoch 64/100
4407/4407 - 33s - loss: 0.4914
Epoch 65/100
4407/4407 - 34s - loss: 0.4913
Epoch 66/100
4407/4407 - 33s - loss: 0.4914
Epoch 67/100
4407/4407 - 33s - loss: 0.4912
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.6275503881082378
Test f1_avg: 0.6112561120258113
