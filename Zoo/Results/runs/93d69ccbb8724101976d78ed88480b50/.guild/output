INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   1%|          | 12/2042 [00:00<00:17, 113.20it/s]Padding data:   1%|          | 24/2042 [00:00<00:18, 110.52it/s]Padding data:   2%|▏         | 36/2042 [00:00<00:18, 110.08it/s]Padding data:   2%|▏         | 48/2042 [00:00<00:19, 102.82it/s]Padding data:   3%|▎         | 60/2042 [00:00<00:18, 106.86it/s]Padding data:   3%|▎         | 71/2042 [00:00<00:20, 96.80it/s] Padding data:   4%|▍         | 81/2042 [00:00<00:20, 96.30it/s]Padding data:   4%|▍         | 91/2042 [00:00<00:20, 96.65it/s]Padding data:   5%|▍         | 101/2042 [00:01<00:20, 96.87it/s]Padding data:   6%|▌         | 115/2042 [00:01<00:17, 107.49it/s]Padding data:   6%|▌         | 126/2042 [00:01<00:17, 107.49it/s]Padding data:   7%|▋         | 137/2042 [00:01<00:17, 107.56it/s]Padding data:   7%|▋         | 151/2042 [00:01<00:16, 115.01it/s]Padding data:   8%|▊         | 163/2042 [00:01<00:17, 108.95it/s]Padding data:   9%|▊         | 176/2042 [00:01<00:16, 114.34it/s]Padding data:   9%|▉         | 188/2042 [00:01<00:16, 109.43it/s]Padding data:  10%|▉         | 200/2042 [00:01<00:16, 111.71it/s]Padding data:  10%|█         | 212/2042 [00:01<00:16, 109.86it/s]Padding data:  11%|█         | 224/2042 [00:02<00:16, 109.10it/s]Padding data:  12%|█▏        | 237/2042 [00:02<00:15, 114.39it/s]Padding data:  12%|█▏        | 249/2042 [00:02<00:15, 113.56it/s]Padding data:  13%|█▎        | 262/2042 [00:02<00:15, 116.16it/s]Padding data:  13%|█▎        | 275/2042 [00:02<00:15, 117.74it/s]Padding data:  14%|█▍        | 287/2042 [00:02<00:14, 117.99it/s]Padding data:  15%|█▍        | 301/2042 [00:02<00:14, 122.28it/s]Padding data:  15%|█▌        | 314/2042 [00:02<00:14, 122.63it/s]Padding data:  16%|█▌        | 327/2042 [00:02<00:13, 124.44it/s]Padding data:  17%|█▋        | 340/2042 [00:03<00:13, 123.40it/s]Padding data:  17%|█▋        | 353/2042 [00:03<00:13, 121.98it/s]Padding data:  18%|█▊        | 366/2042 [00:03<00:13, 122.37it/s]Padding data:  19%|█▊        | 379/2042 [00:03<00:13, 120.49it/s]Padding data:  19%|█▉        | 392/2042 [00:03<00:13, 119.16it/s]Padding data:  20%|█▉        | 405/2042 [00:03<00:13, 121.47it/s]Padding data:  20%|██        | 418/2042 [00:03<00:14, 114.27it/s]Padding data:  21%|██        | 430/2042 [00:03<00:15, 102.39it/s]Padding data:  22%|██▏       | 441/2042 [00:03<00:15, 100.96it/s]Padding data:  22%|██▏       | 453/2042 [00:04<00:15, 103.88it/s]Padding data:  23%|██▎       | 464/2042 [00:04<00:15, 102.15it/s]Padding data:  23%|██▎       | 476/2042 [00:04<00:14, 105.67it/s]Padding data:  24%|██▍       | 487/2042 [00:04<00:15, 98.59it/s] Padding data:  24%|██▍       | 497/2042 [00:04<00:15, 97.11it/s]Padding data:  25%|██▍       | 508/2042 [00:04<00:15, 100.30it/s]Padding data:  25%|██▌       | 519/2042 [00:04<00:16, 94.70it/s] Padding data:  26%|██▌       | 531/2042 [00:04<00:15, 99.80it/s]Padding data:  27%|██▋       | 542/2042 [00:04<00:14, 101.95it/s]Padding data:  27%|██▋       | 554/2042 [00:05<00:14, 104.58it/s]Padding data:  28%|██▊       | 565/2042 [00:05<00:14, 102.66it/s]Padding data:  28%|██▊       | 576/2042 [00:05<00:14, 99.67it/s] Padding data:  29%|██▉       | 590/2042 [00:05<00:13, 110.07it/s]Padding data:  29%|██▉       | 602/2042 [00:05<00:13, 107.37it/s]Padding data:  30%|███       | 614/2042 [00:05<00:12, 110.36it/s]Padding data:  31%|███       | 626/2042 [00:05<00:13, 108.65it/s]Padding data:  31%|███       | 637/2042 [00:05<00:13, 107.32it/s]Padding data:  32%|███▏      | 650/2042 [00:05<00:12, 112.75it/s]Padding data:  32%|███▏      | 662/2042 [00:06<00:12, 107.32it/s]Padding data:  33%|███▎      | 675/2042 [00:06<00:12, 112.13it/s]Padding data:  34%|███▎      | 687/2042 [00:06<00:11, 113.46it/s]Padding data:  34%|███▍      | 699/2042 [00:06<00:12, 111.79it/s]Padding data:  35%|███▍      | 712/2042 [00:06<00:11, 114.00it/s]Padding data:  35%|███▌      | 724/2042 [00:06<00:12, 108.05it/s]Padding data:  36%|███▌      | 736/2042 [00:06<00:11, 109.80it/s]Padding data:  37%|███▋      | 748/2042 [00:06<00:11, 112.18it/s]Padding data:  37%|███▋      | 761/2042 [00:06<00:11, 115.81it/s]Padding data:  38%|███▊      | 775/2042 [00:07<00:10, 119.63it/s]Padding data:  39%|███▊      | 787/2042 [00:07<00:10, 117.26it/s]Padding data:  39%|███▉      | 801/2042 [00:07<00:10, 123.07it/s]Padding data:  40%|███▉      | 814/2042 [00:07<00:09, 123.75it/s]Padding data:  40%|████      | 827/2042 [00:07<00:09, 124.71it/s]Padding data:  41%|████      | 841/2042 [00:07<00:09, 128.86it/s]Padding data:  42%|████▏     | 854/2042 [00:07<00:09, 124.05it/s]Padding data:  42%|████▏     | 867/2042 [00:07<00:09, 123.45it/s]Padding data:  43%|████▎     | 880/2042 [00:07<00:09, 122.18it/s]Padding data:  44%|████▎     | 893/2042 [00:08<00:09, 122.63it/s]Padding data:  44%|████▍     | 907/2042 [00:08<00:08, 126.66it/s]Padding data:  45%|████▌     | 920/2042 [00:08<00:08, 126.63it/s]Padding data:  46%|████▌     | 933/2042 [00:08<00:08, 125.44it/s]Padding data:  46%|████▋     | 947/2042 [00:08<00:08, 126.97it/s]Padding data:  47%|████▋     | 960/2042 [00:08<00:08, 123.22it/s]Padding data:  48%|████▊     | 974/2042 [00:08<00:08, 127.92it/s]Padding data:  48%|████▊     | 987/2042 [00:08<00:08, 126.66it/s]Padding data:  49%|████▉     | 1002/2042 [00:08<00:07, 131.24it/s]Padding data:  50%|████▉     | 1016/2042 [00:08<00:08, 126.65it/s]Padding data:  50%|█████     | 1029/2042 [00:09<00:08, 126.20it/s]Padding data:  51%|█████     | 1043/2042 [00:09<00:07, 129.69it/s]Padding data:  52%|█████▏    | 1057/2042 [00:09<00:08, 120.91it/s]Padding data:  52%|█████▏    | 1070/2042 [00:09<00:08, 118.98it/s]Padding data:  53%|█████▎    | 1083/2042 [00:09<00:07, 121.96it/s]Padding data:  54%|█████▎    | 1096/2042 [00:09<00:07, 122.52it/s]Padding data:  54%|█████▍    | 1111/2042 [00:09<00:07, 128.84it/s]Padding data:  55%|█████▌    | 1124/2042 [00:09<00:07, 126.34it/s]Padding data:  56%|█████▌    | 1137/2042 [00:09<00:07, 126.40it/s]Padding data:  56%|█████▋    | 1151/2042 [00:10<00:06, 128.89it/s]Padding data:  57%|█████▋    | 1164/2042 [00:10<00:06, 127.02it/s]Padding data:  58%|█████▊    | 1179/2042 [00:10<00:06, 131.43it/s]Padding data:  58%|█████▊    | 1193/2042 [00:10<00:06, 126.39it/s]Padding data:  59%|█████▉    | 1206/2042 [00:10<00:06, 126.42it/s]Padding data:  60%|█████▉    | 1220/2042 [00:10<00:06, 129.16it/s]Padding data:  60%|██████    | 1233/2042 [00:10<00:06, 126.15it/s]Padding data:  61%|██████    | 1247/2042 [00:10<00:06, 127.92it/s]Padding data:  62%|██████▏   | 1260/2042 [00:10<00:06, 125.14it/s]Padding data:  62%|██████▏   | 1274/2042 [00:11<00:06, 126.79it/s]Padding data:  63%|██████▎   | 1288/2042 [00:11<00:05, 130.49it/s]Padding data:  64%|██████▍   | 1302/2042 [00:11<00:06, 118.98it/s]Padding data:  64%|██████▍   | 1315/2042 [00:11<00:06, 116.21it/s]Padding data:  65%|██████▍   | 1327/2042 [00:11<00:06, 115.77it/s]Padding data:  66%|██████▌   | 1340/2042 [00:11<00:06, 116.73it/s]Padding data:  66%|██████▌   | 1352/2042 [00:11<00:05, 116.26it/s]Padding data:  67%|██████▋   | 1364/2042 [00:11<00:06, 111.33it/s]Padding data:  67%|██████▋   | 1376/2042 [00:11<00:06, 109.91it/s]Padding data:  68%|██████▊   | 1389/2042 [00:12<00:05, 113.78it/s]Padding data:  69%|██████▊   | 1402/2042 [00:12<00:05, 117.56it/s]Padding data:  69%|██████▉   | 1416/2042 [00:12<00:05, 123.67it/s]Padding data:  70%|██████▉   | 1429/2042 [00:12<00:05, 118.23it/s]Padding data:  71%|███████   | 1444/2042 [00:12<00:04, 126.15it/s]Padding data:  71%|███████▏  | 1457/2042 [00:12<00:04, 123.17it/s]Padding data:  72%|███████▏  | 1470/2042 [00:12<00:04, 121.77it/s]Padding data:  73%|███████▎  | 1484/2042 [00:12<00:04, 125.56it/s]Padding data:  73%|███████▎  | 1497/2042 [00:12<00:04, 126.74it/s]Padding data:  74%|███████▍  | 1511/2042 [00:12<00:04, 127.83it/s]Padding data:  75%|███████▍  | 1524/2042 [00:13<00:04, 124.48it/s]Padding data:  75%|███████▌  | 1539/2042 [00:13<00:03, 129.96it/s]Padding data:  76%|███████▌  | 1553/2042 [00:13<00:03, 132.07it/s]Padding data:  77%|███████▋  | 1567/2042 [00:13<00:03, 131.75it/s]Padding data:  77%|███████▋  | 1581/2042 [00:13<00:03, 131.28it/s]Padding data:  78%|███████▊  | 1595/2042 [00:13<00:03, 127.18it/s]Padding data:  79%|███████▊  | 1608/2042 [00:13<00:03, 125.95it/s]Padding data:  79%|███████▉  | 1622/2042 [00:13<00:03, 126.04it/s]Padding data:  80%|████████  | 1636/2042 [00:13<00:03, 128.78it/s]Padding data:  81%|████████  | 1651/2042 [00:14<00:02, 133.89it/s]Padding data:  82%|████████▏ | 1665/2042 [00:14<00:02, 132.10it/s]Padding data:  82%|████████▏ | 1679/2042 [00:14<00:02, 131.95it/s]Padding data:  83%|████████▎ | 1693/2042 [00:14<00:02, 133.61it/s]Padding data:  84%|████████▎ | 1707/2042 [00:14<00:02, 132.93it/s]Padding data:  84%|████████▍ | 1721/2042 [00:14<00:02, 134.14it/s]Padding data:  85%|████████▍ | 1735/2042 [00:14<00:02, 124.94it/s]Padding data:  86%|████████▌ | 1749/2042 [00:14<00:02, 127.47it/s]Padding data:  86%|████████▋ | 1762/2042 [00:14<00:02, 125.31it/s]Padding data:  87%|████████▋ | 1775/2042 [00:15<00:02, 121.97it/s]Padding data:  88%|████████▊ | 1788/2042 [00:15<00:02, 123.29it/s]Padding data:  88%|████████▊ | 1801/2042 [00:15<00:02, 120.29it/s]Padding data:  89%|████████▉ | 1814/2042 [00:15<00:01, 118.71it/s]Padding data:  89%|████████▉ | 1827/2042 [00:15<00:01, 120.70it/s]Padding data:  90%|█████████ | 1840/2042 [00:15<00:01, 114.56it/s]Padding data:  91%|█████████ | 1854/2042 [00:15<00:01, 120.71it/s]Padding data:  91%|█████████▏| 1868/2042 [00:15<00:01, 124.21it/s]Padding data:  92%|█████████▏| 1881/2042 [00:15<00:01, 123.86it/s]Padding data:  93%|█████████▎| 1896/2042 [00:16<00:01, 130.07it/s]Padding data:  94%|█████████▎| 1910/2042 [00:16<00:01, 126.57it/s]Padding data:  94%|█████████▍| 1924/2042 [00:16<00:00, 129.06it/s]Padding data:  95%|█████████▍| 1937/2042 [00:16<00:00, 126.25it/s]Padding data:  95%|█████████▌| 1950/2042 [00:16<00:00, 125.07it/s]Padding data:  96%|█████████▌| 1964/2042 [00:16<00:00, 128.72it/s]Padding data:  97%|█████████▋| 1977/2042 [00:16<00:00, 123.89it/s]Padding data:  97%|█████████▋| 1990/2042 [00:16<00:00, 125.09it/s]Padding data:  98%|█████████▊| 2003/2042 [00:16<00:00, 121.24it/s]Padding data:  99%|█████████▊| 2016/2042 [00:17<00:00, 120.02it/s]Padding data:  99%|█████████▉| 2030/2042 [00:17<00:00, 123.94it/s]Padding data: 100%|██████████| 2042/2042 [00:17<00:00, 118.62it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 200, 125)]   0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 200, 64)      64064       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 200, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 200, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 200, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 200, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 200, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 200, 64)      8064        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 200, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 200, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 200, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 200, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 200, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 200, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 200, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 200, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 200, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 200, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 200, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 200, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 200, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 200, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 200, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 200, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 200, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 200, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 200, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 200, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 200, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 200, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 200, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 200, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 200, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 200, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 200, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 200, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 578,242
Trainable params: 575,682
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4383/4383 - 467s - loss: 0.6307
Epoch 2/100
4383/4383 - 426s - loss: 0.6155
Epoch 3/100
4383/4383 - 424s - loss: 0.6124
Epoch 4/100
4383/4383 - 415s - loss: 0.6078
Epoch 5/100
4383/4383 - 423s - loss: 0.6067
Epoch 6/100
4383/4383 - 419s - loss: 0.6026
Epoch 7/100
4383/4383 - 430s - loss: 0.5953
Epoch 8/100
4383/4383 - 430s - loss: 0.5888
Epoch 9/100
4383/4383 - 416s - loss: 0.5826
Epoch 10/100
4383/4383 - 421s - loss: 0.5750
Epoch 11/100
4383/4383 - 426s - loss: 0.5687
Epoch 12/100
4383/4383 - 431s - loss: 0.5611
Epoch 13/100
4383/4383 - 429s - loss: 0.5561
Epoch 14/100
4383/4383 - 429s - loss: 0.5537
Epoch 15/100
4383/4383 - 436s - loss: 0.5474
Epoch 16/100
4383/4383 - 431s - loss: 0.5473
Epoch 17/100
4383/4383 - 426s - loss: 0.5363
Epoch 18/100
4383/4383 - 413s - loss: 0.5317
Epoch 19/100
4383/4383 - 402s - loss: 0.5281
Epoch 20/100
4383/4383 - 404s - loss: 0.5249
Epoch 21/100
4383/4383 - 407s - loss: 0.5136
Epoch 22/100
4383/4383 - 404s - loss: 0.5112
Epoch 23/100
4383/4383 - 409s - loss: 0.5017
Epoch 24/100
4383/4383 - 408s - loss: 0.4983
Epoch 25/100
4383/4383 - 406s - loss: 0.4912
Epoch 26/100
4383/4383 - 415s - loss: 0.4800
Epoch 27/100
4383/4383 - 411s - loss: 0.4787
Epoch 28/100
4383/4383 - 407s - loss: 0.4653
Epoch 29/100
4383/4383 - 411s - loss: 0.4567
Epoch 30/100
4383/4383 - 417s - loss: 0.4515
Epoch 31/100
4383/4383 - 419s - loss: 0.4421
Epoch 32/100
4383/4383 - 419s - loss: 0.4338
Epoch 33/100
4383/4383 - 420s - loss: 0.4266
Epoch 34/100
4383/4383 - 414s - loss: 0.4143
Epoch 35/100
4383/4383 - 404s - loss: 0.4113
Epoch 36/100
4383/4383 - 403s - loss: 0.3969
Epoch 37/100
4383/4383 - 408s - loss: 0.3953
Epoch 38/100
4383/4383 - 414s - loss: 0.3866
Epoch 39/100
4383/4383 - 410s - loss: 0.3853
Epoch 40/100
4383/4383 - 410s - loss: 0.3717
Epoch 41/100
4383/4383 - 410s - loss: 0.3696
Epoch 42/100
4383/4383 - 405s - loss: 0.3581
Epoch 43/100
4383/4383 - 403s - loss: 0.3476
Epoch 44/100
4383/4383 - 406s - loss: 0.3469
Epoch 45/100
4383/4383 - 414s - loss: 0.3371
Epoch 46/100
4383/4383 - 418s - loss: 0.3371
Epoch 47/100
4383/4383 - 413s - loss: 0.3277
Epoch 48/100
4383/4383 - 412s - loss: 0.3249
Epoch 49/100
4383/4383 - 414s - loss: 0.3273
Epoch 50/100
4383/4383 - 407s - loss: 0.3151
Epoch 51/100
4383/4383 - 416s - loss: 0.3119
Epoch 52/100
4383/4383 - 418s - loss: 0.3072
Epoch 53/100
4383/4383 - 411s - loss: 0.3047
Epoch 54/100
4383/4383 - 408s - loss: 0.3046
Epoch 55/100
4383/4383 - 405s - loss: 0.2914
Epoch 56/100
4383/4383 - 402s - loss: 0.2925
Epoch 57/100
4383/4383 - 399s - loss: 0.2889
Epoch 58/100
4383/4383 - 414s - loss: 0.2903
Epoch 59/100
4383/4383 - 402s - loss: 0.2822
Epoch 60/100
4383/4383 - 402s - loss: 0.2850
Epoch 61/100
4383/4383 - 410s - loss: 0.2784
Epoch 62/100
4383/4383 - 407s - loss: 0.2781
Epoch 63/100
4383/4383 - 407s - loss: 0.2731
Epoch 64/100
4383/4383 - 401s - loss: 0.2659
Epoch 65/100
4383/4383 - 396s - loss: 0.2713
Epoch 66/100
4383/4383 - 404s - loss: 0.2624
Epoch 67/100
4383/4383 - 395s - loss: 0.2658
Epoch 68/100
4383/4383 - 389s - loss: 0.2696
Epoch 69/100
4383/4383 - 391s - loss: 0.2583
Epoch 70/100
4383/4383 - 403s - loss: 0.2471
Epoch 71/100
4383/4383 - 402s - loss: 0.2490
Epoch 72/100
4383/4383 - 403s - loss: 0.2577
Epoch 73/100
4383/4383 - 405s - loss: 0.2429
Epoch 74/100
4383/4383 - 400s - loss: 0.2527
Epoch 75/100
4383/4383 - 402s - loss: 0.2409
Epoch 76/100
4383/4383 - 400s - loss: 0.2411
Epoch 77/100
4383/4383 - 397s - loss: 0.2408
Epoch 78/100
4383/4383 - 396s - loss: 0.2399
Epoch 79/100
4383/4383 - 399s - loss: 0.2415
Epoch 80/100
4383/4383 - 399s - loss: 0.2363
Epoch 81/100
4383/4383 - 399s - loss: 0.2382
Epoch 82/100
4383/4383 - 401s - loss: 0.2367
Epoch 83/100
4383/4383 - 401s - loss: 0.2264
Epoch 84/100
4383/4383 - 402s - loss: 0.2305
Epoch 85/100
4383/4383 - 400s - loss: 0.2239
Epoch 86/100
4383/4383 - 400s - loss: 0.2329
Epoch 87/100
4383/4383 - 404s - loss: 0.2253
Epoch 88/100
4383/4383 - 401s - loss: 0.2227
Epoch 89/100
4383/4383 - 400s - loss: 0.2188
Epoch 90/100
4383/4383 - 404s - loss: 0.2270
Epoch 91/100
4383/4383 - 402s - loss: 0.2125
Epoch 92/100
4383/4383 - 399s - loss: 0.2195
Epoch 93/100
4383/4383 - 395s - loss: 0.2153
Epoch 94/100
4383/4383 - 390s - loss: 0.2168
Epoch 95/100
4383/4383 - 396s - loss: 0.2208

Epoch 00095: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 96/100
4383/4383 - 393s - loss: 0.1790
Epoch 97/100
4383/4383 - 396s - loss: 0.1714
Epoch 98/100
4383/4383 - 397s - loss: 0.1660
Epoch 99/100
4383/4383 - 387s - loss: 0.1601
Epoch 100/100
4383/4383 - 391s - loss: 0.1602
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.5195989438448048
Test f1_avg: 0.5224537243805577
