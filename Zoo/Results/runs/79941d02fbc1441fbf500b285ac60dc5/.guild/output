INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 46/2042 [00:00<00:04, 457.27it/s]Padding data:   5%|▍         | 93/2042 [00:00<00:04, 458.63it/s]Padding data:   7%|▋         | 140/2042 [00:00<00:04, 462.82it/s]Padding data:   9%|▉         | 190/2042 [00:00<00:03, 476.47it/s]Padding data:  12%|█▏        | 240/2042 [00:00<00:03, 482.51it/s]Padding data:  14%|█▍        | 290/2042 [00:00<00:03, 485.58it/s]Padding data:  17%|█▋        | 339/2042 [00:00<00:03, 482.44it/s]Padding data:  19%|█▉        | 388/2042 [00:00<00:03, 481.34it/s]Padding data:  21%|██▏       | 437/2042 [00:00<00:03, 478.56it/s]Padding data:  24%|██▍       | 485/2042 [00:01<00:03, 474.76it/s]Padding data:  26%|██▋       | 537/2042 [00:01<00:03, 485.91it/s]Padding data:  29%|██▉       | 593/2042 [00:01<00:02, 506.40it/s]Padding data:  32%|███▏      | 645/2042 [00:01<00:02, 508.52it/s]Padding data:  34%|███▍      | 698/2042 [00:01<00:02, 513.51it/s]Padding data:  37%|███▋      | 750/2042 [00:01<00:02, 506.43it/s]Padding data:  39%|███▉      | 803/2042 [00:01<00:02, 510.86it/s]Padding data:  42%|████▏     | 856/2042 [00:01<00:02, 514.48it/s]Padding data:  45%|████▍     | 909/2042 [00:01<00:02, 516.25it/s]Padding data:  47%|████▋     | 961/2042 [00:01<00:02, 517.14it/s]Padding data:  50%|████▉     | 1014/2042 [00:02<00:01, 518.04it/s]Padding data:  52%|█████▏    | 1066/2042 [00:02<00:01, 517.61it/s]Padding data:  55%|█████▍    | 1119/2042 [00:02<00:01, 519.13it/s]Padding data:  57%|█████▋    | 1172/2042 [00:02<00:01, 521.44it/s]Padding data:  60%|██████    | 1226/2042 [00:02<00:01, 526.14it/s]Padding data:  63%|██████▎   | 1281/2042 [00:02<00:01, 530.99it/s]Padding data:  65%|██████▌   | 1335/2042 [00:02<00:01, 532.89it/s]Padding data:  68%|██████▊   | 1389/2042 [00:02<00:01, 533.97it/s]Padding data:  71%|███████   | 1443/2042 [00:02<00:01, 533.15it/s]Padding data:  73%|███████▎  | 1497/2042 [00:02<00:01, 529.22it/s]Padding data:  76%|███████▌  | 1550/2042 [00:03<00:00, 527.05it/s]Padding data:  79%|███████▊  | 1604/2042 [00:03<00:00, 529.74it/s]Padding data:  81%|████████  | 1659/2042 [00:03<00:00, 533.40it/s]Padding data:  84%|████████▍ | 1713/2042 [00:03<00:00, 535.10it/s]Padding data:  87%|████████▋ | 1767/2042 [00:03<00:00, 527.29it/s]Padding data:  89%|████████▉ | 1820/2042 [00:03<00:00, 524.93it/s]Padding data:  92%|█████████▏| 1873/2042 [00:03<00:00, 517.11it/s]Padding data:  94%|█████████▍| 1926/2042 [00:03<00:00, 519.69it/s]Padding data:  97%|█████████▋| 1979/2042 [00:03<00:00, 521.26it/s]Padding data: 100%|█████████▉| 2032/2042 [00:03<00:00, 519.38it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 511.79it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 7)]          0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            12635     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,094
Trainable params: 20,094
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4408/4408 - 28s - loss: 0.6215
Epoch 2/100
4408/4408 - 31s - loss: 0.5945
Epoch 3/100
4408/4408 - 31s - loss: 0.5877
Epoch 4/100
4408/4408 - 31s - loss: 0.5801
Epoch 5/100
4408/4408 - 31s - loss: 0.5744
Epoch 6/100
4408/4408 - 31s - loss: 0.5695
Epoch 7/100
4408/4408 - 31s - loss: 0.5677
Epoch 8/100
4408/4408 - 31s - loss: 0.5658
Epoch 9/100
4408/4408 - 31s - loss: 0.5616
Epoch 10/100
4408/4408 - 31s - loss: 0.5580
Epoch 11/100
4408/4408 - 31s - loss: 0.5562
Epoch 12/100
4408/4408 - 31s - loss: 0.5528
Epoch 13/100
4408/4408 - 31s - loss: 0.5539
Epoch 14/100
4408/4408 - 31s - loss: 0.5528
Epoch 15/100
4408/4408 - 31s - loss: 0.5495
Epoch 16/100
4408/4408 - 31s - loss: 0.5503
Epoch 17/100
4408/4408 - 31s - loss: 0.5477
Epoch 18/100
4408/4408 - 32s - loss: 0.5492
Epoch 19/100
4408/4408 - 31s - loss: 0.5489
Epoch 20/100
4408/4408 - 31s - loss: 0.5478
Epoch 21/100
4408/4408 - 32s - loss: 0.5471
Epoch 22/100
4408/4408 - 31s - loss: 0.5474
Epoch 23/100
4408/4408 - 31s - loss: 0.5461
Epoch 24/100
4408/4408 - 31s - loss: 0.5454
Epoch 25/100
4408/4408 - 31s - loss: 0.5454
Epoch 26/100
4408/4408 - 32s - loss: 0.5451
Epoch 27/100
4408/4408 - 31s - loss: 0.5443
Epoch 28/100
4408/4408 - 31s - loss: 0.5440
Epoch 29/100
4408/4408 - 31s - loss: 0.5457
Epoch 30/100
4408/4408 - 32s - loss: 0.5435
Epoch 31/100
4408/4408 - 32s - loss: 0.5441
Epoch 32/100
4408/4408 - 31s - loss: 0.5429
Epoch 33/100
4408/4408 - 31s - loss: 0.5439
Epoch 34/100
4408/4408 - 31s - loss: 0.5433
Epoch 35/100
4408/4408 - 31s - loss: 0.5418
Epoch 36/100
4408/4408 - 31s - loss: 0.5423
Epoch 37/100
4408/4408 - 31s - loss: 0.5424
Epoch 38/100
4408/4408 - 29s - loss: 0.5420
Epoch 39/100
4408/4408 - 31s - loss: 0.5426

Epoch 00039: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 40/100
4408/4408 - 31s - loss: 0.5381
Epoch 41/100
4408/4408 - 31s - loss: 0.5383
Epoch 42/100
4408/4408 - 31s - loss: 0.5381
Epoch 43/100
4408/4408 - 30s - loss: 0.5375
Epoch 44/100
4408/4408 - 32s - loss: 0.5379
Epoch 45/100
4408/4408 - 31s - loss: 0.5374
Epoch 46/100
4408/4408 - 31s - loss: 0.5371
Epoch 47/100
4408/4408 - 29s - loss: 0.5369
Epoch 48/100
4408/4408 - 31s - loss: 0.5371
Epoch 49/100
4408/4408 - 31s - loss: 0.5367
Epoch 50/100
4408/4408 - 31s - loss: 0.5367
Epoch 51/100
4408/4408 - 30s - loss: 0.5365
Epoch 52/100
4408/4408 - 29s - loss: 0.5364
Epoch 53/100
4408/4408 - 31s - loss: 0.5355
Epoch 54/100
4408/4408 - 31s - loss: 0.5347
Epoch 55/100
4408/4408 - 31s - loss: 0.5358
Epoch 56/100
4408/4408 - 32s - loss: 0.5350
Epoch 57/100
4408/4408 - 31s - loss: 0.5351
Epoch 58/100
4408/4408 - 31s - loss: 0.5357

Epoch 00058: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 59/100
4408/4408 - 31s - loss: 0.5330
Epoch 60/100
4408/4408 - 31s - loss: 0.5328
Epoch 61/100
4408/4408 - 31s - loss: 0.5324
Epoch 62/100
4408/4408 - 31s - loss: 0.5326
Epoch 63/100
4408/4408 - 31s - loss: 0.5324
Epoch 64/100
4408/4408 - 31s - loss: 0.5325
Epoch 65/100
4408/4408 - 31s - loss: 0.5319
Epoch 66/100
4408/4408 - 31s - loss: 0.5318
Epoch 67/100
4408/4408 - 31s - loss: 0.5318
Epoch 68/100
4408/4408 - 31s - loss: 0.5316
Epoch 69/100
4408/4408 - 31s - loss: 0.5315
Epoch 70/100
4408/4408 - 31s - loss: 0.5312
Epoch 71/100
4408/4408 - 31s - loss: 0.5315
Epoch 72/100
4408/4408 - 31s - loss: 0.5315
Epoch 73/100
4408/4408 - 31s - loss: 0.5310
Epoch 74/100
4408/4408 - 31s - loss: 0.5311
Epoch 75/100
4408/4408 - 31s - loss: 0.5305
Epoch 76/100
4408/4408 - 31s - loss: 0.5311
Epoch 77/100
4408/4408 - 31s - loss: 0.5308
Epoch 78/100
4408/4408 - 31s - loss: 0.5306
Epoch 79/100
4408/4408 - 31s - loss: 0.5307

Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 80/100
4408/4408 - 31s - loss: 0.5293
Epoch 81/100
4408/4408 - 31s - loss: 0.5294
Epoch 82/100
4408/4408 - 31s - loss: 0.5291
Epoch 83/100
4408/4408 - 31s - loss: 0.5292
Epoch 84/100
4408/4408 - 31s - loss: 0.5291
Epoch 85/100
4408/4408 - 31s - loss: 0.5290
Epoch 86/100
4408/4408 - 31s - loss: 0.5290

Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 87/100
4408/4408 - 31s - loss: 0.5291
Epoch 88/100
4408/4408 - 31s - loss: 0.5287
Epoch 89/100
4408/4408 - 31s - loss: 0.5288
Epoch 90/100
4408/4408 - 31s - loss: 0.5288
Epoch 91/100
4408/4408 - 31s - loss: 0.5278
Epoch 92/100
4408/4408 - 31s - loss: 0.5285
Epoch 93/100
4408/4408 - 31s - loss: 0.5284
Epoch 94/100
4408/4408 - 31s - loss: 0.5284
Epoch 95/100
4408/4408 - 31s - loss: 0.5286
Epoch 96/100
4408/4408 - 31s - loss: 0.5282
Epoch 97/100
4408/4408 - 31s - loss: 0.5281
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.5830787221285509
Test f1_avg: 0.5741138131231783
