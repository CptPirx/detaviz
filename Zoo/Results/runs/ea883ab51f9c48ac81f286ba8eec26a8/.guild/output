INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 44/2042 [00:00<00:04, 428.45it/s]Padding data:   4%|▍         | 90/2042 [00:00<00:04, 442.71it/s]Padding data:   7%|▋         | 135/2042 [00:00<00:04, 443.06it/s]Padding data:   9%|▉         | 182/2042 [00:00<00:04, 452.02it/s]Padding data:  11%|█         | 228/2042 [00:00<00:04, 450.87it/s]Padding data:  14%|█▎        | 277/2042 [00:00<00:03, 463.32it/s]Padding data:  16%|█▌        | 328/2042 [00:00<00:03, 475.67it/s]Padding data:  19%|█▊        | 378/2042 [00:00<00:03, 482.10it/s]Padding data:  21%|██        | 430/2042 [00:00<00:03, 490.02it/s]Padding data:  24%|██▎       | 481/2042 [00:01<00:03, 494.63it/s]Padding data:  26%|██▌       | 532/2042 [00:01<00:03, 499.07it/s]Padding data:  29%|██▊       | 583/2042 [00:01<00:02, 499.69it/s]Padding data:  31%|███       | 633/2042 [00:01<00:02, 496.17it/s]Padding data:  33%|███▎      | 683/2042 [00:01<00:02, 495.12it/s]Padding data:  36%|███▌      | 733/2042 [00:01<00:02, 485.03it/s]Padding data:  38%|███▊      | 783/2042 [00:01<00:02, 489.34it/s]Padding data:  41%|████      | 834/2042 [00:01<00:02, 495.40it/s]Padding data:  43%|████▎     | 884/2042 [00:01<00:02, 496.10it/s]Padding data:  46%|████▌     | 935/2042 [00:01<00:02, 497.97it/s]Padding data:  48%|████▊     | 986/2042 [00:02<00:02, 499.15it/s]Padding data:  51%|█████     | 1036/2042 [00:02<00:02, 498.62it/s]Padding data:  53%|█████▎    | 1087/2042 [00:02<00:01, 500.52it/s]Padding data:  56%|█████▌    | 1139/2042 [00:02<00:01, 504.70it/s]Padding data:  58%|█████▊    | 1190/2042 [00:02<00:01, 505.59it/s]Padding data:  61%|██████    | 1241/2042 [00:02<00:01, 506.67it/s]Padding data:  63%|██████▎   | 1292/2042 [00:02<00:01, 495.28it/s]Padding data:  66%|██████▌   | 1342/2042 [00:02<00:01, 480.30it/s]Padding data:  68%|██████▊   | 1391/2042 [00:02<00:01, 476.15it/s]Padding data:  70%|███████   | 1439/2042 [00:02<00:01, 447.09it/s]Padding data:  73%|███████▎  | 1485/2042 [00:03<00:01, 437.33it/s]Padding data:  75%|███████▍  | 1529/2042 [00:03<00:01, 437.18it/s]Padding data:  77%|███████▋  | 1573/2042 [00:03<00:01, 437.84it/s]Padding data:  79%|███████▉  | 1617/2042 [00:03<00:01, 424.97it/s]Padding data:  81%|████████▏ | 1663/2042 [00:03<00:00, 434.55it/s]Padding data:  84%|████████▎ | 1707/2042 [00:03<00:00, 415.06it/s]Padding data:  86%|████████▌ | 1749/2042 [00:03<00:00, 407.47it/s]Padding data:  88%|████████▊ | 1790/2042 [00:03<00:00, 406.58it/s]Padding data:  90%|████████▉ | 1831/2042 [00:03<00:00, 398.96it/s]Padding data:  92%|█████████▏| 1873/2042 [00:04<00:00, 404.50it/s]Padding data:  94%|█████████▎| 1914/2042 [00:04<00:00, 405.15it/s]Padding data:  96%|█████████▌| 1958/2042 [00:04<00:00, 413.42it/s]Padding data:  98%|█████████▊| 2005/2042 [00:04<00:00, 428.41it/s]Padding data: 100%|██████████| 2042/2042 [00:04<00:00, 461.84it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 200, 7)]     0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 200, 64)      3648        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 200, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 200, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 200, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 200, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 200, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 200, 64)      512         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 200, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 200, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 200, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 200, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 200, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 200, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 200, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 200, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 200, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 200, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 200, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 200, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 200, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 200, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 200, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 200, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 200, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 200, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 200, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 200, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 200, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 200, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 200, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 200, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 200, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 200, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 200, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 200, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 510,274
Trainable params: 507,714
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4407/4407 - 338s - loss: 0.6238
Epoch 2/100
4407/4407 - 305s - loss: 0.5903
Epoch 3/100
4407/4407 - 307s - loss: 0.5588
Epoch 4/100
4407/4407 - 308s - loss: 0.5429
Epoch 5/100
4407/4407 - 308s - loss: 0.5255
Epoch 6/100
4407/4407 - 307s - loss: 0.5171
Epoch 7/100
4407/4407 - 306s - loss: 0.5106
Epoch 8/100
4407/4407 - 307s - loss: 0.5069
Epoch 9/100
4407/4407 - 308s - loss: 0.5005
Epoch 10/100
4407/4407 - 307s - loss: 0.4959
Epoch 11/100
4407/4407 - 308s - loss: 0.4939
Epoch 12/100
4407/4407 - 306s - loss: 0.4882
Epoch 13/100
4407/4407 - 308s - loss: 0.4842
Epoch 14/100
4407/4407 - 309s - loss: 0.4846
Epoch 15/100
4407/4407 - 308s - loss: 0.4796
Epoch 16/100
4407/4407 - 306s - loss: 0.4722
Epoch 17/100
4407/4407 - 306s - loss: 0.4738
Epoch 18/100
4407/4407 - 306s - loss: 0.4687
Epoch 19/100
4407/4407 - 309s - loss: 0.4672
Epoch 20/100
4407/4407 - 308s - loss: 0.4636
Epoch 21/100
4407/4407 - 308s - loss: 0.4564
Epoch 22/100
4407/4407 - 307s - loss: 0.4586
Epoch 23/100
4407/4407 - 307s - loss: 0.4594
Epoch 24/100
4407/4407 - 307s - loss: 0.4544
Epoch 25/100
4407/4407 - 306s - loss: 0.4524
Epoch 26/100
4407/4407 - 302s - loss: 0.4522
Epoch 27/100
4407/4407 - 302s - loss: 0.4509
Epoch 28/100
4407/4407 - 302s - loss: 0.4496
Epoch 29/100
4407/4407 - 302s - loss: 0.4450
Epoch 30/100
4407/4407 - 301s - loss: 0.4416
Epoch 31/100
4407/4407 - 301s - loss: 0.4409
Epoch 32/100
4407/4407 - 301s - loss: 0.4381
Epoch 33/100
4407/4407 - 301s - loss: 0.4362
Epoch 34/100
4407/4407 - 302s - loss: 0.4371
Epoch 35/100
4407/4407 - 301s - loss: 0.4326
Epoch 36/100
4407/4407 - 301s - loss: 0.4320
Epoch 37/100
4407/4407 - 302s - loss: 0.4260
Epoch 38/100
4407/4407 - 302s - loss: 0.4237
Epoch 39/100
4407/4407 - 302s - loss: 0.4182
Epoch 40/100
4407/4407 - 302s - loss: 0.4195
Epoch 41/100
4407/4407 - 302s - loss: 0.4132
Epoch 42/100
4407/4407 - 301s - loss: 0.4106
Epoch 43/100
4407/4407 - 301s - loss: 0.4144
Epoch 44/100
4407/4407 - 302s - loss: 0.4064
Epoch 45/100
4407/4407 - 301s - loss: 0.4025
Epoch 46/100
4407/4407 - 301s - loss: 0.3968
Epoch 47/100
4407/4407 - 301s - loss: 0.3958
Epoch 48/100
4407/4407 - 301s - loss: 0.3899
Epoch 49/100
4407/4407 - 301s - loss: 0.3893
Epoch 50/100
4407/4407 - 301s - loss: 0.3875
Epoch 51/100
4407/4407 - 301s - loss: 0.3811
Epoch 52/100
4407/4407 - 301s - loss: 0.3773
Epoch 53/100
4407/4407 - 301s - loss: 0.3764
Epoch 54/100
4407/4407 - 302s - loss: 0.3710
Epoch 55/100
4407/4407 - 301s - loss: 0.3655
Epoch 56/100
4407/4407 - 301s - loss: 0.3650
Epoch 57/100
4407/4407 - 301s - loss: 0.3604
Epoch 58/100
4407/4407 - 301s - loss: 0.3538
Epoch 59/100
4407/4407 - 302s - loss: 0.3553
Epoch 60/100
4407/4407 - 301s - loss: 0.3530
Epoch 61/100
4407/4407 - 301s - loss: 0.3482
Epoch 62/100
4407/4407 - 301s - loss: 0.3441
Epoch 63/100
4407/4407 - 302s - loss: 0.3411
Epoch 64/100
4407/4407 - 301s - loss: 0.3380
Epoch 65/100
4407/4407 - 301s - loss: 0.3304
Epoch 66/100
4407/4407 - 302s - loss: 0.3287
Epoch 67/100
4407/4407 - 302s - loss: 0.3218
Epoch 68/100
4407/4407 - 301s - loss: 0.3237
Epoch 69/100
4407/4407 - 302s - loss: 0.3221
Epoch 70/100
4407/4407 - 302s - loss: 0.3135
Epoch 71/100
4407/4407 - 302s - loss: 0.3165
Epoch 72/100
4407/4407 - 302s - loss: 0.3090
Epoch 73/100
4407/4407 - 301s - loss: 0.3088
Epoch 74/100
4407/4407 - 303s - loss: 0.3004
Epoch 75/100
4407/4407 - 302s - loss: 0.2945
Epoch 76/100
4407/4407 - 301s - loss: 0.2911
Epoch 77/100
4407/4407 - 301s - loss: 0.2904
Epoch 78/100
4407/4407 - 302s - loss: 0.2864
Epoch 79/100
4407/4407 - 301s - loss: 0.2826
Epoch 80/100
4407/4407 - 301s - loss: 0.2801
Epoch 81/100
4407/4407 - 301s - loss: 0.2715
Epoch 82/100
4407/4407 - 301s - loss: 0.2747
Epoch 83/100
4407/4407 - 302s - loss: 0.2710
Epoch 84/100
4407/4407 - 301s - loss: 0.2647
Epoch 85/100
4407/4407 - 303s - loss: 0.2689
Epoch 86/100
4407/4407 - 304s - loss: 0.2616
Epoch 87/100
4407/4407 - 303s - loss: 0.2546
Epoch 88/100
4407/4407 - 302s - loss: 0.2522
Epoch 89/100
4407/4407 - 301s - loss: 0.2569
Epoch 90/100
4407/4407 - 302s - loss: 0.2436
Epoch 91/100
4407/4407 - 302s - loss: 0.2532
Epoch 92/100
4407/4407 - 302s - loss: 0.2445
Epoch 93/100
4407/4407 - 301s - loss: 0.2433
Epoch 94/100
4407/4407 - 301s - loss: 0.2376
Epoch 95/100
4407/4407 - 302s - loss: 0.2361
Epoch 96/100
4407/4407 - 302s - loss: 0.2321
Epoch 97/100
4407/4407 - 301s - loss: 0.2267
Epoch 98/100
4407/4407 - 301s - loss: 0.2233
Epoch 99/100
4407/4407 - 301s - loss: 0.2219
Epoch 100/100
4407/4407 - 302s - loss: 0.2258
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.5071793953404513
Test f1_avg: 0.5062053045475234
