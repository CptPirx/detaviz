INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 51/2042 [00:00<00:03, 502.35it/s]Padding data:   5%|▌         | 103/2042 [00:00<00:03, 510.41it/s]Padding data:   8%|▊         | 155/2042 [00:00<00:03, 514.11it/s]Padding data:  10%|█         | 207/2042 [00:00<00:03, 515.76it/s]Padding data:  13%|█▎        | 259/2042 [00:00<00:03, 516.03it/s]Padding data:  15%|█▌        | 312/2042 [00:00<00:03, 518.36it/s]Padding data:  18%|█▊        | 364/2042 [00:00<00:03, 512.39it/s]Padding data:  20%|██        | 417/2042 [00:00<00:03, 516.12it/s]Padding data:  23%|██▎       | 470/2042 [00:00<00:03, 518.59it/s]Padding data:  26%|██▌       | 526/2042 [00:01<00:02, 530.28it/s]Padding data:  29%|██▊       | 582/2042 [00:01<00:02, 536.74it/s]Padding data:  31%|███       | 637/2042 [00:01<00:02, 539.99it/s]Padding data:  34%|███▍      | 692/2042 [00:01<00:02, 494.54it/s]Padding data:  36%|███▋      | 743/2042 [00:01<00:02, 477.23it/s]Padding data:  39%|███▉      | 792/2042 [00:01<00:02, 479.30it/s]Padding data:  41%|████      | 841/2042 [00:01<00:02, 480.02it/s]Padding data:  44%|████▍     | 894/2042 [00:01<00:02, 492.73it/s]Padding data:  47%|████▋     | 952/2042 [00:01<00:02, 516.54it/s]Padding data:  49%|████▉     | 1010/2042 [00:01<00:01, 534.68it/s]Padding data:  52%|█████▏    | 1067/2042 [00:02<00:01, 544.73it/s]Padding data:  55%|█████▌    | 1125/2042 [00:02<00:01, 554.60it/s]Padding data:  58%|█████▊    | 1183/2042 [00:02<00:01, 560.96it/s]Padding data:  61%|██████    | 1241/2042 [00:02<00:01, 565.18it/s]Padding data:  64%|██████▎   | 1299/2042 [00:02<00:01, 569.53it/s]Padding data:  67%|██████▋   | 1358/2042 [00:02<00:01, 572.79it/s]Padding data:  69%|██████▉   | 1416/2042 [00:02<00:01, 573.15it/s]Padding data:  72%|███████▏  | 1474/2042 [00:02<00:00, 573.50it/s]Padding data:  75%|███████▌  | 1532/2042 [00:02<00:00, 573.57it/s]Padding data:  78%|███████▊  | 1590/2042 [00:02<00:00, 568.84it/s]Padding data:  81%|████████  | 1648/2042 [00:03<00:00, 570.83it/s]Padding data:  84%|████████▎ | 1707/2042 [00:03<00:00, 573.87it/s]Padding data:  86%|████████▋ | 1765/2042 [00:03<00:00, 572.44it/s]Padding data:  89%|████████▉ | 1823/2042 [00:03<00:00, 571.62it/s]Padding data:  92%|█████████▏| 1881/2042 [00:03<00:00, 560.37it/s]Padding data:  95%|█████████▍| 1938/2042 [00:03<00:00, 539.70it/s]Padding data:  98%|█████████▊| 1993/2042 [00:03<00:00, 523.33it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 534.20it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 500, 7)]     0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 500, 64)      3648        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 500, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 500, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 500, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 500, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 500, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 500, 64)      512         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 500, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 500, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 500, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 500, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 500, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 500, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 500, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 500, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 500, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 500, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 500, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 500, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 500, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 500, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 500, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 500, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 500, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 500, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 500, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 500, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 500, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 500, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 500, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 500, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 500, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 500, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 500, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 510,274
Trainable params: 507,714
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4406/4406 - 733s - loss: 0.5893
Epoch 2/100
4406/4406 - 693s - loss: 0.5486
Epoch 3/100
4406/4406 - 692s - loss: 0.5099
Epoch 4/100
4406/4406 - 691s - loss: 0.4916
Epoch 5/100
4406/4406 - 691s - loss: 0.4893
Epoch 6/100
4406/4406 - 691s - loss: 0.4833
Epoch 7/100
4406/4406 - 690s - loss: 0.4823
Epoch 8/100
4406/4406 - 691s - loss: 0.4795
Epoch 9/100
4406/4406 - 691s - loss: 0.4723
Epoch 10/100
4406/4406 - 691s - loss: 0.4706
Epoch 11/100
4406/4406 - 691s - loss: 0.4682
Epoch 12/100
4406/4406 - 691s - loss: 0.4687
Epoch 13/100
4406/4406 - 691s - loss: 0.4642
Epoch 14/100
4406/4406 - 691s - loss: 0.4664
Epoch 15/100
4406/4406 - 692s - loss: 0.4628
Epoch 16/100
4406/4406 - 692s - loss: 0.4618
Epoch 17/100
4406/4406 - 692s - loss: 0.4576
Epoch 18/100
4406/4406 - 693s - loss: 0.4568
Epoch 19/100
4406/4406 - 692s - loss: 0.4574
Epoch 20/100
4406/4406 - 691s - loss: 0.4574
Epoch 21/100
4406/4406 - 691s - loss: 0.4550
Epoch 22/100
4406/4406 - 691s - loss: 0.4549
Epoch 23/100
4406/4406 - 692s - loss: 0.4554
Epoch 24/100
4406/4406 - 692s - loss: 0.4497
Epoch 25/100
4406/4406 - 692s - loss: 0.4489
Epoch 26/100
4406/4406 - 692s - loss: 0.4495
Epoch 27/100
4406/4406 - 691s - loss: 0.4436
Epoch 28/100
4406/4406 - 692s - loss: 0.4450
Epoch 29/100
4406/4406 - 691s - loss: 0.4486
Epoch 30/100
4406/4406 - 692s - loss: 0.4465
Epoch 31/100
4406/4406 - 691s - loss: 0.4423
Epoch 32/100
4406/4406 - 691s - loss: 0.4415
Epoch 33/100
4406/4406 - 691s - loss: 0.4429
Epoch 34/100
4406/4406 - 691s - loss: 0.4410
Epoch 35/100
4406/4406 - 693s - loss: 0.4360
Epoch 36/100
4406/4406 - 692s - loss: 0.4402
Epoch 37/100
4406/4406 - 691s - loss: 0.4405
Epoch 38/100
4406/4406 - 691s - loss: 0.4364
Epoch 39/100
4406/4406 - 691s - loss: 0.4352
Epoch 40/100
4406/4406 - 692s - loss: 0.4355
Epoch 41/100
4406/4406 - 691s - loss: 0.4260
Epoch 42/100
4406/4406 - 692s - loss: 0.4279
Epoch 43/100
4406/4406 - 692s - loss: 0.4253
Epoch 44/100
4406/4406 - 693s - loss: 0.4243
Epoch 45/100
4406/4406 - 691s - loss: 0.4203
Epoch 46/100
4406/4406 - 690s - loss: 0.4194
Epoch 47/100
4406/4406 - 690s - loss: 0.4149
Epoch 48/100
4406/4406 - 690s - loss: 0.4166
Epoch 49/100
4406/4406 - 690s - loss: 0.4129
Epoch 50/100
4406/4406 - 690s - loss: 0.4108
Epoch 51/100
4406/4406 - 690s - loss: 0.4092
Epoch 52/100
4406/4406 - 690s - loss: 0.4044
Epoch 53/100
4406/4406 - 689s - loss: 0.4024
Epoch 54/100
4406/4406 - 690s - loss: 0.4032
Epoch 55/100
4406/4406 - 690s - loss: 0.3967
Epoch 56/100
4406/4406 - 690s - loss: 0.4030
Epoch 57/100
4406/4406 - 690s - loss: 0.3936
Epoch 58/100
4406/4406 - 690s - loss: 0.3863
Epoch 59/100
4406/4406 - 690s - loss: 0.3851
Epoch 60/100
4406/4406 - 689s - loss: 0.3817
Epoch 61/100
4406/4406 - 691s - loss: 0.3795
Epoch 62/100
4406/4406 - 690s - loss: 0.3770
Epoch 63/100
4406/4406 - 690s - loss: 0.3728
Epoch 64/100
4406/4406 - 689s - loss: 0.3687
Epoch 65/100
4406/4406 - 690s - loss: 0.3662
Epoch 66/100
4406/4406 - 690s - loss: 0.3602
Epoch 67/100
4406/4406 - 690s - loss: 0.3624
Epoch 68/100
4406/4406 - 689s - loss: 0.3529
Epoch 69/100
4406/4406 - 689s - loss: 0.3497
Epoch 70/100
4406/4406 - 689s - loss: 0.3473
Epoch 71/100
4406/4406 - 689s - loss: 0.3471
Epoch 72/100
4406/4406 - 689s - loss: 0.3426
Epoch 73/100
4406/4406 - 689s - loss: 0.3371
Epoch 74/100
4406/4406 - 689s - loss: 0.3392
Epoch 75/100
4406/4406 - 689s - loss: 0.3318
Epoch 76/100
4406/4406 - 690s - loss: 0.3276
Epoch 77/100
4406/4406 - 690s - loss: 0.3243
Epoch 78/100
4406/4406 - 690s - loss: 0.3181
Epoch 79/100
4406/4406 - 689s - loss: 0.3135
Epoch 80/100
4406/4406 - 689s - loss: 0.3119
Epoch 81/100
4406/4406 - 689s - loss: 0.3054
Epoch 82/100
4406/4406 - 689s - loss: 0.3007
Epoch 83/100
4406/4406 - 689s - loss: 0.3024
Epoch 84/100
4406/4406 - 690s - loss: 0.2912
Epoch 85/100
4406/4406 - 689s - loss: 0.2917
Epoch 86/100
4406/4406 - 689s - loss: 0.2878
Epoch 87/100
4406/4406 - 689s - loss: 0.2826
Epoch 88/100
4406/4406 - 689s - loss: 0.2813
Epoch 89/100
4406/4406 - 688s - loss: 0.2761
Epoch 90/100
4406/4406 - 689s - loss: 0.2757
Epoch 91/100
4406/4406 - 689s - loss: 0.2702
Epoch 92/100
4406/4406 - 690s - loss: 0.2632
Epoch 93/100
4406/4406 - 689s - loss: 0.2608
Epoch 94/100
4406/4406 - 689s - loss: 0.2590
Epoch 95/100
4406/4406 - 690s - loss: 0.2601
Epoch 96/100
4406/4406 - 690s - loss: 0.2407
Epoch 97/100
4406/4406 - 689s - loss: 0.2497
Epoch 98/100
4406/4406 - 690s - loss: 0.2439
Epoch 99/100
4406/4406 - 689s - loss: 0.2403
Epoch 100/100
4406/4406 - 689s - loss: 0.2399
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.4487128801447734
Test f1_avg: 0.44941741270622726
