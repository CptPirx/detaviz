INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 56/2042 [00:00<00:03, 551.99it/s]Padding data:   6%|▌         | 113/2042 [00:00<00:03, 557.79it/s]Padding data:   8%|▊         | 170/2042 [00:00<00:03, 561.66it/s]Padding data:  11%|█         | 227/2042 [00:00<00:03, 562.56it/s]Padding data:  14%|█▍        | 284/2042 [00:00<00:03, 561.95it/s]Padding data:  17%|█▋        | 342/2042 [00:00<00:02, 567.03it/s]Padding data:  20%|█▉        | 401/2042 [00:00<00:02, 572.40it/s]Padding data:  22%|██▏       | 459/2042 [00:00<00:02, 573.78it/s]Padding data:  25%|██▌       | 518/2042 [00:00<00:02, 576.71it/s]Padding data:  28%|██▊       | 576/2042 [00:01<00:02, 576.49it/s]Padding data:  31%|███       | 635/2042 [00:01<00:02, 578.02it/s]Padding data:  34%|███▍      | 693/2042 [00:01<00:02, 577.52it/s]Padding data:  37%|███▋      | 751/2042 [00:01<00:02, 575.31it/s]Padding data:  40%|███▉      | 809/2042 [00:01<00:02, 575.80it/s]Padding data:  42%|████▏     | 867/2042 [00:01<00:02, 573.81it/s]Padding data:  45%|████▌     | 925/2042 [00:01<00:01, 571.46it/s]Padding data:  48%|████▊     | 983/2042 [00:01<00:01, 572.81it/s]Padding data:  51%|█████     | 1041/2042 [00:01<00:01, 574.57it/s]Padding data:  54%|█████▍    | 1100/2042 [00:01<00:01, 577.01it/s]Padding data:  57%|█████▋    | 1159/2042 [00:02<00:01, 579.31it/s]Padding data:  60%|█████▉    | 1218/2042 [00:02<00:01, 580.09it/s]Padding data:  63%|██████▎   | 1277/2042 [00:02<00:01, 581.11it/s]Padding data:  65%|██████▌   | 1336/2042 [00:02<00:01, 580.34it/s]Padding data:  68%|██████▊   | 1395/2042 [00:02<00:01, 580.88it/s]Padding data:  71%|███████   | 1454/2042 [00:02<00:01, 582.64it/s]Padding data:  74%|███████▍  | 1513/2042 [00:02<00:00, 581.87it/s]Padding data:  77%|███████▋  | 1572/2042 [00:02<00:00, 583.06it/s]Padding data:  80%|███████▉  | 1632/2042 [00:02<00:00, 585.19it/s]Padding data:  83%|████████▎ | 1691/2042 [00:02<00:00, 584.61it/s]Padding data:  86%|████████▌ | 1750/2042 [00:03<00:00, 584.76it/s]Padding data:  89%|████████▊ | 1809/2042 [00:03<00:00, 582.69it/s]Padding data:  91%|█████████▏| 1868/2042 [00:03<00:00, 582.88it/s]Padding data:  94%|█████████▍| 1927/2042 [00:03<00:00, 580.64it/s]Padding data:  97%|█████████▋| 1986/2042 [00:03<00:00, 582.69it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 577.38it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 80)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            13000     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,459
Trainable params: 20,459
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 31s - loss: 0.6405
Epoch 2/100
4383/4383 - 30s - loss: 0.5993
Epoch 3/100
4383/4383 - 30s - loss: 0.5853
Epoch 4/100
4383/4383 - 30s - loss: 0.5799
Epoch 5/100
4383/4383 - 30s - loss: 0.5742
Epoch 6/100
4383/4383 - 31s - loss: 0.5644
Epoch 7/100
4383/4383 - 30s - loss: 0.5548
Epoch 8/100
4383/4383 - 30s - loss: 0.5519
Epoch 9/100
4383/4383 - 30s - loss: 0.5483
Epoch 10/100
4383/4383 - 30s - loss: 0.5451
Epoch 11/100
4383/4383 - 30s - loss: 0.5426
Epoch 12/100
4383/4383 - 30s - loss: 0.5394
Epoch 13/100
4383/4383 - 31s - loss: 0.5363
Epoch 14/100
4383/4383 - 30s - loss: 0.5344
Epoch 15/100
4383/4383 - 30s - loss: 0.5309
Epoch 16/100
4383/4383 - 30s - loss: 0.5259
Epoch 17/100
4383/4383 - 30s - loss: 0.5270
Epoch 18/100
4383/4383 - 30s - loss: 0.5243
Epoch 19/100
4383/4383 - 30s - loss: 0.5193
Epoch 20/100
4383/4383 - 30s - loss: 0.5177
Epoch 21/100
4383/4383 - 30s - loss: 0.5157
Epoch 22/100
4383/4383 - 30s - loss: 0.5135
Epoch 23/100
4383/4383 - 30s - loss: 0.5120
Epoch 24/100
4383/4383 - 31s - loss: 0.5101
Epoch 25/100
4383/4383 - 31s - loss: 0.5066
Epoch 26/100
4383/4383 - 31s - loss: 0.5067
Epoch 27/100
4383/4383 - 31s - loss: 0.5053
Epoch 28/100
4383/4383 - 31s - loss: 0.5039
Epoch 29/100
4383/4383 - 30s - loss: 0.5026
Epoch 30/100
4383/4383 - 30s - loss: 0.5025
Epoch 31/100
4383/4383 - 31s - loss: 0.4993
Epoch 32/100
4383/4383 - 31s - loss: 0.4967
Epoch 33/100
4383/4383 - 31s - loss: 0.4997
Epoch 34/100
4383/4383 - 31s - loss: 0.4965
Epoch 35/100
4383/4383 - 31s - loss: 0.4940
Epoch 36/100
4383/4383 - 31s - loss: 0.4935
Epoch 37/100
4383/4383 - 31s - loss: 0.4898
Epoch 38/100
4383/4383 - 31s - loss: 0.4914
Epoch 39/100
4383/4383 - 31s - loss: 0.4880
Epoch 40/100
4383/4383 - 31s - loss: 0.4895
Epoch 41/100
4383/4383 - 31s - loss: 0.4902
Epoch 42/100
4383/4383 - 31s - loss: 0.4878
Epoch 43/100
4383/4383 - 31s - loss: 0.4846
Epoch 44/100
4383/4383 - 31s - loss: 0.4835
Epoch 45/100
4383/4383 - 31s - loss: 0.4833
Epoch 46/100
4383/4383 - 31s - loss: 0.4832
Epoch 47/100
4383/4383 - 31s - loss: 0.4834
Epoch 48/100
4383/4383 - 31s - loss: 0.4834
Epoch 49/100
4383/4383 - 31s - loss: 0.4809
Epoch 50/100
4383/4383 - 30s - loss: 0.4781
Epoch 51/100
4383/4383 - 30s - loss: 0.4821
Epoch 52/100
4383/4383 - 31s - loss: 0.4803
Epoch 53/100
4383/4383 - 30s - loss: 0.4807
Epoch 54/100
4383/4383 - 31s - loss: 0.4780

Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 55/100
4383/4383 - 30s - loss: 0.4648
Epoch 56/100
4383/4383 - 31s - loss: 0.4622
Epoch 57/100
4383/4383 - 31s - loss: 0.4600
Epoch 58/100
4383/4383 - 31s - loss: 0.4599
Epoch 59/100
4383/4383 - 31s - loss: 0.4604
Epoch 60/100
4383/4383 - 31s - loss: 0.4587
Epoch 61/100
4383/4383 - 31s - loss: 0.4598
Epoch 62/100
4383/4383 - 30s - loss: 0.4583
Epoch 63/100
4383/4383 - 30s - loss: 0.4593
Epoch 64/100
4383/4383 - 30s - loss: 0.4578
Epoch 65/100
4383/4383 - 31s - loss: 0.4579
Epoch 66/100
4383/4383 - 31s - loss: 0.4569
Epoch 67/100
4383/4383 - 31s - loss: 0.4568
Epoch 68/100
4383/4383 - 31s - loss: 0.4561
Epoch 69/100
4383/4383 - 31s - loss: 0.4567
Epoch 70/100
4383/4383 - 31s - loss: 0.4553
Epoch 71/100
4383/4383 - 31s - loss: 0.4541
Epoch 72/100
4383/4383 - 30s - loss: 0.4558
Epoch 73/100
4383/4383 - 31s - loss: 0.4540
Epoch 74/100
4383/4383 - 31s - loss: 0.4551
Epoch 75/100
4383/4383 - 31s - loss: 0.4544
Epoch 76/100
4383/4383 - 31s - loss: 0.4540
Epoch 77/100
4383/4383 - 30s - loss: 0.4533
Epoch 78/100
4383/4383 - 30s - loss: 0.4538
Epoch 79/100
4383/4383 - 30s - loss: 0.4527
Epoch 80/100
4383/4383 - 30s - loss: 0.4521
Epoch 81/100
4383/4383 - 30s - loss: 0.4522
Epoch 82/100
4383/4383 - 30s - loss: 0.4519
Epoch 83/100
4383/4383 - 30s - loss: 0.4535
Epoch 84/100
4383/4383 - 30s - loss: 0.4500
Epoch 85/100
4383/4383 - 31s - loss: 0.4512
Epoch 86/100
4383/4383 - 31s - loss: 0.4529
Epoch 87/100
4383/4383 - 31s - loss: 0.4511
Epoch 88/100
4383/4383 - 31s - loss: 0.4512

Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 89/100
4383/4383 - 31s - loss: 0.4427
Epoch 90/100
4383/4383 - 31s - loss: 0.4421
Epoch 91/100
4383/4383 - 31s - loss: 0.4417
Epoch 92/100
4383/4383 - 31s - loss: 0.4414
Epoch 93/100
4383/4383 - 31s - loss: 0.4409
Epoch 94/100
4383/4383 - 30s - loss: 0.4398
Epoch 95/100
4383/4383 - 31s - loss: 0.4411
Epoch 96/100
4383/4383 - 31s - loss: 0.4400
Epoch 97/100
4383/4383 - 30s - loss: 0.4396
Epoch 98/100
4383/4383 - 31s - loss: 0.4397
Epoch 99/100
4383/4383 - 31s - loss: 0.4408
Epoch 100/100
4383/4383 - 31s - loss: 0.4387
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.7228534326001341
Test f1_avg: 0.6827159618862798
