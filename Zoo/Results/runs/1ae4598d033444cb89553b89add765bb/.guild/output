INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 33/2042 [00:00<00:06, 321.25it/s]Padding data:   3%|3         | 66/2042 [00:00<00:06, 315.84it/s]Padding data:   5%|4         | 99/2042 [00:00<00:06, 319.69it/s]Padding data:   7%|6         | 133/2042 [00:00<00:05, 326.63it/s]Padding data:   8%|8         | 167/2042 [00:00<00:05, 330.52it/s]Padding data:  10%|9         | 201/2042 [00:00<00:05, 332.89it/s]Padding data:  12%|#1        | 238/2042 [00:00<00:05, 344.09it/s]Padding data:  13%|#3        | 275/2042 [00:00<00:05, 351.47it/s]Padding data:  15%|#5        | 315/2042 [00:00<00:04, 365.71it/s]Padding data:  17%|#7        | 355/2042 [00:01<00:04, 374.24it/s]Padding data:  19%|#9        | 395/2042 [00:01<00:04, 381.22it/s]Padding data:  21%|##1       | 434/2042 [00:01<00:04, 383.02it/s]Padding data:  23%|##3       | 473/2042 [00:01<00:04, 379.73it/s]Padding data:  25%|##5       | 511/2042 [00:01<00:04, 377.87it/s]Padding data:  27%|##6       | 551/2042 [00:01<00:03, 383.61it/s]Padding data:  29%|##8       | 591/2042 [00:01<00:03, 386.50it/s]Padding data:  31%|###       | 630/2042 [00:01<00:03, 386.70it/s]Padding data:  33%|###2      | 669/2042 [00:01<00:03, 385.70it/s]Padding data:  35%|###4      | 708/2042 [00:01<00:03, 382.74it/s]Padding data:  37%|###6      | 747/2042 [00:02<00:03, 382.93it/s]Padding data:  39%|###8      | 788/2042 [00:02<00:03, 387.81it/s]Padding data:  41%|####      | 829/2042 [00:02<00:03, 391.22it/s]Padding data:  43%|####2     | 869/2042 [00:02<00:02, 391.81it/s]Padding data:  45%|####4     | 909/2042 [00:02<00:02, 391.08it/s]Padding data:  47%|####6     | 950/2042 [00:02<00:02, 395.81it/s]Padding data:  48%|####8     | 990/2042 [00:02<00:02, 392.72it/s]Padding data:  50%|#####     | 1030/2042 [00:02<00:02, 384.98it/s]Padding data:  52%|#####2    | 1069/2042 [00:02<00:02, 381.20it/s]Padding data:  54%|#####4    | 1108/2042 [00:02<00:02, 382.95it/s]Padding data:  56%|#####6    | 1147/2042 [00:03<00:02, 384.19it/s]Padding data:  58%|#####8    | 1187/2042 [00:03<00:02, 386.88it/s]Padding data:  60%|######    | 1226/2042 [00:03<00:02, 383.59it/s]Padding data:  62%|######2   | 1267/2042 [00:03<00:01, 388.25it/s]Padding data:  64%|######3   | 1306/2042 [00:03<00:01, 387.93it/s]Padding data:  66%|######5   | 1345/2042 [00:03<00:01, 386.57it/s]Padding data:  68%|######7   | 1385/2042 [00:03<00:01, 388.55it/s]Padding data:  70%|######9   | 1424/2042 [00:03<00:01, 387.00it/s]Padding data:  72%|#######1  | 1463/2042 [00:03<00:01, 383.66it/s]Padding data:  74%|#######3  | 1502/2042 [00:03<00:01, 383.58it/s]Padding data:  76%|#######5  | 1543/2042 [00:04<00:01, 390.54it/s]Padding data:  78%|#######7  | 1584/2042 [00:04<00:01, 395.45it/s]Padding data:  80%|#######9  | 1625/2042 [00:04<00:01, 397.73it/s]Padding data:  82%|########1 | 1665/2042 [00:04<00:00, 390.62it/s]Padding data:  83%|########3 | 1705/2042 [00:04<00:00, 379.23it/s]Padding data:  85%|########5 | 1744/2042 [00:04<00:00, 381.53it/s]Padding data:  87%|########7 | 1783/2042 [00:04<00:00, 379.88it/s]Padding data:  89%|########9 | 1822/2042 [00:04<00:00, 377.61it/s]Padding data:  91%|#########1| 1861/2042 [00:04<00:00, 379.33it/s]Padding data:  93%|#########3| 1900/2042 [00:05<00:00, 381.64it/s]Padding data:  95%|#########4| 1939/2042 [00:05<00:00, 371.31it/s]Padding data:  97%|#########6| 1977/2042 [00:05<00:00, 370.90it/s]Padding data:  99%|#########8| 2016/2042 [00:05<00:00, 374.57it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 377.83it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 200, 60)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 200, 64)      30784       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 200, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 200, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 200, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 200, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 200, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 200, 64)      3904        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 200, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 200, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 200, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 200, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 200, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 200, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 200, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 200, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 200, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 200, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 200, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 200, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 200, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 200, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 200, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 200, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 200, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 200, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 200, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 200, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 200, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 200, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 200, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 200, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 200, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 200, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 200, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 200, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 4)            516         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 541,060
Trainable params: 538,500
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4388/4388 - 496s - loss: 0.9595
Epoch 2/100
4388/4388 - 549s - loss: 0.9061
Epoch 3/100
4388/4388 - 539s - loss: 0.8633
Epoch 4/100
4388/4388 - 527s - loss: 0.8409
Epoch 5/100
4388/4388 - 508s - loss: 0.8125
Epoch 6/100
4388/4388 - 520s - loss: 0.7868
Epoch 7/100
4388/4388 - 506s - loss: 0.7677
Epoch 8/100
4388/4388 - 532s - loss: 0.7473
Epoch 9/100
4388/4388 - 581s - loss: 0.7249
Epoch 10/100
4388/4388 - 589s - loss: 0.7083
Epoch 11/100
4388/4388 - 555s - loss: 0.6909
Epoch 12/100
4388/4388 - 541s - loss: 0.6727
Epoch 13/100
4388/4388 - 582s - loss: 0.6506
Epoch 14/100
4388/4388 - 593s - loss: 0.6337
Epoch 15/100
4388/4388 - 573s - loss: 0.6139
Epoch 16/100
4388/4388 - 560s - loss: 0.5978
Epoch 17/100
4388/4388 - 579s - loss: 0.5702
Epoch 18/100
4388/4388 - 563s - loss: 0.5490
Epoch 19/100
4388/4388 - 546s - loss: 0.5305
Epoch 20/100
4388/4388 - 541s - loss: 0.5137
Epoch 21/100
4388/4388 - 560s - loss: 0.4896
Epoch 22/100
4388/4388 - 554s - loss: 0.4653
Epoch 23/100
4388/4388 - 558s - loss: 0.4531
Epoch 24/100
4388/4388 - 558s - loss: 0.4267
Epoch 25/100
4388/4388 - 556s - loss: 0.4083
Epoch 26/100
4388/4388 - 550s - loss: 0.3899
Epoch 27/100
4388/4388 - 549s - loss: 0.3664
Epoch 28/100
4388/4388 - 571s - loss: 0.3490
Epoch 29/100
4388/4388 - 578s - loss: 0.3329
Epoch 30/100
4388/4388 - 594s - loss: 0.3187
Epoch 31/100
4388/4388 - 579s - loss: 0.2998
Epoch 32/100
4388/4388 - 560s - loss: 0.2844
Epoch 33/100
4388/4388 - 544s - loss: 0.2748
Epoch 34/100
4388/4388 - 550s - loss: 0.2610
Epoch 35/100
4388/4388 - 544s - loss: 0.2386
Epoch 36/100
4388/4388 - 573s - loss: 0.2392
Epoch 37/100
4388/4388 - 556s - loss: 0.2304
Epoch 38/100
4388/4388 - 575s - loss: 0.2186
Epoch 39/100
4388/4388 - 595s - loss: 0.2049
Epoch 40/100
4388/4388 - 593s - loss: 0.1978
Epoch 41/100
4388/4388 - 584s - loss: 0.1931
Epoch 42/100
4388/4388 - 578s - loss: 0.1868
Epoch 43/100
4388/4388 - 553s - loss: 0.1701
Epoch 44/100
4388/4388 - 565s - loss: 0.1684
Epoch 45/100
4388/4388 - 566s - loss: 0.1657
Epoch 46/100
4388/4388 - 576s - loss: 0.1546
Epoch 47/100
4388/4388 - 571s - loss: 0.1440
Epoch 48/100
4388/4388 - 552s - loss: 0.1539
Epoch 49/100
4388/4388 - 554s - loss: 0.1330
Epoch 50/100
4388/4388 - 547s - loss: 0.1374
Epoch 51/100
4388/4388 - 558s - loss: 0.1305
Epoch 52/100
4388/4388 - 556s - loss: 0.1280
Epoch 53/100
4388/4388 - 561s - loss: 0.1187
Epoch 54/100
4388/4388 - 564s - loss: 0.1307
Epoch 55/100
4388/4388 - 575s - loss: 0.1152
Epoch 56/100
4388/4388 - 587s - loss: 0.1121
Epoch 57/100
4388/4388 - 589s - loss: 0.1142
Epoch 58/100
4388/4388 - 576s - loss: 0.1091
Epoch 59/100
4388/4388 - 589s - loss: 0.1022
Epoch 60/100
4388/4388 - 593s - loss: 0.1044
Epoch 61/100
4388/4388 - 589s - loss: 0.0919
Epoch 62/100
4388/4388 - 581s - loss: 0.1018
Epoch 63/100
4388/4388 - 591s - loss: 0.0993
Epoch 64/100
4388/4388 - 595s - loss: 0.0974
Epoch 65/100
4388/4388 - 606s - loss: 0.0987

Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 66/100
4388/4388 - 614s - loss: 0.0464
Epoch 67/100
4388/4388 - 613s - loss: 0.0341
Epoch 68/100
4388/4388 - 616s - loss: 0.0364
Epoch 69/100
4388/4388 - 611s - loss: 0.0310
Epoch 70/100
4388/4388 - 624s - loss: 0.0297
Epoch 71/100
4388/4388 - 618s - loss: 0.0290
Epoch 72/100
4388/4388 - 618s - loss: 0.0283
Epoch 73/100
4388/4388 - 630s - loss: 0.0284
Epoch 74/100
4388/4388 - 629s - loss: 0.0255
Epoch 75/100
4388/4388 - 629s - loss: 0.0272
Epoch 76/100
4388/4388 - 621s - loss: 0.0259
Epoch 77/100
4388/4388 - 628s - loss: 0.0263
Epoch 78/100
4388/4388 - 643s - loss: 0.0220
Epoch 79/100
4388/4388 - 626s - loss: 0.0247
Epoch 80/100
4388/4388 - 625s - loss: 0.0235
Epoch 81/100
4388/4388 - 638s - loss: 0.0211
Epoch 82/100
4388/4388 - 639s - loss: 0.0185
Epoch 83/100
4388/4388 - 634s - loss: 0.0282
Epoch 84/100
4388/4388 - 634s - loss: 0.0214
Epoch 85/100
4388/4388 - 632s - loss: 0.0194
Epoch 86/100
4388/4388 - 629s - loss: 0.0210

Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 87/100
4388/4388 - 626s - loss: 0.0154
Epoch 88/100
4388/4388 - 631s - loss: 0.0098
Epoch 89/100
4388/4388 - 630s - loss: 0.0090
Epoch 90/100
4388/4388 - 636s - loss: 0.0097
Epoch 91/100
4388/4388 - 645s - loss: 0.0100
Epoch 92/100
4388/4388 - 637s - loss: 0.0091
Epoch 93/100
4388/4388 - 633s - loss: 0.0084
Epoch 94/100
4388/4388 - 636s - loss: 0.0083
Epoch 95/100
4388/4388 - 637s - loss: 0.0086
Epoch 96/100
4388/4388 - 635s - loss: 0.0085
Epoch 97/100
4388/4388 - 645s - loss: 0.0078
Epoch 98/100
4388/4388 - 648s - loss: 0.0081
Epoch 99/100
4388/4388 - 645s - loss: 0.0078
Epoch 100/100
4388/4388 - 645s - loss: 0.0071
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.29399749018881605
Test f1_avg: 0.28918110177960665
