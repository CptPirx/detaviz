INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 60/2042 [00:00<00:03, 598.11it/s]Padding data:   6%|▌         | 122/2042 [00:00<00:03, 609.65it/s]Padding data:   9%|▉         | 184/2042 [00:00<00:03, 610.07it/s]Padding data:  12%|█▏        | 246/2042 [00:00<00:02, 611.68it/s]Padding data:  15%|█▌        | 308/2042 [00:00<00:02, 611.34it/s]Padding data:  18%|█▊        | 370/2042 [00:00<00:02, 607.17it/s]Padding data:  21%|██        | 431/2042 [00:00<00:02, 603.01it/s]Padding data:  24%|██▍       | 492/2042 [00:00<00:02, 603.11it/s]Padding data:  27%|██▋       | 553/2042 [00:00<00:02, 603.68it/s]Padding data:  30%|███       | 614/2042 [00:01<00:02, 599.14it/s]Padding data:  33%|███▎      | 674/2042 [00:01<00:02, 595.15it/s]Padding data:  36%|███▌      | 735/2042 [00:01<00:02, 597.32it/s]Padding data:  39%|███▉      | 796/2042 [00:01<00:02, 600.66it/s]Padding data:  42%|████▏     | 858/2042 [00:01<00:01, 604.33it/s]Padding data:  45%|████▌     | 919/2042 [00:01<00:01, 605.69it/s]Padding data:  48%|████▊     | 981/2042 [00:01<00:01, 608.70it/s]Padding data:  51%|█████     | 1042/2042 [00:01<00:01, 608.76it/s]Padding data:  54%|█████▍    | 1103/2042 [00:01<00:01, 600.56it/s]Padding data:  57%|█████▋    | 1166/2042 [00:01<00:01, 607.57it/s]Padding data:  60%|██████    | 1228/2042 [00:02<00:01, 609.11it/s]Padding data:  63%|██████▎   | 1291/2042 [00:02<00:01, 614.21it/s]Padding data:  66%|██████▋   | 1353/2042 [00:02<00:01, 613.00it/s]Padding data:  69%|██████▉   | 1415/2042 [00:02<00:01, 612.31it/s]Padding data:  72%|███████▏  | 1477/2042 [00:02<00:00, 612.18it/s]Padding data:  75%|███████▌  | 1539/2042 [00:02<00:00, 612.49it/s]Padding data:  78%|███████▊  | 1602/2042 [00:02<00:00, 615.42it/s]Padding data:  82%|████████▏ | 1665/2042 [00:02<00:00, 617.93it/s]Padding data:  85%|████████▍ | 1727/2042 [00:02<00:00, 616.15it/s]Padding data:  88%|████████▊ | 1789/2042 [00:02<00:00, 612.67it/s]Padding data:  91%|█████████ | 1851/2042 [00:03<00:00, 609.82it/s]Padding data:  94%|█████████▎| 1912/2042 [00:03<00:00, 606.92it/s]Padding data:  97%|█████████▋| 1973/2042 [00:03<00:00, 607.72it/s]Padding data: 100%|█████████▉| 2034/2042 [00:03<00:00, 608.04it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 607.75it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 500, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            60900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 68,359
Trainable params: 68,359
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4382/4382 - 85s - loss: 0.7271
Epoch 2/100
4382/4382 - 82s - loss: 0.5804
Epoch 3/100
4382/4382 - 82s - loss: 0.5676
Epoch 4/100
4382/4382 - 81s - loss: 0.5579
Epoch 5/100
4382/4382 - 82s - loss: 0.5429
Epoch 6/100
4382/4382 - 82s - loss: 0.5361
Epoch 7/100
4382/4382 - 82s - loss: 0.5274
Epoch 8/100
4382/4382 - 81s - loss: 0.5189
Epoch 9/100
4382/4382 - 80s - loss: 0.5097
Epoch 10/100
4382/4382 - 78s - loss: 0.5021
Epoch 11/100
4382/4382 - 77s - loss: 0.4945
Epoch 12/100
4382/4382 - 80s - loss: 0.4898
Epoch 13/100
4382/4382 - 78s - loss: 0.4847
Epoch 14/100
4382/4382 - 78s - loss: 0.4835
Epoch 15/100
4382/4382 - 80s - loss: 0.4789
Epoch 16/100
4382/4382 - 79s - loss: 0.4734
Epoch 17/100
4382/4382 - 78s - loss: 0.4707
Epoch 18/100
4382/4382 - 79s - loss: 0.4664
Epoch 19/100
4382/4382 - 79s - loss: 0.4669
Epoch 20/100
4382/4382 - 79s - loss: 0.4624
Epoch 21/100
4382/4382 - 77s - loss: 0.4584
Epoch 22/100
4382/4382 - 76s - loss: 0.4541
Epoch 23/100
4382/4382 - 76s - loss: 0.4517
Epoch 24/100
4382/4382 - 77s - loss: 0.4504
Epoch 25/100
4382/4382 - 77s - loss: 0.4459
Epoch 26/100
4382/4382 - 79s - loss: 0.4445
Epoch 27/100
4382/4382 - 78s - loss: 0.4420
Epoch 28/100
4382/4382 - 78s - loss: 0.4424
Epoch 29/100
4382/4382 - 78s - loss: 0.4429
Epoch 30/100
4382/4382 - 77s - loss: 0.4395
Epoch 31/100
4382/4382 - 77s - loss: 0.4357
Epoch 32/100
4382/4382 - 77s - loss: 0.4373
Epoch 33/100
4382/4382 - 77s - loss: 0.4348
Epoch 34/100
4382/4382 - 77s - loss: 0.4327
Epoch 35/100
4382/4382 - 77s - loss: 0.4307
Epoch 36/100
4382/4382 - 77s - loss: 0.4301
Epoch 37/100
4382/4382 - 77s - loss: 0.4277
Epoch 38/100
4382/4382 - 76s - loss: 0.4254
Epoch 39/100
4382/4382 - 77s - loss: 0.4280
Epoch 40/100
4382/4382 - 75s - loss: 0.4250
Epoch 41/100
4382/4382 - 76s - loss: 0.4262
Epoch 42/100
4382/4382 - 76s - loss: 0.4242
Epoch 43/100
4382/4382 - 76s - loss: 0.4219
Epoch 44/100
4382/4382 - 76s - loss: 0.4220
Epoch 45/100
4382/4382 - 76s - loss: 0.4179
Epoch 46/100
4382/4382 - 76s - loss: 0.4203
Epoch 47/100
4382/4382 - 76s - loss: 0.4218
Epoch 48/100
4382/4382 - 76s - loss: 0.4188
Epoch 49/100
4382/4382 - 76s - loss: 0.4169
Epoch 50/100
4382/4382 - 77s - loss: 0.4139
Epoch 51/100
4382/4382 - 76s - loss: 0.4149
Epoch 52/100
4382/4382 - 76s - loss: 0.4145
Epoch 53/100
4382/4382 - 76s - loss: 0.4155
Epoch 54/100
4382/4382 - 75s - loss: 0.4102
Epoch 55/100
4382/4382 - 75s - loss: 0.4125
Epoch 56/100
4382/4382 - 76s - loss: 0.4138
Epoch 57/100
4382/4382 - 75s - loss: 0.4089
Epoch 58/100
4382/4382 - 75s - loss: 0.4096
Epoch 59/100
4382/4382 - 78s - loss: 0.4085
Epoch 60/100
4382/4382 - 78s - loss: 0.4124
Epoch 61/100
4382/4382 - 79s - loss: 0.4064
Epoch 62/100
4382/4382 - 78s - loss: 0.4083
Epoch 63/100
4382/4382 - 79s - loss: 0.4058
Epoch 64/100
4382/4382 - 78s - loss: 0.4058
Epoch 65/100
4382/4382 - 78s - loss: 0.4072
Epoch 66/100
4382/4382 - 78s - loss: 0.4026
Epoch 67/100
4382/4382 - 78s - loss: 0.4007
Epoch 68/100
4382/4382 - 78s - loss: 0.4047
Epoch 69/100
4382/4382 - 78s - loss: 0.4005
Epoch 70/100
4382/4382 - 78s - loss: 0.4010
Epoch 71/100
4382/4382 - 79s - loss: 0.4013
Epoch 72/100
4382/4382 - 79s - loss: 0.4022
Epoch 73/100
4382/4382 - 79s - loss: 0.4023

Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 74/100
4382/4382 - 78s - loss: 0.3803
Epoch 75/100
4382/4382 - 79s - loss: 0.3784
Epoch 76/100
4382/4382 - 78s - loss: 0.3789
Epoch 77/100
4382/4382 - 77s - loss: 0.3780
Epoch 78/100
4382/4382 - 77s - loss: 0.3763
Epoch 79/100
4382/4382 - 77s - loss: 0.3767
Epoch 80/100
4382/4382 - 78s - loss: 0.3752
Epoch 81/100
4382/4382 - 78s - loss: 0.3747
Epoch 82/100
4382/4382 - 76s - loss: 0.3762
Epoch 83/100
4382/4382 - 78s - loss: 0.3738
Epoch 84/100
4382/4382 - 77s - loss: 0.3735
Epoch 85/100
4382/4382 - 78s - loss: 0.3753
Epoch 86/100
4382/4382 - 77s - loss: 0.3747
Epoch 87/100
4382/4382 - 78s - loss: 0.3737
Epoch 88/100
4382/4382 - 77s - loss: 0.3719
Epoch 89/100
4382/4382 - 77s - loss: 0.3732
Epoch 90/100
4382/4382 - 77s - loss: 0.3713
Epoch 91/100
4382/4382 - 77s - loss: 0.3729
Epoch 92/100
4382/4382 - 77s - loss: 0.3701
Epoch 93/100
4382/4382 - 77s - loss: 0.3719
Epoch 94/100
4382/4382 - 76s - loss: 0.3718
Epoch 95/100
4382/4382 - 78s - loss: 0.3722
Epoch 96/100
4382/4382 - 77s - loss: 0.3717

Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 97/100
4382/4382 - 78s - loss: 0.3611
Epoch 98/100
4382/4382 - 77s - loss: 0.3597
Epoch 99/100
4382/4382 - 79s - loss: 0.3584
Epoch 100/100
4382/4382 - 78s - loss: 0.3593
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.8039567317558549
Test f1_avg: 0.7332478636940307
