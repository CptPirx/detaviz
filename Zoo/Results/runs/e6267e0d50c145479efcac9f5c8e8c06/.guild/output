INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 52/2042 [00:00<00:03, 512.82it/s]Padding data:   5%|▌         | 106/2042 [00:00<00:03, 526.42it/s]Padding data:   8%|▊         | 159/2042 [00:00<00:03, 518.11it/s]Padding data:  10%|█         | 211/2042 [00:00<00:03, 463.32it/s]Padding data:  13%|█▎        | 259/2042 [00:00<00:03, 460.17it/s]Padding data:  15%|█▍        | 306/2042 [00:00<00:03, 446.86it/s]Padding data:  17%|█▋        | 351/2042 [00:00<00:03, 445.42it/s]Padding data:  19%|█▉        | 396/2042 [00:00<00:03, 446.18it/s]Padding data:  22%|██▏       | 442/2042 [00:00<00:03, 447.73it/s]Padding data:  24%|██▍       | 490/2042 [00:01<00:03, 455.08it/s]Padding data:  27%|██▋       | 544/2042 [00:01<00:03, 479.62it/s]Padding data:  29%|██▉       | 596/2042 [00:01<00:02, 489.98it/s]Padding data:  32%|███▏      | 649/2042 [00:01<00:02, 501.73it/s]Padding data:  34%|███▍      | 703/2042 [00:01<00:02, 512.45it/s]Padding data:  37%|███▋      | 757/2042 [00:01<00:02, 519.53it/s]Padding data:  40%|███▉      | 812/2042 [00:01<00:02, 527.73it/s]Padding data:  42%|████▏     | 865/2042 [00:01<00:02, 514.95it/s]Padding data:  45%|████▍     | 917/2042 [00:01<00:02, 515.28it/s]Padding data:  47%|████▋     | 969/2042 [00:01<00:02, 512.46it/s]Padding data:  50%|█████     | 1023/2042 [00:02<00:01, 519.33it/s]Padding data:  53%|█████▎    | 1076/2042 [00:02<00:01, 521.20it/s]Padding data:  55%|█████▌    | 1133/2042 [00:02<00:01, 535.64it/s]Padding data:  58%|█████▊    | 1190/2042 [00:02<00:01, 544.96it/s]Padding data:  61%|██████    | 1245/2042 [00:02<00:01, 544.55it/s]Padding data:  64%|██████▍   | 1302/2042 [00:02<00:01, 550.50it/s]Padding data:  67%|██████▋   | 1358/2042 [00:02<00:01, 542.04it/s]Padding data:  69%|██████▉   | 1413/2042 [00:02<00:01, 543.79it/s]Padding data:  72%|███████▏  | 1468/2042 [00:02<00:01, 526.14it/s]Padding data:  74%|███████▍  | 1521/2042 [00:03<00:01, 498.40it/s]Padding data:  77%|███████▋  | 1572/2042 [00:03<00:00, 492.71it/s]Padding data:  80%|███████▉  | 1627/2042 [00:03<00:00, 506.48it/s]Padding data:  82%|████████▏ | 1681/2042 [00:03<00:00, 515.13it/s]Padding data:  85%|████████▍ | 1733/2042 [00:03<00:00, 502.99it/s]Padding data:  87%|████████▋ | 1784/2042 [00:03<00:00, 500.01it/s]Padding data:  90%|████████▉ | 1835/2042 [00:03<00:00, 490.28it/s]Padding data:  92%|█████████▏| 1885/2042 [00:03<00:00, 483.55it/s]Padding data:  95%|█████████▍| 1934/2042 [00:03<00:00, 475.88it/s]Padding data:  97%|█████████▋| 1983/2042 [00:03<00:00, 479.70it/s]Padding data: 100%|█████████▉| 2038/2042 [00:04<00:00, 497.70it/s]Padding data: 100%|██████████| 2042/2042 [00:04<00:00, 501.17it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 400, 7)]     0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 400, 64)      3648        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 400, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 400, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 400, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 400, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 400, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 400, 64)      512         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 400, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 400, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 400, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 400, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 400, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 400, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 400, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 400, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 400, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 400, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 400, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 400, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 400, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 400, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 400, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 400, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 400, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 400, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 400, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 400, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 400, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 400, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 400, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 400, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 400, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 400, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 400, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 400, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 510,274
Trainable params: 507,714
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4407/4407 - 603s - loss: 0.6051
Epoch 2/100
4407/4407 - 565s - loss: 0.5663
Epoch 3/100
4407/4407 - 566s - loss: 0.5363
Epoch 4/100
4407/4407 - 566s - loss: 0.5118
Epoch 5/100
4407/4407 - 566s - loss: 0.4978
Epoch 6/100
4407/4407 - 566s - loss: 0.4915
Epoch 7/100
4407/4407 - 567s - loss: 0.4845
Epoch 8/100
4407/4407 - 566s - loss: 0.4805
Epoch 9/100
4407/4407 - 566s - loss: 0.4790
Epoch 10/100
4407/4407 - 566s - loss: 0.4791
Epoch 11/100
4407/4407 - 565s - loss: 0.4756
Epoch 12/100
4407/4407 - 564s - loss: 0.4710
Epoch 13/100
4407/4407 - 564s - loss: 0.4698
Epoch 14/100
4407/4407 - 564s - loss: 0.4666
Epoch 15/100
4407/4407 - 565s - loss: 0.4661
Epoch 16/100
4407/4407 - 565s - loss: 0.4645
Epoch 17/100
4407/4407 - 564s - loss: 0.4626
Epoch 18/100
4407/4407 - 565s - loss: 0.4588
Epoch 19/100
4407/4407 - 564s - loss: 0.4545
Epoch 20/100
4407/4407 - 565s - loss: 0.4582
Epoch 21/100
4407/4407 - 564s - loss: 0.4552
Epoch 22/100
4407/4407 - 564s - loss: 0.4518
Epoch 23/100
4407/4407 - 565s - loss: 0.4485
Epoch 24/100
4407/4407 - 564s - loss: 0.4482
Epoch 25/100
4407/4407 - 564s - loss: 0.4479
Epoch 26/100
4407/4407 - 564s - loss: 0.4454
Epoch 27/100
4407/4407 - 565s - loss: 0.4428
Epoch 28/100
4407/4407 - 564s - loss: 0.4438
Epoch 29/100
4407/4407 - 564s - loss: 0.4410
Epoch 30/100
4407/4407 - 564s - loss: 0.4408
Epoch 31/100
4407/4407 - 565s - loss: 0.4369
Epoch 32/100
4407/4407 - 565s - loss: 0.4350
Epoch 33/100
4407/4407 - 564s - loss: 0.4281
Epoch 34/100
4407/4407 - 565s - loss: 0.4281
Epoch 35/100
4407/4407 - 564s - loss: 0.4274
Epoch 36/100
4407/4407 - 564s - loss: 0.4243
Epoch 37/100
4407/4407 - 565s - loss: 0.4217
Epoch 38/100
4407/4407 - 565s - loss: 0.4216
Epoch 39/100
4407/4407 - 565s - loss: 0.4184
Epoch 40/100
4407/4407 - 565s - loss: 0.4141
Epoch 41/100
4407/4407 - 565s - loss: 0.4139
Epoch 42/100
4407/4407 - 564s - loss: 0.4153
Epoch 43/100
4407/4407 - 565s - loss: 0.4103
Epoch 44/100
4407/4407 - 565s - loss: 0.4038
Epoch 45/100
4407/4407 - 564s - loss: 0.4017
Epoch 46/100
4407/4407 - 564s - loss: 0.4000
Epoch 47/100
4407/4407 - 565s - loss: 0.3975
Epoch 48/100
4407/4407 - 565s - loss: 0.3908
Epoch 49/100
4407/4407 - 565s - loss: 0.3915
Epoch 50/100
4407/4407 - 566s - loss: 0.3878
Epoch 51/100
4407/4407 - 564s - loss: 0.3871
Epoch 52/100
4407/4407 - 564s - loss: 0.3834
Epoch 53/100
4407/4407 - 564s - loss: 0.3780
Epoch 54/100
4407/4407 - 564s - loss: 0.3710
Epoch 55/100
4407/4407 - 564s - loss: 0.3679
Epoch 56/100
4407/4407 - 564s - loss: 0.3677
Epoch 57/100
4407/4407 - 564s - loss: 0.3656
Epoch 58/100
4407/4407 - 563s - loss: 0.3584
Epoch 59/100
4407/4407 - 564s - loss: 0.3578
Epoch 60/100
4407/4407 - 563s - loss: 0.3572
Epoch 61/100
4407/4407 - 563s - loss: 0.3512
Epoch 62/100
4407/4407 - 563s - loss: 0.3461
Epoch 63/100
4407/4407 - 564s - loss: 0.3393
Epoch 64/100
4407/4407 - 564s - loss: 0.3364
Epoch 65/100
4407/4407 - 565s - loss: 0.3363
Epoch 66/100
4407/4407 - 565s - loss: 0.3347
Epoch 67/100
4407/4407 - 564s - loss: 0.3278
Epoch 68/100
4407/4407 - 564s - loss: 0.3240
Epoch 69/100
4407/4407 - 565s - loss: 0.3210
Epoch 70/100
4407/4407 - 565s - loss: 0.3129
Epoch 71/100
4407/4407 - 564s - loss: 0.3144
Epoch 72/100
4407/4407 - 564s - loss: 0.3050
Epoch 73/100
4407/4407 - 564s - loss: 0.3097
Epoch 74/100
4407/4407 - 564s - loss: 0.3023
Epoch 75/100
4407/4407 - 564s - loss: 0.2949
Epoch 76/100
4407/4407 - 564s - loss: 0.2963
Epoch 77/100
4407/4407 - 564s - loss: 0.2915
Epoch 78/100
4407/4407 - 563s - loss: 0.2867
Epoch 79/100
4407/4407 - 564s - loss: 0.2766
Epoch 80/100
4407/4407 - 562s - loss: 0.2795
Epoch 81/100
4407/4407 - 564s - loss: 0.2756
Epoch 82/100
4407/4407 - 564s - loss: 0.2705
Epoch 83/100
4407/4407 - 564s - loss: 0.2580
Epoch 84/100
4407/4407 - 563s - loss: 0.2618
Epoch 85/100
4407/4407 - 564s - loss: 0.2576
Epoch 86/100
4407/4407 - 564s - loss: 0.2559
Epoch 87/100
4407/4407 - 564s - loss: 0.2465
Epoch 88/100
4407/4407 - 564s - loss: 0.2503
Epoch 89/100
4407/4407 - 564s - loss: 0.2443
Epoch 90/100
4407/4407 - 563s - loss: 0.2412
Epoch 91/100
4407/4407 - 564s - loss: 0.2366
Epoch 92/100
4407/4407 - 564s - loss: 0.2343
Epoch 93/100
4407/4407 - 564s - loss: 0.2300
Epoch 94/100
4407/4407 - 564s - loss: 0.2220
Epoch 95/100
4407/4407 - 564s - loss: 0.2249
Epoch 96/100
4407/4407 - 563s - loss: 0.2198
Epoch 97/100
4407/4407 - 564s - loss: 0.2183
Epoch 98/100
4407/4407 - 564s - loss: 0.2119
Epoch 99/100
4407/4407 - 564s - loss: 0.2057
Epoch 100/100
4407/4407 - 563s - loss: 0.2122
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.4983733118238246
Test f1_avg: 0.49850259725917356
