INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 34/2042 [00:00<00:05, 334.97it/s]Padding data:   3%|▎         | 68/2042 [00:00<00:05, 337.48it/s]Padding data:   5%|▌         | 109/2042 [00:00<00:05, 367.33it/s]Padding data:   7%|▋         | 148/2042 [00:00<00:05, 375.63it/s]Padding data:   9%|▉         | 188/2042 [00:00<00:04, 381.72it/s]Padding data:  11%|█         | 227/2042 [00:00<00:04, 372.49it/s]Padding data:  13%|█▎        | 267/2042 [00:00<00:04, 380.35it/s]Padding data:  15%|█▍        | 306/2042 [00:00<00:04, 380.15it/s]Padding data:  17%|█▋        | 345/2042 [00:00<00:04, 381.11it/s]Padding data:  19%|█▉        | 386/2042 [00:01<00:04, 388.71it/s]Padding data:  21%|██        | 428/2042 [00:01<00:04, 396.15it/s]Padding data:  23%|██▎       | 468/2042 [00:01<00:04, 389.71it/s]Padding data:  25%|██▍       | 508/2042 [00:01<00:03, 390.61it/s]Padding data:  27%|██▋       | 548/2042 [00:01<00:03, 392.59it/s]Padding data:  29%|██▉       | 588/2042 [00:01<00:03, 384.69it/s]Padding data:  31%|███       | 627/2042 [00:01<00:03, 383.24it/s]Padding data:  33%|███▎      | 666/2042 [00:01<00:03, 371.11it/s]Padding data:  34%|███▍      | 704/2042 [00:01<00:03, 367.89it/s]Padding data:  36%|███▋      | 741/2042 [00:01<00:03, 357.65it/s]Padding data:  38%|███▊      | 778/2042 [00:02<00:03, 359.88it/s]Padding data:  40%|███▉      | 815/2042 [00:02<00:03, 356.13it/s]Padding data:  42%|████▏     | 857/2042 [00:02<00:03, 373.93it/s]Padding data:  44%|████▍     | 896/2042 [00:02<00:03, 378.42it/s]Padding data:  46%|████▌     | 934/2042 [00:02<00:02, 376.58it/s]Padding data:  48%|████▊     | 972/2042 [00:02<00:02, 369.02it/s]Padding data:  49%|████▉     | 1009/2042 [00:02<00:02, 368.71it/s]Padding data:  51%|█████▏    | 1047/2042 [00:02<00:02, 368.74it/s]Padding data:  53%|█████▎    | 1084/2042 [00:02<00:02, 366.25it/s]Padding data:  55%|█████▍    | 1121/2042 [00:03<00:02, 357.86it/s]Padding data:  57%|█████▋    | 1161/2042 [00:03<00:02, 369.93it/s]Padding data:  59%|█████▊    | 1199/2042 [00:03<00:02, 371.57it/s]Padding data:  61%|██████    | 1237/2042 [00:03<00:02, 373.58it/s]Padding data:  63%|██████▎   | 1282/2042 [00:03<00:01, 395.92it/s]Padding data:  65%|██████▍   | 1324/2042 [00:03<00:01, 401.66it/s]Padding data:  67%|██████▋   | 1365/2042 [00:03<00:01, 389.02it/s]Padding data:  69%|██████▉   | 1410/2042 [00:03<00:01, 405.61it/s]Padding data:  71%|███████   | 1451/2042 [00:03<00:01, 404.86it/s]Padding data:  73%|███████▎  | 1492/2042 [00:03<00:01, 388.78it/s]Padding data:  75%|███████▌  | 1532/2042 [00:04<00:01, 376.98it/s]Padding data:  77%|███████▋  | 1570/2042 [00:04<00:01, 370.25it/s]Padding data:  79%|███████▉  | 1609/2042 [00:04<00:01, 373.25it/s]Padding data:  81%|████████  | 1648/2042 [00:04<00:01, 372.11it/s]Padding data:  83%|████████▎ | 1686/2042 [00:04<00:00, 371.49it/s]Padding data:  85%|████████▍ | 1727/2042 [00:04<00:00, 381.31it/s]Padding data:  87%|████████▋ | 1767/2042 [00:04<00:00, 384.24it/s]Padding data:  88%|████████▊ | 1806/2042 [00:04<00:00, 362.74it/s]Padding data:  90%|█████████ | 1843/2042 [00:04<00:00, 362.89it/s]Padding data:  92%|█████████▏| 1880/2042 [00:05<00:00, 359.93it/s]Padding data:  94%|█████████▍| 1917/2042 [00:05<00:00, 362.49it/s]Padding data:  96%|█████████▌| 1962/2042 [00:05<00:00, 385.41it/s]Padding data:  98%|█████████▊| 2001/2042 [00:05<00:00, 374.25it/s]Padding data: 100%|█████████▉| 2039/2042 [00:05<00:00, 373.63it/s]Padding data: 100%|██████████| 2042/2042 [00:05<00:00, 376.03it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 400, 7)]          0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            48635     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 56,094
Trainable params: 56,094
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4407/4407 - 36s - loss: 0.5967
Epoch 2/100
4407/4407 - 34s - loss: 0.5678
Epoch 3/100
4407/4407 - 34s - loss: 0.5533
Epoch 4/100
4407/4407 - 34s - loss: 0.5432
Epoch 5/100
4407/4407 - 34s - loss: 0.5340
Epoch 6/100
4407/4407 - 34s - loss: 0.5289
Epoch 7/100
4407/4407 - 34s - loss: 0.5228
Epoch 8/100
4407/4407 - 34s - loss: 0.5173
Epoch 9/100
4407/4407 - 34s - loss: 0.5105
Epoch 10/100
4407/4407 - 34s - loss: 0.5056
Epoch 11/100
4407/4407 - 36s - loss: 0.5026
Epoch 12/100
4407/4407 - 36s - loss: 0.4977
Epoch 13/100
4407/4407 - 32s - loss: 0.4964
Epoch 14/100
4407/4407 - 35s - loss: 0.4951
Epoch 15/100
4407/4407 - 35s - loss: 0.4885
Epoch 16/100
4407/4407 - 36s - loss: 0.4913
Epoch 17/100
4407/4407 - 37s - loss: 0.4875
Epoch 18/100
4407/4407 - 33s - loss: 0.4883
Epoch 19/100
4407/4407 - 34s - loss: 0.4856
Epoch 20/100
4407/4407 - 36s - loss: 0.4863
Epoch 21/100
4407/4407 - 35s - loss: 0.4836
Epoch 22/100
4407/4407 - 37s - loss: 0.4826
Epoch 23/100
4407/4407 - 35s - loss: 0.4837
Epoch 24/100
4407/4407 - 37s - loss: 0.4802
Epoch 25/100
4407/4407 - 32s - loss: 0.4807
Epoch 26/100
4407/4407 - 35s - loss: 0.4799
Epoch 27/100
4407/4407 - 32s - loss: 0.4801
Epoch 28/100
4407/4407 - 34s - loss: 0.4802
Epoch 29/100
4407/4407 - 35s - loss: 0.4787
Epoch 30/100
4407/4407 - 32s - loss: 0.4790
Epoch 31/100
4407/4407 - 32s - loss: 0.4775
Epoch 32/100
4407/4407 - 33s - loss: 0.4783
Epoch 33/100
4407/4407 - 36s - loss: 0.4772
Epoch 34/100
4407/4407 - 36s - loss: 0.4751
Epoch 35/100
4407/4407 - 32s - loss: 0.4757
Epoch 36/100
4407/4407 - 32s - loss: 0.4761
Epoch 37/100
4407/4407 - 36s - loss: 0.4743
Epoch 38/100
4407/4407 - 32s - loss: 0.4725
Epoch 39/100
4407/4407 - 32s - loss: 0.4753
Epoch 40/100
4407/4407 - 37s - loss: 0.4729
Epoch 41/100
4407/4407 - 33s - loss: 0.4742
Epoch 42/100
4407/4407 - 34s - loss: 0.4728

Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 43/100
4407/4407 - 32s - loss: 0.4657
Epoch 44/100
4407/4407 - 36s - loss: 0.4654
Epoch 45/100
4407/4407 - 33s - loss: 0.4647
Epoch 46/100
4407/4407 - 34s - loss: 0.4636
Epoch 47/100
4407/4407 - 36s - loss: 0.4630
Epoch 48/100
4407/4407 - 32s - loss: 0.4644
Epoch 49/100
4407/4407 - 32s - loss: 0.4623
Epoch 50/100
4407/4407 - 33s - loss: 0.4632
Epoch 51/100
4407/4407 - 32s - loss: 0.4628
Epoch 52/100
4407/4407 - 36s - loss: 0.4637
Epoch 53/100
4407/4407 - 32s - loss: 0.4634

Epoch 00053: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 54/100
4407/4407 - 34s - loss: 0.4586
Epoch 55/100
4407/4407 - 36s - loss: 0.4591
Epoch 56/100
4407/4407 - 36s - loss: 0.4579
Epoch 57/100
4407/4407 - 36s - loss: 0.4584
Epoch 58/100
4407/4407 - 32s - loss: 0.4582
Epoch 59/100
4407/4407 - 33s - loss: 0.4582
Epoch 60/100
4407/4407 - 32s - loss: 0.4580

Epoch 00060: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 61/100
4407/4407 - 31s - loss: 0.4556
Epoch 62/100
4407/4407 - 31s - loss: 0.4554
Epoch 63/100
4407/4407 - 32s - loss: 0.4556
Epoch 64/100
4407/4407 - 31s - loss: 0.4553
Epoch 65/100
4407/4407 - 30s - loss: 0.4552
Epoch 66/100
4407/4407 - 34s - loss: 0.4550
Epoch 67/100
4407/4407 - 32s - loss: 0.4552
Epoch 68/100
4407/4407 - 31s - loss: 0.4553
Epoch 69/100
4407/4407 - 32s - loss: 0.4550
Epoch 70/100
4407/4407 - 35s - loss: 0.4549
Epoch 71/100
4407/4407 - 30s - loss: 0.4551
Epoch 72/100
4407/4407 - 32s - loss: 0.4549
Epoch 73/100
4407/4407 - 30s - loss: 0.4548
Epoch 74/100
4407/4407 - 34s - loss: 0.4544
Epoch 75/100
4407/4407 - 30s - loss: 0.4545
Epoch 76/100
4407/4407 - 30s - loss: 0.4545
Epoch 77/100
4407/4407 - 31s - loss: 0.4544
Epoch 78/100
4407/4407 - 35s - loss: 0.4541
Epoch 79/100
4407/4407 - 35s - loss: 0.4542
Epoch 80/100
4407/4407 - 31s - loss: 0.4542
Epoch 81/100
4407/4407 - 31s - loss: 0.4545
Epoch 82/100
4407/4407 - 34s - loss: 0.4542

Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 83/100
4407/4407 - 34s - loss: 0.4537
Epoch 84/100
4407/4407 - 32s - loss: 0.4536
Epoch 85/100
4407/4407 - 33s - loss: 0.4538
Epoch 86/100
4407/4407 - 32s - loss: 0.4533
Epoch 87/100
4407/4407 - 31s - loss: 0.4537
Epoch 88/100
4407/4407 - 31s - loss: 0.4533
Epoch 89/100
4407/4407 - 35s - loss: 0.4536
Epoch 90/100
4407/4407 - 32s - loss: 0.4530
Epoch 91/100
4407/4407 - 35s - loss: 0.4534
Epoch 92/100
4407/4407 - 33s - loss: 0.4535
Epoch 93/100
4407/4407 - 36s - loss: 0.4529
Epoch 94/100
4407/4407 - 35s - loss: 0.4531
Epoch 95/100
4407/4407 - 32s - loss: 0.4529
Epoch 96/100
4407/4407 - 31s - loss: 0.4530
Epoch 97/100
4407/4407 - 34s - loss: 0.4533
Epoch 98/100
4407/4407 - 34s - loss: 0.4529
Epoch 99/100
4407/4407 - 33s - loss: 0.4531
Epoch 100/100
4407/4407 - 32s - loss: 0.4525
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.6922849893701971
Test f1_avg: 0.6759273093395768
