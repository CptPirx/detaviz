INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 53/2042 [00:00<00:03, 526.58it/s]Padding data:   5%|▌         | 106/2042 [00:00<00:03, 527.96it/s]Padding data:   8%|▊         | 159/2042 [00:00<00:03, 524.79it/s]Padding data:  10%|█         | 212/2042 [00:00<00:03, 526.10it/s]Padding data:  13%|█▎        | 265/2042 [00:00<00:03, 525.49it/s]Padding data:  16%|█▌        | 319/2042 [00:00<00:03, 527.05it/s]Padding data:  18%|█▊        | 372/2042 [00:00<00:03, 527.90it/s]Padding data:  21%|██        | 426/2042 [00:00<00:03, 530.82it/s]Padding data:  24%|██▎       | 480/2042 [00:00<00:02, 533.03it/s]Padding data:  26%|██▌       | 534/2042 [00:01<00:02, 533.80it/s]Padding data:  29%|██▉       | 588/2042 [00:01<00:02, 535.30it/s]Padding data:  31%|███▏      | 643/2042 [00:01<00:02, 537.28it/s]Padding data:  34%|███▍      | 698/2042 [00:01<00:02, 538.85it/s]Padding data:  37%|███▋      | 752/2042 [00:01<00:02, 538.47it/s]Padding data:  40%|███▉      | 807/2042 [00:01<00:02, 539.36it/s]Padding data:  42%|████▏     | 862/2042 [00:01<00:02, 539.94it/s]Padding data:  45%|████▍     | 917/2042 [00:01<00:02, 541.64it/s]Padding data:  48%|████▊     | 972/2042 [00:01<00:01, 541.60it/s]Padding data:  50%|█████     | 1027/2042 [00:01<00:01, 543.65it/s]Padding data:  53%|█████▎    | 1082/2042 [00:02<00:01, 541.46it/s]Padding data:  56%|█████▌    | 1138/2042 [00:02<00:01, 544.46it/s]Padding data:  58%|█████▊    | 1193/2042 [00:02<00:01, 545.87it/s]Padding data:  61%|██████    | 1248/2042 [00:02<00:01, 546.50it/s]Padding data:  64%|██████▍   | 1303/2042 [00:02<00:01, 546.04it/s]Padding data:  67%|██████▋   | 1358/2042 [00:02<00:01, 545.02it/s]Padding data:  69%|██████▉   | 1413/2042 [00:02<00:01, 545.09it/s]Padding data:  72%|███████▏  | 1468/2042 [00:02<00:01, 544.30it/s]Padding data:  75%|███████▍  | 1524/2042 [00:02<00:00, 546.63it/s]Padding data:  77%|███████▋  | 1579/2042 [00:02<00:00, 543.68it/s]Padding data:  80%|████████  | 1634/2042 [00:03<00:00, 545.08it/s]Padding data:  83%|████████▎ | 1690/2042 [00:03<00:00, 547.52it/s]Padding data:  85%|████████▌ | 1745/2042 [00:03<00:00, 546.14it/s]Padding data:  88%|████████▊ | 1800/2042 [00:03<00:00, 545.53it/s]Padding data:  91%|█████████ | 1855/2042 [00:03<00:00, 544.54it/s]Padding data:  94%|█████████▎| 1911/2042 [00:03<00:00, 546.58it/s]Padding data:  96%|█████████▋| 1966/2042 [00:03<00:00, 546.59it/s]Padding data:  99%|█████████▉| 2021/2042 [00:03<00:00, 545.59it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 540.52it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 100)]        0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            13100     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,559
Trainable params: 20,559
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 37s - loss: 0.6377
Epoch 2/100
4383/4383 - 36s - loss: 0.5917
Epoch 3/100
4383/4383 - 36s - loss: 0.5775
Epoch 4/100
4383/4383 - 36s - loss: 0.5665
Epoch 5/100
4383/4383 - 36s - loss: 0.5553
Epoch 6/100
4383/4383 - 36s - loss: 0.5473
Epoch 7/100
4383/4383 - 36s - loss: 0.5412
Epoch 8/100
4383/4383 - 37s - loss: 0.5357
Epoch 9/100
4383/4383 - 36s - loss: 0.5300
Epoch 10/100
4383/4383 - 36s - loss: 0.5277
Epoch 11/100
4383/4383 - 36s - loss: 0.5228
Epoch 12/100
4383/4383 - 36s - loss: 0.5211
Epoch 13/100
4383/4383 - 36s - loss: 0.5171
Epoch 14/100
4383/4383 - 36s - loss: 0.5146
Epoch 15/100
4383/4383 - 36s - loss: 0.5129
Epoch 16/100
4383/4383 - 36s - loss: 0.5108
Epoch 17/100
4383/4383 - 36s - loss: 0.5087
Epoch 18/100
4383/4383 - 36s - loss: 0.5072
Epoch 19/100
4383/4383 - 36s - loss: 0.5030
Epoch 20/100
4383/4383 - 36s - loss: 0.5044
Epoch 21/100
4383/4383 - 36s - loss: 0.5023
Epoch 22/100
4383/4383 - 35s - loss: 0.5024
Epoch 23/100
4383/4383 - 35s - loss: 0.4990
Epoch 24/100
4383/4383 - 36s - loss: 0.4997
Epoch 25/100
4383/4383 - 36s - loss: 0.4986
Epoch 26/100
4383/4383 - 36s - loss: 0.4956
Epoch 27/100
4383/4383 - 36s - loss: 0.4964
Epoch 28/100
4383/4383 - 36s - loss: 0.4941
Epoch 29/100
4383/4383 - 37s - loss: 0.4937
Epoch 30/100
4383/4383 - 36s - loss: 0.4910
Epoch 31/100
4383/4383 - 36s - loss: 0.4914
Epoch 32/100
4383/4383 - 36s - loss: 0.4887
Epoch 33/100
4383/4383 - 36s - loss: 0.4878
Epoch 34/100
4383/4383 - 36s - loss: 0.4902
Epoch 35/100
4383/4383 - 36s - loss: 0.4872
Epoch 36/100
4383/4383 - 36s - loss: 0.4869
Epoch 37/100
4383/4383 - 36s - loss: 0.4872
Epoch 38/100
4383/4383 - 36s - loss: 0.4842
Epoch 39/100
4383/4383 - 36s - loss: 0.4850
Epoch 40/100
4383/4383 - 37s - loss: 0.4858
Epoch 41/100
4383/4383 - 36s - loss: 0.4818
Epoch 42/100
4383/4383 - 36s - loss: 0.4808
Epoch 43/100
4383/4383 - 36s - loss: 0.4805
Epoch 44/100
4383/4383 - 36s - loss: 0.4789
Epoch 45/100
4383/4383 - 36s - loss: 0.4796
Epoch 46/100
4383/4383 - 36s - loss: 0.4773
Epoch 47/100
4383/4383 - 36s - loss: 0.4800
Epoch 48/100
4383/4383 - 36s - loss: 0.4756
Epoch 49/100
4383/4383 - 36s - loss: 0.4756
Epoch 50/100
4383/4383 - 36s - loss: 0.4766
Epoch 51/100
4383/4383 - 37s - loss: 0.4743
Epoch 52/100
4383/4383 - 36s - loss: 0.4736
Epoch 53/100
4383/4383 - 36s - loss: 0.4743
Epoch 54/100
4383/4383 - 36s - loss: 0.4773
Epoch 55/100
4383/4383 - 36s - loss: 0.4738
Epoch 56/100
4383/4383 - 36s - loss: 0.4723
Epoch 57/100
4383/4383 - 36s - loss: 0.4700
Epoch 58/100
4383/4383 - 36s - loss: 0.4717
Epoch 59/100
4383/4383 - 36s - loss: 0.4699
Epoch 60/100
4383/4383 - 36s - loss: 0.4702
Epoch 61/100
4383/4383 - 36s - loss: 0.4699
Epoch 62/100
4383/4383 - 36s - loss: 0.4686
Epoch 63/100
4383/4383 - 36s - loss: 0.4689
Epoch 64/100
4383/4383 - 36s - loss: 0.4683
Epoch 65/100
4383/4383 - 36s - loss: 0.4700
Epoch 66/100
4383/4383 - 36s - loss: 0.4664
Epoch 67/100
4383/4383 - 36s - loss: 0.4694
Epoch 68/100
4383/4383 - 36s - loss: 0.4658
Epoch 69/100
4383/4383 - 36s - loss: 0.4682
Epoch 70/100
4383/4383 - 36s - loss: 0.4674
Epoch 71/100
4383/4383 - 36s - loss: 0.4673
Epoch 72/100
4383/4383 - 36s - loss: 0.4651
Epoch 73/100
4383/4383 - 36s - loss: 0.4659
Epoch 74/100
4383/4383 - 36s - loss: 0.4653
Epoch 75/100
4383/4383 - 36s - loss: 0.4678
Epoch 76/100
4383/4383 - 36s - loss: 0.4646
Epoch 77/100
4383/4383 - 36s - loss: 0.4638
Epoch 78/100
4383/4383 - 36s - loss: 0.4646
Epoch 79/100
4383/4383 - 36s - loss: 0.4621
Epoch 80/100
4383/4383 - 36s - loss: 0.4621
Epoch 81/100
4383/4383 - 36s - loss: 0.4624
Epoch 82/100
4383/4383 - 36s - loss: 0.4638
Epoch 83/100
4383/4383 - 36s - loss: 0.4613
Epoch 84/100
4383/4383 - 36s - loss: 0.4615
Epoch 85/100
4383/4383 - 36s - loss: 0.4616
Epoch 86/100
4383/4383 - 36s - loss: 0.4611
Epoch 87/100
4383/4383 - 36s - loss: 0.4610
Epoch 88/100
4383/4383 - 36s - loss: 0.4609
Epoch 89/100
4383/4383 - 36s - loss: 0.4608
Epoch 90/100
4383/4383 - 36s - loss: 0.4586
Epoch 91/100
4383/4383 - 36s - loss: 0.4611
Epoch 92/100
4383/4383 - 36s - loss: 0.4610
Epoch 93/100
4383/4383 - 36s - loss: 0.4589
Epoch 94/100
4383/4383 - 36s - loss: 0.4579
Epoch 95/100
4383/4383 - 36s - loss: 0.4601
Epoch 96/100
4383/4383 - 36s - loss: 0.4600
Epoch 97/100
4383/4383 - 36s - loss: 0.4578
Epoch 98/100
4383/4383 - 36s - loss: 0.4607

Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 99/100
4383/4383 - 36s - loss: 0.4435
Epoch 100/100
4383/4383 - 36s - loss: 0.4416
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.701910056659205
Test f1_avg: 0.6629217337821125
