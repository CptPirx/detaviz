INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 66/2042 [00:00<00:03, 655.30it/s]Padding data:   6%|▋         | 132/2042 [00:00<00:02, 657.20it/s]Padding data:  10%|▉         | 198/2042 [00:00<00:02, 654.83it/s]Padding data:  13%|█▎        | 265/2042 [00:00<00:02, 657.52it/s]Padding data:  16%|█▌        | 331/2042 [00:00<00:02, 657.18it/s]Padding data:  20%|█▉        | 400/2042 [00:00<00:02, 664.92it/s]Padding data:  23%|██▎       | 467/2042 [00:00<00:02, 666.00it/s]Padding data:  26%|██▌       | 534/2042 [00:00<00:02, 666.38it/s]Padding data:  29%|██▉       | 601/2042 [00:00<00:02, 665.08it/s]Padding data:  33%|███▎      | 668/2042 [00:01<00:02, 664.14it/s]Padding data:  36%|███▌      | 735/2042 [00:01<00:01, 661.82it/s]Padding data:  39%|███▉      | 802/2042 [00:01<00:01, 661.74it/s]Padding data:  43%|████▎     | 869/2042 [00:01<00:01, 661.65it/s]Padding data:  46%|████▌     | 938/2042 [00:01<00:01, 667.27it/s]Padding data:  49%|████▉     | 1007/2042 [00:01<00:01, 672.62it/s]Padding data:  53%|█████▎    | 1076/2042 [00:01<00:01, 676.10it/s]Padding data:  56%|█████▌    | 1144/2042 [00:01<00:01, 669.73it/s]Padding data:  59%|█████▉    | 1212/2042 [00:01<00:01, 670.51it/s]Padding data:  63%|██████▎   | 1280/2042 [00:01<00:01, 670.26it/s]Padding data:  66%|██████▌   | 1348/2042 [00:02<00:01, 667.37it/s]Padding data:  69%|██████▉   | 1416/2042 [00:02<00:00, 670.47it/s]Padding data:  73%|███████▎  | 1484/2042 [00:02<00:00, 672.70it/s]Padding data:  76%|███████▌  | 1553/2042 [00:02<00:00, 675.28it/s]Padding data:  79%|███████▉  | 1622/2042 [00:02<00:00, 677.24it/s]Padding data:  83%|████████▎ | 1692/2042 [00:02<00:00, 682.25it/s]Padding data:  86%|████████▌ | 1761/2042 [00:02<00:00, 673.78it/s]Padding data:  90%|████████▉ | 1829/2042 [00:02<00:00, 674.25it/s]Padding data:  93%|█████████▎| 1898/2042 [00:02<00:00, 676.27it/s]Padding data:  96%|█████████▋| 1966/2042 [00:02<00:00, 675.93it/s]Padding data: 100%|█████████▉| 2034/2042 [00:03<00:00, 676.29it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 669.14it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 40)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            12800     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,259
Trainable params: 20,259
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 21s - loss: 0.6643
Epoch 2/100
4383/4383 - 20s - loss: 0.6103
Epoch 3/100
4383/4383 - 20s - loss: 0.5995
Epoch 4/100
4383/4383 - 20s - loss: 0.5950
Epoch 5/100
4383/4383 - 20s - loss: 0.5921
Epoch 6/100
4383/4383 - 20s - loss: 0.5888
Epoch 7/100
4383/4383 - 20s - loss: 0.5846
Epoch 8/100
4383/4383 - 20s - loss: 0.5825
Epoch 9/100
4383/4383 - 20s - loss: 0.5786
Epoch 10/100
4383/4383 - 20s - loss: 0.5783
Epoch 11/100
4383/4383 - 20s - loss: 0.5757
Epoch 12/100
4383/4383 - 20s - loss: 0.5743
Epoch 13/100
4383/4383 - 20s - loss: 0.5749
Epoch 14/100
4383/4383 - 20s - loss: 0.5730
Epoch 15/100
4383/4383 - 20s - loss: 0.5687
Epoch 16/100
4383/4383 - 20s - loss: 0.5676
Epoch 17/100
4383/4383 - 20s - loss: 0.5668
Epoch 18/100
4383/4383 - 20s - loss: 0.5617
Epoch 19/100
4383/4383 - 20s - loss: 0.5605
Epoch 20/100
4383/4383 - 20s - loss: 0.5592
Epoch 21/100
4383/4383 - 20s - loss: 0.5557
Epoch 22/100
4383/4383 - 20s - loss: 0.5550
Epoch 23/100
4383/4383 - 20s - loss: 0.5534
Epoch 24/100
4383/4383 - 20s - loss: 0.5520
Epoch 25/100
4383/4383 - 20s - loss: 0.5522
Epoch 26/100
4383/4383 - 20s - loss: 0.5504
Epoch 27/100
4383/4383 - 20s - loss: 0.5489
Epoch 28/100
4383/4383 - 20s - loss: 0.5474
Epoch 29/100
4383/4383 - 20s - loss: 0.5475
Epoch 30/100
4383/4383 - 20s - loss: 0.5472
Epoch 31/100
4383/4383 - 20s - loss: 0.5465
Epoch 32/100
4383/4383 - 20s - loss: 0.5453
Epoch 33/100
4383/4383 - 20s - loss: 0.5434
Epoch 34/100
4383/4383 - 20s - loss: 0.5443
Epoch 35/100
4383/4383 - 20s - loss: 0.5453
Epoch 36/100
4383/4383 - 21s - loss: 0.5423
Epoch 37/100
4383/4383 - 20s - loss: 0.5453
Epoch 38/100
4383/4383 - 20s - loss: 0.5422
Epoch 39/100
4383/4383 - 20s - loss: 0.5421
Epoch 40/100
4383/4383 - 20s - loss: 0.5403
Epoch 41/100
4383/4383 - 20s - loss: 0.5404
Epoch 42/100
4383/4383 - 20s - loss: 0.5385
Epoch 43/100
4383/4383 - 20s - loss: 0.5392
Epoch 44/100
4383/4383 - 20s - loss: 0.5403
Epoch 45/100
4383/4383 - 20s - loss: 0.5387
Epoch 46/100
4383/4383 - 20s - loss: 0.5375
Epoch 47/100
4383/4383 - 20s - loss: 0.5352
Epoch 48/100
4383/4383 - 20s - loss: 0.5395
Epoch 49/100
4383/4383 - 20s - loss: 0.5357
Epoch 50/100
4383/4383 - 20s - loss: 0.5362
Epoch 51/100
4383/4383 - 20s - loss: 0.5362

Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 52/100
4383/4383 - 20s - loss: 0.5238
Epoch 53/100
4383/4383 - 20s - loss: 0.5232
Epoch 54/100
4383/4383 - 20s - loss: 0.5222
Epoch 55/100
4383/4383 - 20s - loss: 0.5225
Epoch 56/100
4383/4383 - 20s - loss: 0.5207
Epoch 57/100
4383/4383 - 20s - loss: 0.5215
Epoch 58/100
4383/4383 - 20s - loss: 0.5210
Epoch 59/100
4383/4383 - 20s - loss: 0.5202
Epoch 60/100
4383/4383 - 20s - loss: 0.5196
Epoch 61/100
4383/4383 - 20s - loss: 0.5180
Epoch 62/100
4383/4383 - 20s - loss: 0.5187
Epoch 63/100
4383/4383 - 20s - loss: 0.5191
Epoch 64/100
4383/4383 - 20s - loss: 0.5176
Epoch 65/100
4383/4383 - 20s - loss: 0.5173
Epoch 66/100
4383/4383 - 20s - loss: 0.5168
Epoch 67/100
4383/4383 - 20s - loss: 0.5153
Epoch 68/100
4383/4383 - 20s - loss: 0.5153
Epoch 69/100
4383/4383 - 20s - loss: 0.5153
Epoch 70/100
4383/4383 - 20s - loss: 0.5155
Epoch 71/100
4383/4383 - 20s - loss: 0.5154

Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 72/100
4383/4383 - 20s - loss: 0.5074
Epoch 73/100
4383/4383 - 20s - loss: 0.5066
Epoch 74/100
4383/4383 - 22s - loss: 0.5072
Epoch 75/100
4383/4383 - 20s - loss: 0.5048
Epoch 76/100
4383/4383 - 20s - loss: 0.5048
Epoch 77/100
4383/4383 - 20s - loss: 0.5060
Epoch 78/100
4383/4383 - 20s - loss: 0.5050
Epoch 79/100
4383/4383 - 20s - loss: 0.5036
Epoch 80/100
4383/4383 - 20s - loss: 0.5035
Epoch 81/100
4383/4383 - 20s - loss: 0.5034
Epoch 82/100
4383/4383 - 20s - loss: 0.5037
Epoch 83/100
4383/4383 - 20s - loss: 0.5043
Epoch 84/100
4383/4383 - 20s - loss: 0.5033
Epoch 85/100
4383/4383 - 20s - loss: 0.5031
Epoch 86/100
4383/4383 - 20s - loss: 0.5026
Epoch 87/100
4383/4383 - 21s - loss: 0.5035
Epoch 88/100
4383/4383 - 21s - loss: 0.5030
Epoch 89/100
4383/4383 - 20s - loss: 0.5022
Epoch 90/100
4383/4383 - 22s - loss: 0.5026
Epoch 91/100
4383/4383 - 20s - loss: 0.5018
Epoch 92/100
4383/4383 - 20s - loss: 0.5003
Epoch 93/100
4383/4383 - 20s - loss: 0.5003
Epoch 94/100
4383/4383 - 20s - loss: 0.5007
Epoch 95/100
4383/4383 - 20s - loss: 0.5009
Epoch 96/100
4383/4383 - 20s - loss: 0.5010

Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 97/100
4383/4383 - 20s - loss: 0.4963
Epoch 98/100
4383/4383 - 20s - loss: 0.4962
Epoch 99/100
4383/4383 - 20s - loss: 0.4963
Epoch 100/100
4383/4383 - 20s - loss: 0.4951
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.6787192896725567
Test f1_avg: 0.6474772573283911
