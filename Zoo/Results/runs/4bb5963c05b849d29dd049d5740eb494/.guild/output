INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 43/2042 [00:00<00:04, 423.82it/s]Padding data:   4%|▍         | 87/2042 [00:00<00:04, 427.72it/s]Padding data:   6%|▋         | 130/2042 [00:00<00:04, 416.87it/s]Padding data:   8%|▊         | 172/2042 [00:00<00:04, 397.13it/s]Padding data:  10%|█         | 212/2042 [00:00<00:04, 388.74it/s]Padding data:  12%|█▏        | 251/2042 [00:00<00:04, 386.09it/s]Padding data:  14%|█▍        | 290/2042 [00:00<00:04, 381.24it/s]Padding data:  16%|█▌        | 329/2042 [00:00<00:04, 382.11it/s]Padding data:  18%|█▊        | 368/2042 [00:00<00:04, 375.71it/s]Padding data:  20%|██        | 410/2042 [00:01<00:04, 388.11it/s]Padding data:  22%|██▏       | 451/2042 [00:01<00:04, 393.72it/s]Padding data:  24%|██▍       | 491/2042 [00:01<00:03, 393.25it/s]Padding data:  26%|██▌       | 533/2042 [00:01<00:03, 399.59it/s]Padding data:  28%|██▊       | 577/2042 [00:01<00:03, 410.91it/s]Padding data:  31%|███       | 623/2042 [00:01<00:03, 422.60it/s]Padding data:  33%|███▎      | 668/2042 [00:01<00:03, 428.45it/s]Padding data:  35%|███▍      | 711/2042 [00:01<00:03, 426.65it/s]Padding data:  37%|███▋      | 754/2042 [00:01<00:03, 422.52it/s]Padding data:  39%|███▉      | 797/2042 [00:01<00:02, 417.71it/s]Padding data:  41%|████      | 839/2042 [00:02<00:02, 414.48it/s]Padding data:  43%|████▎     | 881/2042 [00:02<00:02, 413.04it/s]Padding data:  45%|████▌     | 924/2042 [00:02<00:02, 415.81it/s]Padding data:  47%|████▋     | 969/2042 [00:02<00:02, 425.84it/s]Padding data:  50%|████▉     | 1017/2042 [00:02<00:02, 439.78it/s]Padding data:  52%|█████▏    | 1062/2042 [00:02<00:02, 441.94it/s]Padding data:  54%|█████▍    | 1108/2042 [00:02<00:02, 444.73it/s]Padding data:  57%|█████▋    | 1154/2042 [00:02<00:01, 448.06it/s]Padding data:  59%|█████▉    | 1201/2042 [00:02<00:01, 452.90it/s]Padding data:  61%|██████    | 1247/2042 [00:02<00:01, 452.15it/s]Padding data:  63%|██████▎   | 1293/2042 [00:03<00:01, 454.46it/s]Padding data:  66%|██████▌   | 1340/2042 [00:03<00:01, 457.19it/s]Padding data:  68%|██████▊   | 1386/2042 [00:03<00:01, 453.45it/s]Padding data:  70%|███████   | 1432/2042 [00:03<00:01, 431.28it/s]Padding data:  72%|███████▏  | 1476/2042 [00:03<00:01, 408.09it/s]Padding data:  74%|███████▍  | 1518/2042 [00:03<00:01, 400.76it/s]Padding data:  76%|███████▋  | 1559/2042 [00:03<00:01, 393.81it/s]Padding data:  78%|███████▊  | 1599/2042 [00:03<00:01, 394.98it/s]Padding data:  80%|████████  | 1639/2042 [00:03<00:01, 389.34it/s]Padding data:  82%|████████▏ | 1681/2042 [00:04<00:00, 395.13it/s]Padding data:  84%|████████▍ | 1724/2042 [00:04<00:00, 404.90it/s]Padding data:  86%|████████▋ | 1765/2042 [00:04<00:00, 393.88it/s]Padding data:  88%|████████▊ | 1805/2042 [00:04<00:00, 391.72it/s]Padding data:  90%|█████████ | 1845/2042 [00:04<00:00, 380.07it/s]Padding data:  92%|█████████▏| 1884/2042 [00:04<00:00, 378.05it/s]Padding data:  94%|█████████▍| 1923/2042 [00:04<00:00, 378.53it/s]Padding data:  96%|█████████▌| 1961/2042 [00:04<00:00, 360.79it/s]Padding data:  98%|█████████▊| 1998/2042 [00:04<00:00, 351.83it/s]Padding data: 100%|█████████▉| 2034/2042 [00:05<00:00, 348.66it/s]Padding data: 100%|██████████| 2042/2042 [00:05<00:00, 404.40it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 100, 7)]     0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 100, 64)      3648        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 100, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 100, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 100, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 100, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 100, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 100, 64)      512         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 100, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 100, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 100, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 100, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 100, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 100, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 100, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 100, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 100, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 100, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 100, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 100, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 100, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 100, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 100, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 100, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 100, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 100, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 100, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 100, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 100, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 100, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 100, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 100, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 100, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 100, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 510,274
Trainable params: 507,714
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4408/4408 - 219s - loss: 0.6265
Epoch 2/100
4408/4408 - 184s - loss: 0.6054
Epoch 3/100
4408/4408 - 185s - loss: 0.5892
Epoch 4/100
4408/4408 - 186s - loss: 0.5669
Epoch 5/100
4408/4408 - 185s - loss: 0.5433
Epoch 6/100
4408/4408 - 185s - loss: 0.5406
Epoch 7/100
4408/4408 - 186s - loss: 0.5295
Epoch 8/100
4408/4408 - 185s - loss: 0.5217
Epoch 9/100
4408/4408 - 184s - loss: 0.5188
Epoch 10/100
4408/4408 - 184s - loss: 0.5132
Epoch 11/100
4408/4408 - 185s - loss: 0.5120
Epoch 12/100
4408/4408 - 187s - loss: 0.5107
Epoch 13/100
4408/4408 - 187s - loss: 0.5038
Epoch 14/100
4408/4408 - 185s - loss: 0.5035
Epoch 15/100
4408/4408 - 185s - loss: 0.5003
Epoch 16/100
4408/4408 - 185s - loss: 0.4998
Epoch 17/100
4408/4408 - 185s - loss: 0.4985
Epoch 18/100
4408/4408 - 185s - loss: 0.4930
Epoch 19/100
4408/4408 - 182s - loss: 0.4904
Epoch 20/100
4408/4408 - 184s - loss: 0.4902
Epoch 21/100
4408/4408 - 185s - loss: 0.4876
Epoch 22/100
4408/4408 - 185s - loss: 0.4852
Epoch 23/100
4408/4408 - 186s - loss: 0.4831
Epoch 24/100
4408/4408 - 185s - loss: 0.4799
Epoch 25/100
4408/4408 - 183s - loss: 0.4793
Epoch 26/100
4408/4408 - 184s - loss: 0.4750
Epoch 27/100
4408/4408 - 185s - loss: 0.4761
Epoch 28/100
4408/4408 - 185s - loss: 0.4685
Epoch 29/100
4408/4408 - 185s - loss: 0.4736
Epoch 30/100
4408/4408 - 184s - loss: 0.4667
Epoch 31/100
4408/4408 - 184s - loss: 0.4644
Epoch 32/100
4408/4408 - 185s - loss: 0.4625
Epoch 33/100
4408/4408 - 185s - loss: 0.4591
Epoch 34/100
4408/4408 - 185s - loss: 0.4576
Epoch 35/100
4408/4408 - 187s - loss: 0.4554
Epoch 36/100
4408/4408 - 188s - loss: 0.4532
Epoch 37/100
4408/4408 - 189s - loss: 0.4501
Epoch 38/100
4408/4408 - 188s - loss: 0.4439
Epoch 39/100
4408/4408 - 186s - loss: 0.4443
Epoch 40/100
4408/4408 - 184s - loss: 0.4432
Epoch 41/100
4408/4408 - 184s - loss: 0.4319
Epoch 42/100
4408/4408 - 185s - loss: 0.4390
Epoch 43/100
4408/4408 - 186s - loss: 0.4327
Epoch 44/100
4408/4408 - 186s - loss: 0.4317
Epoch 45/100
4408/4408 - 184s - loss: 0.4267
Epoch 46/100
4408/4408 - 186s - loss: 0.4250
Epoch 47/100
4408/4408 - 185s - loss: 0.4221
Epoch 48/100
4408/4408 - 184s - loss: 0.4177
Epoch 49/100
4408/4408 - 185s - loss: 0.4148
Epoch 50/100
4408/4408 - 184s - loss: 0.4099
Epoch 51/100
4408/4408 - 184s - loss: 0.4118
Epoch 52/100
4408/4408 - 186s - loss: 0.4042
Epoch 53/100
4408/4408 - 185s - loss: 0.4014
Epoch 54/100
4408/4408 - 184s - loss: 0.4034
Epoch 55/100
4408/4408 - 187s - loss: 0.3984
Epoch 56/100
4408/4408 - 184s - loss: 0.3955
Epoch 57/100
4408/4408 - 185s - loss: 0.3933
Epoch 58/100
4408/4408 - 186s - loss: 0.3893
Epoch 59/100
4408/4408 - 186s - loss: 0.3876
Epoch 60/100
4408/4408 - 187s - loss: 0.3834
Epoch 61/100
4408/4408 - 184s - loss: 0.3817
Epoch 62/100
4408/4408 - 184s - loss: 0.3796
Epoch 63/100
4408/4408 - 183s - loss: 0.3740
Epoch 64/100
4408/4408 - 184s - loss: 0.3735
Epoch 65/100
4408/4408 - 186s - loss: 0.3730
Epoch 66/100
4408/4408 - 186s - loss: 0.3638
Epoch 67/100
4408/4408 - 185s - loss: 0.3623
Epoch 68/100
4408/4408 - 187s - loss: 0.3569
Epoch 69/100
4408/4408 - 185s - loss: 0.3559
Epoch 70/100
4408/4408 - 186s - loss: 0.3543
Epoch 71/100
4408/4408 - 187s - loss: 0.3537
Epoch 72/100
4408/4408 - 185s - loss: 0.3473
Epoch 73/100
4408/4408 - 184s - loss: 0.3399
Epoch 74/100
4408/4408 - 183s - loss: 0.3398
Epoch 75/100
4408/4408 - 184s - loss: 0.3372
Epoch 76/100
4408/4408 - 186s - loss: 0.3363
Epoch 77/100
4408/4408 - 185s - loss: 0.3284
Epoch 78/100
4408/4408 - 185s - loss: 0.3260
Epoch 79/100
4408/4408 - 184s - loss: 0.3279
Epoch 80/100
4408/4408 - 185s - loss: 0.3231
Epoch 81/100
4408/4408 - 183s - loss: 0.3199
Epoch 82/100
4408/4408 - 184s - loss: 0.3129
Epoch 83/100
4408/4408 - 185s - loss: 0.3191
Epoch 84/100
4408/4408 - 185s - loss: 0.3104
Epoch 85/100
4408/4408 - 185s - loss: 0.3098
Epoch 86/100
4408/4408 - 185s - loss: 0.3077
Epoch 87/100
4408/4408 - 186s - loss: 0.2990
Epoch 88/100
4408/4408 - 185s - loss: 0.2942
Epoch 89/100
4408/4408 - 184s - loss: 0.3015
Epoch 90/100
4408/4408 - 185s - loss: 0.2932
Epoch 91/100
4408/4408 - 183s - loss: 0.2906
Epoch 92/100
4408/4408 - 185s - loss: 0.2891
Epoch 93/100
4408/4408 - 183s - loss: 0.2863
Epoch 94/100
4408/4408 - 184s - loss: 0.2831
Epoch 95/100
4408/4408 - 186s - loss: 0.2783
Epoch 96/100
4408/4408 - 185s - loss: 0.2692
Epoch 97/100
4408/4408 - 184s - loss: 0.2705
Epoch 98/100
4408/4408 - 185s - loss: 0.2721
Epoch 99/100
4408/4408 - 186s - loss: 0.2643
Epoch 100/100
4408/4408 - 187s - loss: 0.2695
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.458431413671621
Test f1_avg: 0.4645491590636674
