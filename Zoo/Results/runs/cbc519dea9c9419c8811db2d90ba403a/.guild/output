INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 42/2042 [00:00<00:04, 407.71it/s]Padding data:   4%|▍         | 83/2042 [00:00<00:04, 399.87it/s]Padding data:   6%|▌         | 124/2042 [00:00<00:04, 401.18it/s]Padding data:   8%|▊         | 166/2042 [00:00<00:04, 404.69it/s]Padding data:  10%|█         | 209/2042 [00:00<00:04, 412.82it/s]Padding data:  12%|█▏        | 255/2042 [00:00<00:04, 427.33it/s]Padding data:  15%|█▍        | 300/2042 [00:00<00:04, 432.65it/s]Padding data:  17%|█▋        | 347/2042 [00:00<00:03, 443.55it/s]Padding data:  19%|█▉        | 392/2042 [00:00<00:03, 444.71it/s]Padding data:  22%|██▏       | 443/2042 [00:01<00:03, 462.42it/s]Padding data:  24%|██▍       | 493/2042 [00:01<00:03, 470.76it/s]Padding data:  26%|██▋       | 541/2042 [00:01<00:03, 458.83it/s]Padding data:  29%|██▊       | 587/2042 [00:01<00:03, 434.71it/s]Padding data:  31%|███       | 631/2042 [00:01<00:03, 415.85it/s]Padding data:  33%|███▎      | 673/2042 [00:01<00:03, 408.98it/s]Padding data:  35%|███▌      | 718/2042 [00:01<00:03, 419.17it/s]Padding data:  37%|███▋      | 761/2042 [00:01<00:03, 402.34it/s]Padding data:  39%|███▉      | 802/2042 [00:01<00:03, 393.60it/s]Padding data:  41%|████      | 842/2042 [00:02<00:03, 383.61it/s]Padding data:  43%|████▎     | 881/2042 [00:02<00:03, 374.60it/s]Padding data:  45%|████▌     | 921/2042 [00:02<00:02, 381.31it/s]Padding data:  47%|████▋     | 963/2042 [00:02<00:02, 392.18it/s]Padding data:  49%|████▉     | 1008/2042 [00:02<00:02, 407.77it/s]Padding data:  52%|█████▏    | 1052/2042 [00:02<00:02, 417.01it/s]Padding data:  54%|█████▎    | 1094/2042 [00:02<00:02, 405.63it/s]Padding data:  56%|█████▌    | 1137/2042 [00:02<00:02, 409.74it/s]Padding data:  58%|█████▊    | 1179/2042 [00:02<00:02, 405.05it/s]Padding data:  60%|█████▉    | 1222/2042 [00:02<00:02, 410.00it/s]Padding data:  62%|██████▏   | 1265/2042 [00:03<00:01, 413.85it/s]Padding data:  64%|██████▍   | 1309/2042 [00:03<00:01, 419.98it/s]Padding data:  66%|██████▋   | 1353/2042 [00:03<00:01, 424.15it/s]Padding data:  68%|██████▊   | 1396/2042 [00:03<00:01, 421.52it/s]Padding data:  70%|███████   | 1439/2042 [00:03<00:01, 409.90it/s]Padding data:  73%|███████▎  | 1481/2042 [00:03<00:01, 404.00it/s]Padding data:  75%|███████▍  | 1522/2042 [00:03<00:01, 398.83it/s]Padding data:  77%|███████▋  | 1563/2042 [00:03<00:01, 401.67it/s]Padding data:  79%|███████▊  | 1604/2042 [00:03<00:01, 400.66it/s]Padding data:  81%|████████  | 1645/2042 [00:03<00:01, 395.02it/s]Padding data:  83%|████████▎ | 1685/2042 [00:04<00:00, 394.15it/s]Padding data:  84%|████████▍ | 1725/2042 [00:04<00:00, 392.19it/s]Padding data:  87%|████████▋ | 1771/2042 [00:04<00:00, 411.33it/s]Padding data:  89%|████████▉ | 1816/2042 [00:04<00:00, 421.45it/s]Padding data:  91%|█████████ | 1862/2042 [00:04<00:00, 430.43it/s]Padding data:  93%|█████████▎| 1908/2042 [00:04<00:00, 439.11it/s]Padding data:  96%|█████████▌| 1952/2042 [00:04<00:00, 437.45it/s]Padding data:  98%|█████████▊| 1996/2042 [00:04<00:00, 431.37it/s]Padding data: 100%|█████████▉| 2041/2042 [00:04<00:00, 436.83it/s]Padding data: 100%|██████████| 2042/2042 [00:04<00:00, 416.10it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 500, 7)]          0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            60635     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 68,094
Trainable params: 68,094
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4406/4406 - 39s - loss: 0.6188
Epoch 2/100
4406/4406 - 35s - loss: 0.5693
Epoch 3/100
4406/4406 - 35s - loss: 0.5508
Epoch 4/100
4406/4406 - 35s - loss: 0.5344
Epoch 5/100
4406/4406 - 35s - loss: 0.5233
Epoch 6/100
4406/4406 - 35s - loss: 0.5196
Epoch 7/100
4406/4406 - 36s - loss: 0.5157
Epoch 8/100
4406/4406 - 35s - loss: 0.5083
Epoch 9/100
4406/4406 - 35s - loss: 0.5051
Epoch 10/100
4406/4406 - 35s - loss: 0.4993
Epoch 11/100
4406/4406 - 36s - loss: 0.4969
Epoch 12/100
4406/4406 - 35s - loss: 0.4931
Epoch 13/100
4406/4406 - 36s - loss: 0.4903
Epoch 14/100
4406/4406 - 35s - loss: 0.4869
Epoch 15/100
4406/4406 - 35s - loss: 0.4858
Epoch 16/100
4406/4406 - 35s - loss: 0.4835
Epoch 17/100
4406/4406 - 36s - loss: 0.4813
Epoch 18/100
4406/4406 - 36s - loss: 0.4793
Epoch 19/100
4406/4406 - 35s - loss: 0.4782
Epoch 20/100
4406/4406 - 35s - loss: 0.4752
Epoch 21/100
4406/4406 - 36s - loss: 0.4729
Epoch 22/100
4406/4406 - 35s - loss: 0.4745
Epoch 23/100
4406/4406 - 36s - loss: 0.4700
Epoch 24/100
4406/4406 - 35s - loss: 0.4715
Epoch 25/100
4406/4406 - 35s - loss: 0.4671
Epoch 26/100
4406/4406 - 35s - loss: 0.4681
Epoch 27/100
4406/4406 - 36s - loss: 0.4683
Epoch 28/100
4406/4406 - 36s - loss: 0.4665
Epoch 29/100
4406/4406 - 36s - loss: 0.4644
Epoch 30/100
4406/4406 - 36s - loss: 0.4633
Epoch 31/100
4406/4406 - 35s - loss: 0.4624
Epoch 32/100
4406/4406 - 35s - loss: 0.4610
Epoch 33/100
4406/4406 - 33s - loss: 0.4610
Epoch 34/100
4406/4406 - 34s - loss: 0.4602
Epoch 35/100
4406/4406 - 36s - loss: 0.4589
Epoch 36/100
4406/4406 - 36s - loss: 0.4580
Epoch 37/100
4406/4406 - 35s - loss: 0.4586
Epoch 38/100
4406/4406 - 36s - loss: 0.4577
Epoch 39/100
4406/4406 - 36s - loss: 0.4567
Epoch 40/100
4406/4406 - 35s - loss: 0.4567
Epoch 41/100
4406/4406 - 36s - loss: 0.4535
Epoch 42/100
4406/4406 - 34s - loss: 0.4530
Epoch 43/100
4406/4406 - 35s - loss: 0.4562
Epoch 44/100
4406/4406 - 34s - loss: 0.4524
Epoch 45/100
4406/4406 - 33s - loss: 0.4554
Epoch 46/100
4406/4406 - 34s - loss: 0.4529
Epoch 47/100
4406/4406 - 35s - loss: 0.4523
Epoch 48/100
4406/4406 - 35s - loss: 0.4523

Epoch 00048: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 49/100
4406/4406 - 35s - loss: 0.4431
Epoch 50/100
4406/4406 - 35s - loss: 0.4424
Epoch 51/100
4406/4406 - 36s - loss: 0.4402
Epoch 52/100
4406/4406 - 36s - loss: 0.4423
Epoch 53/100
4406/4406 - 37s - loss: 0.4409
Epoch 54/100
4406/4406 - 36s - loss: 0.4406
Epoch 55/100
4406/4406 - 36s - loss: 0.4401
Epoch 56/100
4406/4406 - 36s - loss: 0.4400
Epoch 57/100
4406/4406 - 36s - loss: 0.4392
Epoch 58/100
4406/4406 - 36s - loss: 0.4392
Epoch 59/100
4406/4406 - 36s - loss: 0.4384
Epoch 60/100
4406/4406 - 36s - loss: 0.4393
Epoch 61/100
4406/4406 - 36s - loss: 0.4369
Epoch 62/100
4406/4406 - 36s - loss: 0.4382
Epoch 63/100
4406/4406 - 36s - loss: 0.4371
Epoch 64/100
4406/4406 - 36s - loss: 0.4377
Epoch 65/100
4406/4406 - 36s - loss: 0.4382

Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 66/100
4406/4406 - 36s - loss: 0.4323
Epoch 67/100
4406/4406 - 36s - loss: 0.4317
Epoch 68/100
4406/4406 - 35s - loss: 0.4323
Epoch 69/100
4406/4406 - 36s - loss: 0.4319
Epoch 70/100
4406/4406 - 35s - loss: 0.4320
Epoch 71/100
4406/4406 - 35s - loss: 0.4316
Epoch 72/100
4406/4406 - 36s - loss: 0.4315
Epoch 73/100
4406/4406 - 35s - loss: 0.4310
Epoch 74/100
4406/4406 - 35s - loss: 0.4308
Epoch 75/100
4406/4406 - 35s - loss: 0.4309
Epoch 76/100
4406/4406 - 35s - loss: 0.4311
Epoch 77/100
4406/4406 - 35s - loss: 0.4306
Epoch 78/100
4406/4406 - 36s - loss: 0.4311
Epoch 79/100
4406/4406 - 35s - loss: 0.4309
Epoch 80/100
4406/4406 - 37s - loss: 0.4305
Epoch 81/100
4406/4406 - 37s - loss: 0.4300
Epoch 82/100
4406/4406 - 37s - loss: 0.4311
Epoch 83/100
4406/4406 - 36s - loss: 0.4302
Epoch 84/100
4406/4406 - 36s - loss: 0.4299
Epoch 85/100
4406/4406 - 36s - loss: 0.4303

Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 86/100
4406/4406 - 39s - loss: 0.4268
Epoch 87/100
4406/4406 - 36s - loss: 0.4261
Epoch 88/100
4406/4406 - 36s - loss: 0.4267
Epoch 89/100
4406/4406 - 36s - loss: 0.4266
Epoch 90/100
4406/4406 - 37s - loss: 0.4265
Epoch 91/100
4406/4406 - 37s - loss: 0.4263

Epoch 00091: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 92/100
4406/4406 - 37s - loss: 0.4254
Epoch 93/100
4406/4406 - 37s - loss: 0.4250
Epoch 94/100
4406/4406 - 37s - loss: 0.4257
Epoch 95/100
4406/4406 - 37s - loss: 0.4257
Epoch 96/100
4406/4406 - 37s - loss: 0.4253
Epoch 97/100
4406/4406 - 37s - loss: 0.4258
Epoch 98/100
4406/4406 - 37s - loss: 0.4257
Epoch 99/100
4406/4406 - 37s - loss: 0.4255
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.7370399243086423
Test f1_avg: 0.7129434280936684
