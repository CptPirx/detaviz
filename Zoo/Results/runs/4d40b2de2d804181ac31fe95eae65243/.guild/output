INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   1%|          | 12/2042 [00:00<00:18, 109.30it/s]Padding data:   1%|          | 23/2042 [00:00<00:20, 99.50it/s] Padding data:   2%|▏         | 35/2042 [00:00<00:19, 104.56it/s]Padding data:   2%|▏         | 47/2042 [00:00<00:18, 108.66it/s]Padding data:   3%|▎         | 59/2042 [00:00<00:17, 110.93it/s]Padding data:   3%|▎         | 71/2042 [00:00<00:17, 112.61it/s]Padding data:   4%|▍         | 84/2042 [00:00<00:16, 116.38it/s]Padding data:   5%|▍         | 96/2042 [00:00<00:16, 117.44it/s]Padding data:   5%|▌         | 109/2042 [00:00<00:16, 119.46it/s]Padding data:   6%|▌         | 121/2042 [00:01<00:16, 117.25it/s]Padding data:   7%|▋         | 133/2042 [00:01<00:16, 114.23it/s]Padding data:   7%|▋         | 146/2042 [00:01<00:16, 117.13it/s]Padding data:   8%|▊         | 158/2042 [00:01<00:17, 109.09it/s]Padding data:   8%|▊         | 171/2042 [00:01<00:16, 114.64it/s]Padding data:   9%|▉         | 184/2042 [00:01<00:15, 116.29it/s]Padding data:  10%|▉         | 196/2042 [00:01<00:15, 117.27it/s]Padding data:  10%|█         | 208/2042 [00:01<00:15, 117.68it/s]Padding data:  11%|█         | 220/2042 [00:01<00:15, 117.58it/s]Padding data:  11%|█▏        | 233/2042 [00:02<00:15, 119.07it/s]Padding data:  12%|█▏        | 247/2042 [00:02<00:14, 123.89it/s]Padding data:  13%|█▎        | 261/2042 [00:02<00:14, 126.69it/s]Padding data:  13%|█▎        | 275/2042 [00:02<00:13, 128.89it/s]Padding data:  14%|█▍        | 290/2042 [00:02<00:13, 133.12it/s]Padding data:  15%|█▍        | 305/2042 [00:02<00:12, 136.84it/s]Padding data:  16%|█▌        | 320/2042 [00:02<00:12, 138.99it/s]Padding data:  16%|█▋        | 335/2042 [00:02<00:12, 139.08it/s]Padding data:  17%|█▋        | 349/2042 [00:02<00:12, 138.87it/s]Padding data:  18%|█▊        | 363/2042 [00:02<00:12, 138.45it/s]Padding data:  18%|█▊        | 377/2042 [00:03<00:12, 135.74it/s]Padding data:  19%|█▉        | 391/2042 [00:03<00:12, 134.24it/s]Padding data:  20%|█▉        | 406/2042 [00:03<00:12, 136.09it/s]Padding data:  21%|██        | 420/2042 [00:03<00:12, 133.19it/s]Padding data:  21%|██▏       | 434/2042 [00:03<00:12, 127.41it/s]Padding data:  22%|██▏       | 448/2042 [00:03<00:12, 128.57it/s]Padding data:  23%|██▎       | 461/2042 [00:03<00:13, 119.09it/s]Padding data:  23%|██▎       | 474/2042 [00:03<00:13, 115.43it/s]Padding data:  24%|██▍       | 487/2042 [00:03<00:13, 117.36it/s]Padding data:  25%|██▍       | 502/2042 [00:04<00:12, 124.42it/s]Padding data:  25%|██▌       | 516/2042 [00:04<00:12, 123.97it/s]Padding data:  26%|██▌       | 530/2042 [00:04<00:11, 127.16it/s]Padding data:  27%|██▋       | 544/2042 [00:04<00:11, 128.81it/s]Padding data:  27%|██▋       | 559/2042 [00:04<00:11, 133.20it/s]Padding data:  28%|██▊       | 573/2042 [00:04<00:11, 130.44it/s]Padding data:  29%|██▊       | 587/2042 [00:04<00:11, 129.86it/s]Padding data:  29%|██▉       | 601/2042 [00:04<00:11, 128.33it/s]Padding data:  30%|███       | 615/2042 [00:04<00:11, 129.22it/s]Padding data:  31%|███       | 628/2042 [00:05<00:11, 125.78it/s]Padding data:  31%|███▏      | 641/2042 [00:05<00:11, 124.65it/s]Padding data:  32%|███▏      | 655/2042 [00:05<00:10, 127.77it/s]Padding data:  33%|███▎      | 668/2042 [00:05<00:10, 126.97it/s]Padding data:  33%|███▎      | 683/2042 [00:05<00:10, 130.77it/s]Padding data:  34%|███▍      | 697/2042 [00:05<00:10, 127.99it/s]Padding data:  35%|███▍      | 710/2042 [00:05<00:10, 127.75it/s]Padding data:  35%|███▌      | 723/2042 [00:05<00:10, 125.93it/s]Padding data:  36%|███▌      | 736/2042 [00:05<00:10, 126.52it/s]Padding data:  37%|███▋      | 751/2042 [00:06<00:09, 132.01it/s]Padding data:  37%|███▋      | 765/2042 [00:06<00:09, 130.55it/s]Padding data:  38%|███▊      | 779/2042 [00:06<00:09, 131.23it/s]Padding data:  39%|███▉      | 793/2042 [00:06<00:09, 131.92it/s]Padding data:  40%|███▉      | 808/2042 [00:06<00:09, 135.17it/s]Padding data:  40%|████      | 822/2042 [00:06<00:09, 134.55it/s]Padding data:  41%|████      | 836/2042 [00:06<00:08, 135.01it/s]Padding data:  42%|████▏     | 850/2042 [00:06<00:08, 135.54it/s]Padding data:  42%|████▏     | 865/2042 [00:06<00:08, 137.93it/s]Padding data:  43%|████▎     | 879/2042 [00:06<00:08, 138.31it/s]Padding data:  44%|████▎     | 893/2042 [00:07<00:08, 136.49it/s]Padding data:  44%|████▍     | 908/2042 [00:07<00:08, 138.65it/s]Padding data:  45%|████▌     | 923/2042 [00:07<00:07, 141.49it/s]Padding data:  46%|████▌     | 938/2042 [00:07<00:07, 140.32it/s]Padding data:  47%|████▋     | 953/2042 [00:07<00:07, 142.74it/s]Padding data:  47%|████▋     | 968/2042 [00:07<00:07, 143.08it/s]Padding data:  48%|████▊     | 983/2042 [00:07<00:07, 143.59it/s]Padding data:  49%|████▉     | 999/2042 [00:07<00:07, 145.82it/s]Padding data:  50%|████▉     | 1014/2042 [00:07<00:07, 142.63it/s]Padding data:  50%|█████     | 1029/2042 [00:08<00:07, 142.89it/s]Padding data:  51%|█████     | 1044/2042 [00:08<00:07, 142.29it/s]Padding data:  52%|█████▏    | 1059/2042 [00:08<00:07, 135.92it/s]Padding data:  53%|█████▎    | 1073/2042 [00:08<00:07, 130.97it/s]Padding data:  53%|█████▎    | 1088/2042 [00:08<00:07, 135.53it/s]Padding data:  54%|█████▍    | 1102/2042 [00:08<00:07, 133.67it/s]Padding data:  55%|█████▍    | 1116/2042 [00:08<00:06, 135.22it/s]Padding data:  55%|█████▌    | 1130/2042 [00:08<00:06, 134.47it/s]Padding data:  56%|█████▌    | 1145/2042 [00:08<00:06, 136.09it/s]Padding data:  57%|█████▋    | 1159/2042 [00:08<00:06, 137.04it/s]Padding data:  58%|█████▊    | 1175/2042 [00:09<00:06, 141.35it/s]Padding data:  58%|█████▊    | 1190/2042 [00:09<00:06, 139.10it/s]Padding data:  59%|█████▉    | 1205/2042 [00:09<00:05, 139.78it/s]Padding data:  60%|█████▉    | 1219/2042 [00:09<00:05, 139.77it/s]Padding data:  60%|██████    | 1234/2042 [00:09<00:05, 141.87it/s]Padding data:  61%|██████    | 1249/2042 [00:09<00:05, 143.72it/s]Padding data:  62%|██████▏   | 1264/2042 [00:09<00:05, 145.00it/s]Padding data:  63%|██████▎   | 1279/2042 [00:09<00:05, 145.42it/s]Padding data:  63%|██████▎   | 1294/2042 [00:09<00:05, 146.48it/s]Padding data:  64%|██████▍   | 1309/2042 [00:10<00:05, 145.12it/s]Padding data:  65%|██████▍   | 1325/2042 [00:10<00:04, 146.71it/s]Padding data:  66%|██████▌   | 1340/2042 [00:10<00:04, 145.74it/s]Padding data:  66%|██████▋   | 1356/2042 [00:10<00:04, 148.17it/s]Padding data:  67%|██████▋   | 1371/2042 [00:10<00:04, 143.53it/s]Padding data:  68%|██████▊   | 1387/2042 [00:10<00:04, 147.26it/s]Padding data:  69%|██████▊   | 1402/2042 [00:10<00:04, 143.40it/s]Padding data:  69%|██████▉   | 1417/2042 [00:10<00:04, 144.31it/s]Padding data:  70%|███████   | 1432/2042 [00:10<00:04, 136.79it/s]Padding data:  71%|███████   | 1447/2042 [00:10<00:04, 138.72it/s]Padding data:  72%|███████▏  | 1461/2042 [00:11<00:04, 136.63it/s]Padding data:  72%|███████▏  | 1476/2042 [00:11<00:04, 140.07it/s]Padding data:  73%|███████▎  | 1491/2042 [00:11<00:03, 139.58it/s]Padding data:  74%|███████▍  | 1506/2042 [00:11<00:03, 141.88it/s]Padding data:  74%|███████▍  | 1521/2042 [00:11<00:03, 139.41it/s]Padding data:  75%|███████▌  | 1537/2042 [00:11<00:03, 145.27it/s]Padding data:  76%|███████▌  | 1552/2042 [00:11<00:03, 144.23it/s]Padding data:  77%|███████▋  | 1568/2042 [00:11<00:03, 147.16it/s]Padding data:  78%|███████▊  | 1583/2042 [00:11<00:03, 145.12it/s]Padding data:  78%|███████▊  | 1598/2042 [00:12<00:03, 145.60it/s]Padding data:  79%|███████▉  | 1613/2042 [00:12<00:03, 141.86it/s]Padding data:  80%|███████▉  | 1628/2042 [00:12<00:02, 143.93it/s]Padding data:  80%|████████  | 1643/2042 [00:12<00:02, 144.97it/s]Padding data:  81%|████████  | 1659/2042 [00:12<00:02, 146.96it/s]Padding data:  82%|████████▏ | 1674/2042 [00:12<00:02, 143.11it/s]Padding data:  83%|████████▎ | 1689/2042 [00:12<00:02, 144.85it/s]Padding data:  83%|████████▎ | 1705/2042 [00:12<00:02, 147.40it/s]Padding data:  84%|████████▍ | 1721/2042 [00:12<00:02, 149.08it/s]Padding data:  85%|████████▌ | 1736/2042 [00:12<00:02, 141.61it/s]Padding data:  86%|████████▌ | 1751/2042 [00:13<00:02, 140.26it/s]Padding data:  86%|████████▋ | 1766/2042 [00:13<00:02, 136.69it/s]Padding data:  87%|████████▋ | 1780/2042 [00:13<00:01, 135.58it/s]Padding data:  88%|████████▊ | 1794/2042 [00:13<00:01, 135.01it/s]Padding data:  89%|████████▊ | 1809/2042 [00:13<00:01, 136.79it/s]Padding data:  89%|████████▉ | 1823/2042 [00:13<00:01, 137.27it/s]Padding data:  90%|████████▉ | 1837/2042 [00:13<00:01, 135.54it/s]Padding data:  91%|█████████ | 1851/2042 [00:13<00:01, 136.59it/s]Padding data:  91%|█████████▏| 1867/2042 [00:13<00:01, 142.40it/s]Padding data:  92%|█████████▏| 1882/2042 [00:14<00:01, 141.57it/s]Padding data:  93%|█████████▎| 1897/2042 [00:14<00:01, 143.95it/s]Padding data:  94%|█████████▎| 1912/2042 [00:14<00:00, 144.52it/s]Padding data:  94%|█████████▍| 1927/2042 [00:14<00:00, 145.79it/s]Padding data:  95%|█████████▌| 1942/2042 [00:14<00:00, 145.82it/s]Padding data:  96%|█████████▌| 1957/2042 [00:14<00:00, 146.38it/s]Padding data:  97%|█████████▋| 1972/2042 [00:14<00:00, 144.54it/s]Padding data:  97%|█████████▋| 1987/2042 [00:14<00:00, 143.41it/s]Padding data:  98%|█████████▊| 2002/2042 [00:14<00:00, 140.31it/s]Padding data:  99%|█████████▉| 2017/2042 [00:14<00:00, 141.66it/s]Padding data: 100%|█████████▉| 2032/2042 [00:15<00:00, 143.88it/s]Padding data: 100%|██████████| 2042/2042 [00:15<00:00, 134.61it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 100, 125)]   0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 100, 64)      64064       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 100, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 100, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 100, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 100, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 100, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 100, 64)      8064        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 100, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 100, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 100, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 100, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 100, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 100, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 100, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 100, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 100, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 100, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 100, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 100, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 100, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 100, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 100, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 100, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 100, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 100, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 100, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 100, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 100, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 100, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 100, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 100, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 100, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 100, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 578,242
Trainable params: 575,682
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4383/4383 - 256s - loss: 0.6301
Epoch 2/100
4383/4383 - 220s - loss: 0.6169
Epoch 3/100
4383/4383 - 221s - loss: 0.6143
Epoch 4/100
4383/4383 - 223s - loss: 0.6094
Epoch 5/100
4383/4383 - 224s - loss: 0.6057
Epoch 6/100
4383/4383 - 226s - loss: 0.6011
Epoch 7/100
4383/4383 - 224s - loss: 0.5964
Epoch 8/100
4383/4383 - 223s - loss: 0.5896
Epoch 9/100
4383/4383 - 224s - loss: 0.5855
Epoch 10/100
4383/4383 - 222s - loss: 0.5815
Epoch 11/100
4383/4383 - 222s - loss: 0.5755
Epoch 12/100
4383/4383 - 224s - loss: 0.5692
Epoch 13/100
4383/4383 - 222s - loss: 0.5657
Epoch 14/100
4383/4383 - 223s - loss: 0.5589
Epoch 15/100
4383/4383 - 223s - loss: 0.5527
Epoch 16/100
4383/4383 - 220s - loss: 0.5502
Epoch 17/100
4383/4383 - 222s - loss: 0.5434
Epoch 18/100
4383/4383 - 224s - loss: 0.5408
Epoch 19/100
4383/4383 - 222s - loss: 0.5358
Epoch 20/100
4383/4383 - 221s - loss: 0.5244
Epoch 21/100
4383/4383 - 223s - loss: 0.5241
Epoch 22/100
4383/4383 - 224s - loss: 0.5157
Epoch 23/100
4383/4383 - 224s - loss: 0.5093
Epoch 24/100
4383/4383 - 223s - loss: 0.5075
Epoch 25/100
4383/4383 - 224s - loss: 0.4979
Epoch 26/100
4383/4383 - 225s - loss: 0.4898
Epoch 27/100
4383/4383 - 223s - loss: 0.4867
Epoch 28/100
4383/4383 - 224s - loss: 0.4790
Epoch 29/100
4383/4383 - 223s - loss: 0.4732
Epoch 30/100
4383/4383 - 223s - loss: 0.4652
Epoch 31/100
4383/4383 - 224s - loss: 0.4610
Epoch 32/100
4383/4383 - 223s - loss: 0.4525
Epoch 33/100
4383/4383 - 223s - loss: 0.4427
Epoch 34/100
4383/4383 - 223s - loss: 0.4424
Epoch 35/100
4383/4383 - 223s - loss: 0.4335
Epoch 36/100
4383/4383 - 224s - loss: 0.4271
Epoch 37/100
4383/4383 - 224s - loss: 0.4143
Epoch 38/100
4383/4383 - 222s - loss: 0.4107
Epoch 39/100
4383/4383 - 224s - loss: 0.4044
Epoch 40/100
4383/4383 - 224s - loss: 0.3930
Epoch 41/100
4383/4383 - 225s - loss: 0.3886
Epoch 42/100
4383/4383 - 225s - loss: 0.3900
Epoch 43/100
4383/4383 - 226s - loss: 0.3785
Epoch 44/100
4383/4383 - 224s - loss: 0.3734
Epoch 45/100
4383/4383 - 223s - loss: 0.3690
Epoch 46/100
4383/4383 - 223s - loss: 0.3607
Epoch 47/100
4383/4383 - 225s - loss: 0.3530
Epoch 48/100
4383/4383 - 223s - loss: 0.3511
Epoch 49/100
4383/4383 - 227s - loss: 0.3496
Epoch 50/100
4383/4383 - 226s - loss: 0.3409
Epoch 51/100
4383/4383 - 225s - loss: 0.3407
Epoch 52/100
4383/4383 - 227s - loss: 0.3285
Epoch 53/100
4383/4383 - 223s - loss: 0.3261
Epoch 54/100
4383/4383 - 223s - loss: 0.3220
Epoch 55/100
4383/4383 - 221s - loss: 0.3090
Epoch 56/100
4383/4383 - 222s - loss: 0.3088
Epoch 57/100
4383/4383 - 219s - loss: 0.3055
Epoch 58/100
4383/4383 - 220s - loss: 0.3062
Epoch 59/100
4383/4383 - 219s - loss: 0.2935
Epoch 60/100
4383/4383 - 223s - loss: 0.2938
Epoch 61/100
4383/4383 - 224s - loss: 0.2889
Epoch 62/100
4383/4383 - 221s - loss: 0.2887
Epoch 63/100
4383/4383 - 222s - loss: 0.2934
Epoch 64/100
4383/4383 - 223s - loss: 0.2778
Epoch 65/100
4383/4383 - 224s - loss: 0.2805
Epoch 66/100
4383/4383 - 223s - loss: 0.2803
Epoch 67/100
4383/4383 - 225s - loss: 0.2792
Epoch 68/100
4383/4383 - 225s - loss: 0.2763
Epoch 69/100
4383/4383 - 226s - loss: 0.2753
Epoch 70/100
4383/4383 - 226s - loss: 0.2679
Epoch 71/100
4383/4383 - 225s - loss: 0.2722
Epoch 72/100
4383/4383 - 227s - loss: 0.2655
Epoch 73/100
4383/4383 - 226s - loss: 0.2632
Epoch 74/100
4383/4383 - 226s - loss: 0.2607
Epoch 75/100
4383/4383 - 226s - loss: 0.2550
Epoch 76/100
4383/4383 - 227s - loss: 0.2578
Epoch 77/100
4383/4383 - 225s - loss: 0.2492
Epoch 78/100
4383/4383 - 227s - loss: 0.2563
Epoch 79/100
4383/4383 - 226s - loss: 0.2485
Epoch 80/100
4383/4383 - 226s - loss: 0.2464
Epoch 81/100
4383/4383 - 226s - loss: 0.2540
Epoch 82/100
4383/4383 - 224s - loss: 0.2495
Epoch 83/100
4383/4383 - 224s - loss: 0.2421
Epoch 84/100
4383/4383 - 226s - loss: 0.2468
Epoch 85/100
4383/4383 - 226s - loss: 0.2435
Epoch 86/100
4383/4383 - 226s - loss: 0.2527
Epoch 87/100
4383/4383 - 225s - loss: 0.2403
Epoch 88/100
4383/4383 - 226s - loss: 0.2402
Epoch 89/100
4383/4383 - 229s - loss: 0.2376
Epoch 90/100
4383/4383 - 225s - loss: 0.2349
Epoch 91/100
4383/4383 - 226s - loss: 0.2382
Epoch 92/100
4383/4383 - 225s - loss: 0.2298
Epoch 93/100
4383/4383 - 226s - loss: 0.2346
Epoch 94/100
4383/4383 - 226s - loss: 0.2315
Epoch 95/100
4383/4383 - 226s - loss: 0.2325
Epoch 96/100
4383/4383 - 225s - loss: 0.2308

Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 97/100
4383/4383 - 226s - loss: 0.2010
Epoch 98/100
4383/4383 - 226s - loss: 0.1927
Epoch 99/100
4383/4383 - 226s - loss: 0.1908
Epoch 100/100
4383/4383 - 226s - loss: 0.1884
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.5059661351790522
Test f1_avg: 0.4819658603477299
