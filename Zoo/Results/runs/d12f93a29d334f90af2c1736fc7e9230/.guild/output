INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 50/2042 [00:00<00:04, 492.11it/s]Padding data:   5%|▍         | 100/2042 [00:00<00:04, 444.64it/s]Padding data:   7%|▋         | 145/2042 [00:00<00:04, 434.67it/s]Padding data:   9%|▉         | 189/2042 [00:00<00:04, 426.67it/s]Padding data:  12%|█▏        | 241/2042 [00:00<00:03, 457.68it/s]Padding data:  14%|█▍        | 292/2042 [00:00<00:03, 471.74it/s]Padding data:  17%|█▋        | 341/2042 [00:00<00:03, 476.83it/s]Padding data:  19%|█▉        | 391/2042 [00:00<00:03, 481.67it/s]Padding data:  22%|██▏       | 440/2042 [00:00<00:03, 479.29it/s]Padding data:  24%|██▍       | 488/2042 [00:01<00:03, 476.36it/s]Padding data:  26%|██▌       | 536/2042 [00:01<00:03, 470.51it/s]Padding data:  29%|██▊       | 584/2042 [00:01<00:03, 467.45it/s]Padding data:  31%|███       | 632/2042 [00:01<00:03, 468.63it/s]Padding data:  33%|███▎      | 679/2042 [00:01<00:02, 459.88it/s]Padding data:  36%|███▌      | 726/2042 [00:01<00:02, 457.53it/s]Padding data:  38%|███▊      | 772/2042 [00:01<00:02, 454.97it/s]Padding data:  40%|████      | 818/2042 [00:01<00:02, 454.31it/s]Padding data:  42%|████▏     | 864/2042 [00:01<00:02, 454.15it/s]Padding data:  45%|████▍     | 910/2042 [00:01<00:02, 453.52it/s]Padding data:  47%|████▋     | 956/2042 [00:02<00:02, 449.21it/s]Padding data:  49%|████▉     | 1001/2042 [00:02<00:02, 437.83it/s]Padding data:  51%|█████     | 1045/2042 [00:02<00:02, 426.41it/s]Padding data:  53%|█████▎    | 1088/2042 [00:02<00:02, 424.62it/s]Padding data:  56%|█████▌    | 1137/2042 [00:02<00:02, 441.74it/s]Padding data:  58%|█████▊    | 1186/2042 [00:02<00:01, 454.22it/s]Padding data:  60%|██████    | 1232/2042 [00:02<00:01, 448.01it/s]Padding data:  63%|██████▎   | 1277/2042 [00:02<00:01, 442.07it/s]Padding data:  65%|██████▍   | 1322/2042 [00:02<00:01, 435.21it/s]Padding data:  67%|██████▋   | 1366/2042 [00:03<00:01, 435.18it/s]Padding data:  69%|██████▉   | 1415/2042 [00:03<00:01, 450.82it/s]Padding data:  72%|███████▏  | 1464/2042 [00:03<00:01, 461.49it/s]Padding data:  74%|███████▍  | 1514/2042 [00:03<00:01, 470.72it/s]Padding data:  77%|███████▋  | 1563/2042 [00:03<00:01, 474.82it/s]Padding data:  79%|███████▉  | 1611/2042 [00:03<00:00, 475.50it/s]Padding data:  81%|████████▏ | 1660/2042 [00:03<00:00, 477.14it/s]Padding data:  84%|████████▎ | 1709/2042 [00:03<00:00, 478.29it/s]Padding data:  86%|████████▌ | 1757/2042 [00:03<00:00, 475.60it/s]Padding data:  88%|████████▊ | 1805/2042 [00:03<00:00, 465.03it/s]Padding data:  91%|█████████ | 1852/2042 [00:04<00:00, 461.42it/s]Padding data:  93%|█████████▎| 1900/2042 [00:04<00:00, 465.19it/s]Padding data:  95%|█████████▌| 1948/2042 [00:04<00:00, 468.62it/s]Padding data:  98%|█████████▊| 1997/2042 [00:04<00:00, 472.95it/s]Padding data: 100%|██████████| 2042/2042 [00:04<00:00, 459.68it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 300, 7)]          0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            36635     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 44,094
Trainable params: 44,094
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4407/4407 - 32s - loss: 0.5853
Epoch 2/100
4407/4407 - 34s - loss: 0.5550
Epoch 3/100
4407/4407 - 34s - loss: 0.5377
Epoch 4/100
4407/4407 - 31s - loss: 0.5276
Epoch 5/100
4407/4407 - 34s - loss: 0.5209
Epoch 6/100
4407/4407 - 35s - loss: 0.5152
Epoch 7/100
4407/4407 - 35s - loss: 0.5107
Epoch 8/100
4407/4407 - 34s - loss: 0.5100
Epoch 9/100
4407/4407 - 35s - loss: 0.5072
Epoch 10/100
4407/4407 - 35s - loss: 0.5061
Epoch 11/100
4407/4407 - 35s - loss: 0.5033
Epoch 12/100
4407/4407 - 34s - loss: 0.5004
Epoch 13/100
4407/4407 - 33s - loss: 0.5007
Epoch 14/100
4407/4407 - 33s - loss: 0.4995
Epoch 15/100
4407/4407 - 35s - loss: 0.4987
Epoch 16/100
4407/4407 - 35s - loss: 0.4968
Epoch 17/100
4407/4407 - 35s - loss: 0.4978
Epoch 18/100
4407/4407 - 32s - loss: 0.4944
Epoch 19/100
4407/4407 - 35s - loss: 0.4948
Epoch 20/100
4407/4407 - 35s - loss: 0.4943
Epoch 21/100
4407/4407 - 35s - loss: 0.4929
Epoch 22/100
4407/4407 - 33s - loss: 0.4933
Epoch 23/100
4407/4407 - 35s - loss: 0.4918
Epoch 24/100
4407/4407 - 32s - loss: 0.4910
Epoch 25/100
4407/4407 - 35s - loss: 0.4920
Epoch 26/100
4407/4407 - 35s - loss: 0.4907
Epoch 27/100
4407/4407 - 35s - loss: 0.4893
Epoch 28/100
4407/4407 - 33s - loss: 0.4896
Epoch 29/100
4407/4407 - 35s - loss: 0.4888
Epoch 30/100
4407/4407 - 35s - loss: 0.4901
Epoch 31/100
4407/4407 - 35s - loss: 0.4875
Epoch 32/100
4407/4407 - 35s - loss: 0.4898
Epoch 33/100
4407/4407 - 34s - loss: 0.4893
Epoch 34/100
4407/4407 - 34s - loss: 0.4890
Epoch 35/100
4407/4407 - 33s - loss: 0.4882

Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 36/100
4407/4407 - 34s - loss: 0.4788
Epoch 37/100
4407/4407 - 32s - loss: 0.4775
Epoch 38/100
4407/4407 - 33s - loss: 0.4796
Epoch 39/100
4407/4407 - 35s - loss: 0.4781
Epoch 40/100
4407/4407 - 31s - loss: 0.4788
Epoch 41/100
4407/4407 - 35s - loss: 0.4781

Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 42/100
4407/4407 - 34s - loss: 0.4747
Epoch 43/100
4407/4407 - 34s - loss: 0.4733
Epoch 44/100
4407/4407 - 35s - loss: 0.4731
Epoch 45/100
4407/4407 - 35s - loss: 0.4734
Epoch 46/100
4407/4407 - 35s - loss: 0.4731
Epoch 47/100
4407/4407 - 33s - loss: 0.4730
Epoch 48/100
4407/4407 - 33s - loss: 0.4718
Epoch 49/100
4407/4407 - 33s - loss: 0.4724
Epoch 50/100
4407/4407 - 34s - loss: 0.4725
Epoch 51/100
4407/4407 - 35s - loss: 0.4725
Epoch 52/100
4407/4407 - 33s - loss: 0.4718

Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 53/100
4407/4407 - 35s - loss: 0.4702
Epoch 54/100
4407/4407 - 31s - loss: 0.4696
Epoch 55/100
4407/4407 - 34s - loss: 0.4694
Epoch 56/100
4407/4407 - 34s - loss: 0.4697
Epoch 57/100
4407/4407 - 34s - loss: 0.4698
Epoch 58/100
4407/4407 - 34s - loss: 0.4697
Epoch 59/100
4407/4407 - 33s - loss: 0.4699

Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 60/100
4407/4407 - 34s - loss: 0.4689
Epoch 61/100
4407/4407 - 33s - loss: 0.4686
Epoch 62/100
4407/4407 - 31s - loss: 0.4686
Epoch 63/100
4407/4407 - 34s - loss: 0.4686
Epoch 64/100
4407/4407 - 34s - loss: 0.4687
Epoch 65/100
4407/4407 - 34s - loss: 0.4683
Epoch 66/100
4407/4407 - 34s - loss: 0.4685
Epoch 67/100
4407/4407 - 31s - loss: 0.4679
Epoch 68/100
4407/4407 - 35s - loss: 0.4685
Epoch 69/100
4407/4407 - 30s - loss: 0.4685
Epoch 70/100
4407/4407 - 34s - loss: 0.4678
Epoch 71/100
4407/4407 - 34s - loss: 0.4683
Epoch 72/100
4407/4407 - 34s - loss: 0.4681
Epoch 73/100
4407/4407 - 34s - loss: 0.4677
Epoch 74/100
4407/4407 - 34s - loss: 0.4684
Epoch 75/100
4407/4407 - 34s - loss: 0.4680
Epoch 76/100
4407/4407 - 31s - loss: 0.4677
Epoch 77/100
4407/4407 - 34s - loss: 0.4681
Epoch 78/100
4407/4407 - 34s - loss: 0.4675
Epoch 79/100
4407/4407 - 34s - loss: 0.4682
Epoch 80/100
4407/4407 - 34s - loss: 0.4674
Epoch 81/100
4407/4407 - 34s - loss: 0.4679
Epoch 82/100
4407/4407 - 31s - loss: 0.4679
Epoch 83/100
4407/4407 - 34s - loss: 0.4678
Epoch 84/100
4407/4407 - 32s - loss: 0.4670
Epoch 85/100
4407/4407 - 33s - loss: 0.4671
Epoch 86/100
4407/4407 - 34s - loss: 0.4673
Epoch 87/100
4407/4407 - 32s - loss: 0.4674
Epoch 88/100
4407/4407 - 34s - loss: 0.4668
Epoch 89/100
4407/4407 - 34s - loss: 0.4674
Epoch 90/100
4407/4407 - 35s - loss: 0.4673
Epoch 91/100
4407/4407 - 32s - loss: 0.4671
Epoch 92/100
4407/4407 - 32s - loss: 0.4669
Epoch 93/100
4407/4407 - 35s - loss: 0.4669
Epoch 94/100
4407/4407 - 34s - loss: 0.4673
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.6869693719999528
Test f1_avg: 0.6636444668876696
