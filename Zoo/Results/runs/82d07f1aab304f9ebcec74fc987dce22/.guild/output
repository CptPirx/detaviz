INFO: [numexpr.utils] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Only screwdriver data taken
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|▏         | 50/2042 [00:00<00:04, 495.56it/s]Padding data:   5%|▍         | 102/2042 [00:00<00:03, 506.91it/s]Padding data:   7%|▋         | 153/2042 [00:00<00:03, 487.09it/s]Padding data:  10%|█         | 205/2042 [00:00<00:03, 499.23it/s]Padding data:  13%|█▎        | 264/2042 [00:00<00:03, 529.00it/s]Padding data:  16%|█▌        | 324/2042 [00:00<00:03, 550.60it/s]Padding data:  19%|█▉        | 384/2042 [00:00<00:02, 564.82it/s]Padding data:  22%|██▏       | 442/2042 [00:00<00:02, 567.71it/s]Padding data:  24%|██▍       | 499/2042 [00:00<00:02, 561.22it/s]Padding data:  27%|██▋       | 556/2042 [00:01<00:02, 557.23it/s]Padding data:  30%|██▉       | 612/2042 [00:01<00:02, 552.58it/s]Padding data:  33%|███▎      | 668/2042 [00:01<00:02, 548.05it/s]Padding data:  35%|███▌      | 723/2042 [00:01<00:02, 546.16it/s]Padding data:  38%|███▊      | 778/2042 [00:01<00:02, 547.17it/s]Padding data:  41%|████      | 834/2042 [00:01<00:02, 549.31it/s]Padding data:  44%|████▎     | 890/2042 [00:01<00:02, 549.71it/s]Padding data:  46%|████▋     | 946/2042 [00:01<00:01, 551.41it/s]Padding data:  49%|████▉     | 1002/2042 [00:01<00:01, 551.29it/s]Padding data:  52%|█████▏    | 1058/2042 [00:01<00:01, 552.36it/s]Padding data:  55%|█████▍    | 1115/2042 [00:02<00:01, 556.25it/s]Padding data:  57%|█████▋    | 1173/2042 [00:02<00:01, 560.59it/s]Padding data:  60%|██████    | 1230/2042 [00:02<00:01, 561.93it/s]Padding data:  63%|██████▎   | 1287/2042 [00:02<00:01, 563.10it/s]Padding data:  66%|██████▌   | 1344/2042 [00:02<00:01, 562.86it/s]Padding data:  69%|██████▊   | 1401/2042 [00:02<00:01, 561.57it/s]Padding data:  71%|███████▏  | 1458/2042 [00:02<00:01, 561.13it/s]Padding data:  74%|███████▍  | 1515/2042 [00:02<00:00, 548.80it/s]Padding data:  77%|███████▋  | 1570/2042 [00:02<00:00, 544.28it/s]Padding data:  80%|███████▉  | 1625/2042 [00:02<00:00, 544.09it/s]Padding data:  82%|████████▏ | 1681/2042 [00:03<00:00, 547.78it/s]Padding data:  85%|████████▌ | 1737/2042 [00:03<00:00, 549.56it/s]Padding data:  88%|████████▊ | 1793/2042 [00:03<00:00, 549.93it/s]Padding data:  91%|█████████ | 1849/2042 [00:03<00:00, 547.34it/s]Padding data:  93%|█████████▎| 1905/2042 [00:03<00:00, 550.16it/s]Padding data:  96%|█████████▌| 1961/2042 [00:03<00:00, 552.62it/s]Padding data:  99%|█████████▉| 2017/2042 [00:03<00:00, 551.76it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 549.30it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 300, 7)]     0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 300, 64)      3648        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 300, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 300, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 300, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 300, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 300, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 300, 64)      512         input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 300, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 300, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 300, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 300, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 300, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 300, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 300, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 300, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 300, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 300, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 300, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 300, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 300, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 300, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 300, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 300, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 300, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 300, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 300, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 300, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 300, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 300, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 300, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 300, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 300, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 300, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 300, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 300, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 510,274
Trainable params: 507,714
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4407/4407 - 486s - loss: 0.6222
Epoch 2/100
4407/4407 - 451s - loss: 0.5830
Epoch 3/100
4407/4407 - 451s - loss: 0.5560
Epoch 4/100
4407/4407 - 450s - loss: 0.5303
Epoch 5/100
4407/4407 - 451s - loss: 0.5162
Epoch 6/100
4407/4407 - 451s - loss: 0.5101
Epoch 7/100
4407/4407 - 451s - loss: 0.5033
Epoch 8/100
4407/4407 - 451s - loss: 0.4965
Epoch 9/100
4407/4407 - 450s - loss: 0.4948
Epoch 10/100
4407/4407 - 452s - loss: 0.4906
Epoch 11/100
4407/4407 - 451s - loss: 0.4873
Epoch 12/100
4407/4407 - 453s - loss: 0.4827
Epoch 13/100
4407/4407 - 451s - loss: 0.4779
Epoch 14/100
4407/4407 - 452s - loss: 0.4744
Epoch 15/100
4407/4407 - 452s - loss: 0.4730
Epoch 16/100
4407/4407 - 453s - loss: 0.4659
Epoch 17/100
4407/4407 - 452s - loss: 0.4662
Epoch 18/100
4407/4407 - 452s - loss: 0.4666
Epoch 19/100
4407/4407 - 452s - loss: 0.4632
Epoch 20/100
4407/4407 - 453s - loss: 0.4602
Epoch 21/100
4407/4407 - 453s - loss: 0.4584
Epoch 22/100
4407/4407 - 452s - loss: 0.4570
Epoch 23/100
4407/4407 - 454s - loss: 0.4553
Epoch 24/100
4407/4407 - 454s - loss: 0.4526
Epoch 25/100
4407/4407 - 453s - loss: 0.4511
Epoch 26/100
4407/4407 - 453s - loss: 0.4488
Epoch 27/100
4407/4407 - 453s - loss: 0.4463
Epoch 28/100
4407/4407 - 454s - loss: 0.4446
Epoch 29/100
4407/4407 - 453s - loss: 0.4423
Epoch 30/100
4407/4407 - 453s - loss: 0.4415
Epoch 31/100
4407/4407 - 453s - loss: 0.4358
Epoch 32/100
4407/4407 - 453s - loss: 0.4342
Epoch 33/100
4407/4407 - 453s - loss: 0.4334
Epoch 34/100
4407/4407 - 453s - loss: 0.4320
Epoch 35/100
4407/4407 - 453s - loss: 0.4269
Epoch 36/100
4407/4407 - 453s - loss: 0.4292
Epoch 37/100
4407/4407 - 453s - loss: 0.4263
Epoch 38/100
4407/4407 - 452s - loss: 0.4233
Epoch 39/100
4407/4407 - 453s - loss: 0.4201
Epoch 40/100
4407/4407 - 453s - loss: 0.4189
Epoch 41/100
4407/4407 - 454s - loss: 0.4186
Epoch 42/100
4407/4407 - 452s - loss: 0.4136
Epoch 43/100
4407/4407 - 454s - loss: 0.4111
Epoch 44/100
4407/4407 - 452s - loss: 0.4092
Epoch 45/100
4407/4407 - 453s - loss: 0.4054
Epoch 46/100
4407/4407 - 453s - loss: 0.4007
Epoch 47/100
4407/4407 - 453s - loss: 0.4007
Epoch 48/100
4407/4407 - 453s - loss: 0.3982
Epoch 49/100
4407/4407 - 453s - loss: 0.3959
Epoch 50/100
4407/4407 - 453s - loss: 0.3904
Epoch 51/100
4407/4407 - 454s - loss: 0.3910
Epoch 52/100
4407/4407 - 454s - loss: 0.3859
Epoch 53/100
4407/4407 - 453s - loss: 0.3824
Epoch 54/100
4407/4407 - 453s - loss: 0.3811
Epoch 55/100
4407/4407 - 453s - loss: 0.3761
Epoch 56/100
4407/4407 - 453s - loss: 0.3766
Epoch 57/100
4407/4407 - 453s - loss: 0.3775
Epoch 58/100
4407/4407 - 453s - loss: 0.3723
Epoch 59/100
4407/4407 - 453s - loss: 0.3646
Epoch 60/100
4407/4407 - 454s - loss: 0.3618
Epoch 61/100
4407/4407 - 453s - loss: 0.3621
Epoch 62/100
4407/4407 - 452s - loss: 0.3576
Epoch 63/100
4407/4407 - 452s - loss: 0.3567
Epoch 64/100
4407/4407 - 453s - loss: 0.3534
Epoch 65/100
4407/4407 - 454s - loss: 0.3490
Epoch 66/100
4407/4407 - 454s - loss: 0.3485
Epoch 67/100
4407/4407 - 453s - loss: 0.3442
Epoch 68/100
4407/4407 - 453s - loss: 0.3384
Epoch 69/100
4407/4407 - 453s - loss: 0.3379
Epoch 70/100
4407/4407 - 455s - loss: 0.3301
Epoch 71/100
4407/4407 - 454s - loss: 0.3304
Epoch 72/100
4407/4407 - 454s - loss: 0.3296
Epoch 73/100
4407/4407 - 453s - loss: 0.3234
Epoch 74/100
4407/4407 - 453s - loss: 0.3183
Epoch 75/100
4407/4407 - 453s - loss: 0.3122
Epoch 76/100
4407/4407 - 453s - loss: 0.3167
Epoch 77/100
4407/4407 - 453s - loss: 0.3063
Epoch 78/100
4407/4407 - 453s - loss: 0.3064
Epoch 79/100
4407/4407 - 454s - loss: 0.3064
Epoch 80/100
4407/4407 - 455s - loss: 0.2971
Epoch 81/100
4407/4407 - 454s - loss: 0.2970
Epoch 82/100
4407/4407 - 453s - loss: 0.2893
Epoch 83/100
4407/4407 - 453s - loss: 0.2854
Epoch 84/100
4407/4407 - 454s - loss: 0.2850
Epoch 85/100
4407/4407 - 453s - loss: 0.2805
Epoch 86/100
4407/4407 - 453s - loss: 0.2727
Epoch 87/100
4407/4407 - 453s - loss: 0.2706
Epoch 88/100
4407/4407 - 454s - loss: 0.2707
Epoch 89/100
4407/4407 - 453s - loss: 0.2705
Epoch 90/100
4407/4407 - 455s - loss: 0.2584
Epoch 91/100
4407/4407 - 454s - loss: 0.2605
Epoch 92/100
4407/4407 - 454s - loss: 0.2530
Epoch 93/100
4407/4407 - 453s - loss: 0.2554
Epoch 94/100
4407/4407 - 454s - loss: 0.2480
Epoch 95/100
4407/4407 - 453s - loss: 0.2409
Epoch 96/100
4407/4407 - 454s - loss: 0.2457
Epoch 97/100
4407/4407 - 453s - loss: 0.2366
Epoch 98/100
4407/4407 - 454s - loss: 0.2373
Epoch 99/100
4407/4407 - 454s - loss: 0.2293
Epoch 100/100
4407/4407 - 454s - loss: 0.2253
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.5327185176128734
Test f1_avg: 0.5313988292918521
