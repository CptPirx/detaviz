INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 60/2042 [00:00<00:03, 591.23it/s]Padding data:   6%|▌         | 120/2042 [00:00<00:03, 590.68it/s]Padding data:   9%|▉         | 181/2042 [00:00<00:03, 597.10it/s]Padding data:  12%|█▏        | 241/2042 [00:00<00:03, 598.15it/s]Padding data:  15%|█▍        | 303/2042 [00:00<00:02, 602.60it/s]Padding data:  18%|█▊        | 364/2042 [00:00<00:02, 594.57it/s]Padding data:  21%|██        | 425/2042 [00:00<00:02, 599.09it/s]Padding data:  24%|██▍       | 486/2042 [00:00<00:02, 600.38it/s]Padding data:  27%|██▋       | 547/2042 [00:00<00:02, 601.70it/s]Padding data:  30%|██▉       | 609/2042 [00:01<00:02, 605.33it/s]Padding data:  33%|███▎      | 670/2042 [00:01<00:02, 601.68it/s]Padding data:  36%|███▌      | 732/2042 [00:01<00:02, 604.97it/s]Padding data:  39%|███▉      | 793/2042 [00:01<00:02, 603.05it/s]Padding data:  42%|████▏     | 855/2042 [00:01<00:01, 606.80it/s]Padding data:  45%|████▍     | 916/2042 [00:01<00:01, 599.64it/s]Padding data:  48%|████▊     | 977/2042 [00:01<00:01, 601.26it/s]Padding data:  51%|█████     | 1039/2042 [00:01<00:01, 604.72it/s]Padding data:  54%|█████▍    | 1101/2042 [00:01<00:01, 608.89it/s]Padding data:  57%|█████▋    | 1163/2042 [00:01<00:01, 610.96it/s]Padding data:  60%|█████▉    | 1225/2042 [00:02<00:01, 606.84it/s]Padding data:  63%|██████▎   | 1287/2042 [00:02<00:01, 608.53it/s]Padding data:  66%|██████▌   | 1349/2042 [00:02<00:01, 611.32it/s]Padding data:  69%|██████▉   | 1411/2042 [00:02<00:01, 603.33it/s]Padding data:  72%|███████▏  | 1472/2042 [00:02<00:00, 602.13it/s]Padding data:  75%|███████▌  | 1533/2042 [00:02<00:00, 602.30it/s]Padding data:  78%|███████▊  | 1594/2042 [00:02<00:00, 604.16it/s]Padding data:  81%|████████  | 1656/2042 [00:02<00:00, 607.65it/s]Padding data:  84%|████████▍ | 1719/2042 [00:02<00:00, 611.71it/s]Padding data:  87%|████████▋ | 1781/2042 [00:02<00:00, 612.62it/s]Padding data:  90%|█████████ | 1843/2042 [00:03<00:00, 610.04it/s]Padding data:  93%|█████████▎| 1905/2042 [00:03<00:00, 611.56it/s]Padding data:  96%|█████████▋| 1967/2042 [00:03<00:00, 610.75it/s]Padding data:  99%|█████████▉| 2029/2042 [00:03<00:00, 609.77it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 605.20it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            12900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,359
Trainable params: 20,359
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 26s - loss: 0.6414
Epoch 2/100
4383/4383 - 25s - loss: 0.5988
Epoch 3/100
4383/4383 - 25s - loss: 0.5755
Epoch 4/100
4383/4383 - 25s - loss: 0.5614
Epoch 5/100
4383/4383 - 25s - loss: 0.5558
Epoch 6/100
4383/4383 - 25s - loss: 0.5470
Epoch 7/100
4383/4383 - 26s - loss: 0.5430
Epoch 8/100
4383/4383 - 26s - loss: 0.5365
Epoch 9/100
4383/4383 - 26s - loss: 0.5334
Epoch 10/100
4383/4383 - 26s - loss: 0.5302
Epoch 11/100
4383/4383 - 26s - loss: 0.5219
Epoch 12/100
4383/4383 - 25s - loss: 0.5213
Epoch 13/100
4383/4383 - 26s - loss: 0.5161
Epoch 14/100
4383/4383 - 26s - loss: 0.5152
Epoch 15/100
4383/4383 - 26s - loss: 0.5125
Epoch 16/100
4383/4383 - 26s - loss: 0.5124
Epoch 17/100
4383/4383 - 26s - loss: 0.5104
Epoch 18/100
4383/4383 - 26s - loss: 0.5048
Epoch 19/100
4383/4383 - 26s - loss: 0.5040
Epoch 20/100
4383/4383 - 26s - loss: 0.5048
Epoch 21/100
4383/4383 - 26s - loss: 0.5026
Epoch 22/100
4383/4383 - 26s - loss: 0.5002
Epoch 23/100
4383/4383 - 26s - loss: 0.5006
Epoch 24/100
4383/4383 - 26s - loss: 0.4994
Epoch 25/100
4383/4383 - 26s - loss: 0.4968
Epoch 26/100
4383/4383 - 26s - loss: 0.4970
Epoch 27/100
4383/4383 - 26s - loss: 0.4950
Epoch 28/100
4383/4383 - 26s - loss: 0.4934
Epoch 29/100
4383/4383 - 25s - loss: 0.4943
Epoch 30/100
4383/4383 - 25s - loss: 0.4921
Epoch 31/100
4383/4383 - 26s - loss: 0.4939
Epoch 32/100
4383/4383 - 26s - loss: 0.4923
Epoch 33/100
4383/4383 - 25s - loss: 0.4902
Epoch 34/100
4383/4383 - 25s - loss: 0.4901
Epoch 35/100
4383/4383 - 25s - loss: 0.4906
Epoch 36/100
4383/4383 - 25s - loss: 0.4879
Epoch 37/100
4383/4383 - 25s - loss: 0.4915
Epoch 38/100
4383/4383 - 25s - loss: 0.4881
Epoch 39/100
4383/4383 - 26s - loss: 0.4881
Epoch 40/100
4383/4383 - 25s - loss: 0.4851
Epoch 41/100
4383/4383 - 25s - loss: 0.4854
Epoch 42/100
4383/4383 - 26s - loss: 0.4830
Epoch 43/100
4383/4383 - 26s - loss: 0.4837
Epoch 44/100
4383/4383 - 26s - loss: 0.4837
Epoch 45/100
4383/4383 - 25s - loss: 0.4836
Epoch 46/100
4383/4383 - 25s - loss: 0.4833

Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 47/100
4383/4383 - 25s - loss: 0.4708
Epoch 48/100
4383/4383 - 25s - loss: 0.4663
Epoch 49/100
4383/4383 - 26s - loss: 0.4658
Epoch 50/100
4383/4383 - 25s - loss: 0.4658
Epoch 51/100
4383/4383 - 25s - loss: 0.4664
Epoch 52/100
4383/4383 - 26s - loss: 0.4653
Epoch 53/100
4383/4383 - 26s - loss: 0.4644
Epoch 54/100
4383/4383 - 26s - loss: 0.4635
Epoch 55/100
4383/4383 - 26s - loss: 0.4638
Epoch 56/100
4383/4383 - 26s - loss: 0.4628
Epoch 57/100
4383/4383 - 26s - loss: 0.4649
Epoch 58/100
4383/4383 - 26s - loss: 0.4629
Epoch 59/100
4383/4383 - 26s - loss: 0.4638
Epoch 60/100
4383/4383 - 26s - loss: 0.4603
Epoch 61/100
4383/4383 - 26s - loss: 0.4629
Epoch 62/100
4383/4383 - 26s - loss: 0.4618
Epoch 63/100
4383/4383 - 26s - loss: 0.4624
Epoch 64/100
4383/4383 - 26s - loss: 0.4613

Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 65/100
4383/4383 - 26s - loss: 0.4532
Epoch 66/100
4383/4383 - 25s - loss: 0.4520
Epoch 67/100
4383/4383 - 25s - loss: 0.4527
Epoch 68/100
4383/4383 - 25s - loss: 0.4525
Epoch 69/100
4383/4383 - 25s - loss: 0.4516
Epoch 70/100
4383/4383 - 26s - loss: 0.4521
Epoch 71/100
4383/4383 - 26s - loss: 0.4520
Epoch 72/100
4383/4383 - 26s - loss: 0.4515
Epoch 73/100
4383/4383 - 26s - loss: 0.4515
Epoch 74/100
4383/4383 - 26s - loss: 0.4514
Epoch 75/100
4383/4383 - 26s - loss: 0.4498
Epoch 76/100
4383/4383 - 26s - loss: 0.4510
Epoch 77/100
4383/4383 - 26s - loss: 0.4503
Epoch 78/100
4383/4383 - 26s - loss: 0.4506
Epoch 79/100
4383/4383 - 26s - loss: 0.4510

Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 80/100
4383/4383 - 26s - loss: 0.4462
Epoch 81/100
4383/4383 - 26s - loss: 0.4446
Epoch 82/100
4383/4383 - 26s - loss: 0.4459
Epoch 83/100
4383/4383 - 26s - loss: 0.4449
Epoch 84/100
4383/4383 - 26s - loss: 0.4451
Epoch 85/100
4383/4383 - 26s - loss: 0.4439
Epoch 86/100
4383/4383 - 26s - loss: 0.4447
Epoch 87/100
4383/4383 - 26s - loss: 0.4445
Epoch 88/100
4383/4383 - 26s - loss: 0.4444
Epoch 89/100
4383/4383 - 26s - loss: 0.4450

Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0001.
Epoch 90/100
4383/4383 - 26s - loss: 0.4436
Epoch 91/100
4383/4383 - 26s - loss: 0.4431
Epoch 92/100
4383/4383 - 26s - loss: 0.4427
Epoch 93/100
4383/4383 - 26s - loss: 0.4429
Epoch 94/100
4383/4383 - 26s - loss: 0.4438
Epoch 95/100
4383/4383 - 26s - loss: 0.4432
Epoch 96/100
4383/4383 - 25s - loss: 0.4434
Epoch 97/100
4383/4383 - 25s - loss: 0.4427
Epoch 98/100
4383/4383 - 25s - loss: 0.4428
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.73215481231848
Test f1_avg: 0.7066868244159743
