INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 34/2042 [00:00<00:06, 330.98it/s]Padding data:   3%|3         | 68/2042 [00:00<00:05, 334.80it/s]Padding data:   5%|5         | 105/2042 [00:00<00:05, 348.02it/s]Padding data:   7%|7         | 144/2042 [00:00<00:05, 361.98it/s]Padding data:   9%|9         | 184/2042 [00:00<00:04, 373.25it/s]Padding data:  11%|#1        | 226/2042 [00:00<00:04, 388.01it/s]Padding data:  13%|#3        | 270/2042 [00:00<00:04, 403.21it/s]Padding data:  15%|#5        | 313/2042 [00:00<00:04, 408.16it/s]Padding data:  17%|#7        | 355/2042 [00:00<00:04, 409.61it/s]Padding data:  19%|#9        | 397/2042 [00:01<00:04, 410.61it/s]Padding data:  21%|##1       | 439/2042 [00:01<00:03, 401.70it/s]Padding data:  24%|##3       | 480/2042 [00:01<00:03, 392.85it/s]Padding data:  25%|##5       | 520/2042 [00:01<00:03, 391.86it/s]Padding data:  28%|##7       | 565/2042 [00:01<00:03, 405.57it/s]Padding data:  30%|##9       | 607/2042 [00:01<00:03, 408.92it/s]Padding data:  32%|###1      | 649/2042 [00:01<00:03, 410.11it/s]Padding data:  34%|###3      | 692/2042 [00:01<00:03, 412.69it/s]Padding data:  36%|###5      | 735/2042 [00:01<00:03, 414.43it/s]Padding data:  38%|###8      | 778/2042 [00:01<00:03, 416.88it/s]Padding data:  40%|####      | 820/2042 [00:02<00:02, 415.70it/s]Padding data:  42%|####2     | 862/2042 [00:02<00:02, 406.51it/s]Padding data:  44%|####4     | 903/2042 [00:02<00:02, 400.84it/s]Padding data:  46%|####6     | 944/2042 [00:02<00:02, 398.07it/s]Padding data:  48%|####8     | 987/2042 [00:02<00:02, 406.45it/s]Padding data:  50%|#####     | 1029/2042 [00:02<00:02, 408.35it/s]Padding data:  52%|#####2    | 1071/2042 [00:02<00:02, 408.51it/s]Padding data:  55%|#####4    | 1115/2042 [00:02<00:02, 415.65it/s]Padding data:  57%|#####6    | 1158/2042 [00:02<00:02, 418.08it/s]Padding data:  59%|#####8    | 1200/2042 [00:02<00:02, 417.71it/s]Padding data:  61%|######    | 1242/2042 [00:03<00:01, 410.26it/s]Padding data:  63%|######2   | 1284/2042 [00:03<00:01, 405.14it/s]Padding data:  65%|######4   | 1325/2042 [00:03<00:01, 401.01it/s]Padding data:  67%|######6   | 1367/2042 [00:03<00:01, 404.53it/s]Padding data:  69%|######9   | 1411/2042 [00:03<00:01, 412.78it/s]Padding data:  71%|#######1  | 1454/2042 [00:03<00:01, 415.77it/s]Padding data:  73%|#######3  | 1497/2042 [00:03<00:01, 417.55it/s]Padding data:  75%|#######5  | 1539/2042 [00:03<00:01, 417.36it/s]Padding data:  77%|#######7  | 1582/2042 [00:03<00:01, 418.96it/s]Padding data:  80%|#######9  | 1624/2042 [00:04<00:01, 417.13it/s]Padding data:  82%|########1 | 1666/2042 [00:04<00:00, 413.42it/s]Padding data:  84%|########3 | 1708/2042 [00:04<00:00, 408.27it/s]Padding data:  86%|########5 | 1749/2042 [00:04<00:00, 397.51it/s]Padding data:  88%|########7 | 1789/2042 [00:04<00:00, 388.42it/s]Padding data:  90%|########9 | 1829/2042 [00:04<00:00, 390.92it/s]Padding data:  92%|#########1| 1872/2042 [00:04<00:00, 401.42it/s]Padding data:  94%|#########3| 1914/2042 [00:04<00:00, 404.82it/s]Padding data:  96%|#########5| 1957/2042 [00:04<00:00, 408.95it/s]Padding data:  98%|#########7| 2000/2042 [00:04<00:00, 413.04it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 410.57it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 404.34it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            12900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 4)                 251       
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0         
=================================================================
Total params: 20,481
Trainable params: 20,481
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4388/4388 - 48s - loss: 1.1088
Epoch 2/100
4388/4388 - 46s - loss: 0.9299
Epoch 3/100
4388/4388 - 46s - loss: 0.8946
Epoch 4/100
4388/4388 - 46s - loss: 0.8650
Epoch 5/100
4388/4388 - 46s - loss: 0.8453
Epoch 6/100
4388/4388 - 46s - loss: 0.8281
Epoch 7/100
4388/4388 - 46s - loss: 0.8169
Epoch 8/100
4388/4388 - 46s - loss: 0.8037
Epoch 9/100
4388/4388 - 45s - loss: 0.7956
Epoch 10/100
4388/4388 - 46s - loss: 0.7862
Epoch 11/100
4388/4388 - 45s - loss: 0.7788
Epoch 12/100
4388/4388 - 46s - loss: 0.7733
Epoch 13/100
4388/4388 - 45s - loss: 0.7657
Epoch 14/100
4388/4388 - 46s - loss: 0.7616
Epoch 15/100
4388/4388 - 46s - loss: 0.7627
Epoch 16/100
4388/4388 - 46s - loss: 0.7559
Epoch 17/100
4388/4388 - 46s - loss: 0.7520
Epoch 18/100
4388/4388 - 46s - loss: 0.7480
Epoch 19/100
4388/4388 - 46s - loss: 0.7457
Epoch 20/100
4388/4388 - 46s - loss: 0.7420
Epoch 21/100
4388/4388 - 46s - loss: 0.7369
Epoch 22/100
4388/4388 - 46s - loss: 0.7396
Epoch 23/100
4388/4388 - 46s - loss: 0.7338
Epoch 24/100
4388/4388 - 47s - loss: 0.7297
Epoch 25/100
4388/4388 - 46s - loss: 0.7282
Epoch 26/100
4388/4388 - 46s - loss: 0.7246
Epoch 27/100
4388/4388 - 46s - loss: 0.7230
Epoch 28/100
4388/4388 - 46s - loss: 0.7209
Epoch 29/100
4388/4388 - 46s - loss: 0.7195
Epoch 30/100
4388/4388 - 46s - loss: 0.7179
Epoch 31/100
4388/4388 - 48s - loss: 0.7139
Epoch 32/100
4388/4388 - 51s - loss: 0.7114
Epoch 33/100
4388/4388 - 96s - loss: 0.7101
Epoch 34/100
4388/4388 - 49s - loss: 0.7117
Epoch 35/100
4388/4388 - 48s - loss: 0.7072
Epoch 36/100
4388/4388 - 49s - loss: 0.7082
Epoch 37/100
4388/4388 - 50s - loss: 0.7060
Epoch 38/100
4388/4388 - 50s - loss: 0.7029
Epoch 39/100
4388/4388 - 50s - loss: 0.7001
Epoch 40/100
4388/4388 - 49s - loss: 0.6970
Epoch 41/100
4388/4388 - 48s - loss: 0.6984
Epoch 42/100
4388/4388 - 48s - loss: 0.6976
Epoch 43/100
4388/4388 - 47s - loss: 0.6977
Epoch 44/100
4388/4388 - 48s - loss: 0.6924
Epoch 45/100
4388/4388 - 48s - loss: 0.6933
Epoch 46/100
4388/4388 - 49s - loss: 0.6950
Epoch 47/100
4388/4388 - 49s - loss: 0.6921
Epoch 48/100
4388/4388 - 48s - loss: 0.6924
Epoch 49/100
4388/4388 - 48s - loss: 0.6880
Epoch 50/100
4388/4388 - 48s - loss: 0.6890
Epoch 51/100
4388/4388 - 48s - loss: 0.6858
Epoch 52/100
4388/4388 - 48s - loss: 0.6842
Epoch 53/100
4388/4388 - 48s - loss: 0.6833
Epoch 54/100
4388/4388 - 48s - loss: 0.6870
Epoch 55/100
4388/4388 - 48s - loss: 0.6812
Epoch 56/100
4388/4388 - 48s - loss: 0.6813
Epoch 57/100
4388/4388 - 48s - loss: 0.6815
Epoch 58/100
4388/4388 - 48s - loss: 0.6797
Epoch 59/100
4388/4388 - 47s - loss: 0.6806
Epoch 60/100
4388/4388 - 47s - loss: 0.6813
Epoch 61/100
4388/4388 - 48s - loss: 0.6773
Epoch 62/100
4388/4388 - 47s - loss: 0.6770
Epoch 63/100
4388/4388 - 48s - loss: 0.6750
Epoch 64/100
4388/4388 - 49s - loss: 0.6747
Epoch 65/100
4388/4388 - 49s - loss: 0.6752
Epoch 66/100
4388/4388 - 47s - loss: 0.6729
Epoch 67/100
4388/4388 - 48s - loss: 0.6725
Epoch 68/100
4388/4388 - 48s - loss: 0.6722
Epoch 69/100
4388/4388 - 48s - loss: 0.6695
Epoch 70/100
4388/4388 - 48s - loss: 0.6740
Epoch 71/100
4388/4388 - 48s - loss: 0.6690
Epoch 72/100
4388/4388 - 48s - loss: 0.6703
Epoch 73/100
4388/4388 - 48s - loss: 0.6649
Epoch 74/100
4388/4388 - 48s - loss: 0.6699
Epoch 75/100
4388/4388 - 48s - loss: 0.6692
Epoch 76/100
4388/4388 - 49s - loss: 0.6645
Epoch 77/100
4388/4388 - 48s - loss: 0.6688
Epoch 78/100
4388/4388 - 48s - loss: 0.6664
Epoch 79/100
4388/4388 - 48s - loss: 0.6613
Epoch 80/100
4388/4388 - 48s - loss: 0.6647
Epoch 81/100
4388/4388 - 48s - loss: 0.6604
Epoch 82/100
4388/4388 - 48s - loss: 0.6597
Epoch 83/100
4388/4388 - 48s - loss: 0.6650
Epoch 84/100
4388/4388 - 48s - loss: 0.6632
Epoch 85/100
4388/4388 - 48s - loss: 0.6642
Epoch 86/100
4388/4388 - 48s - loss: 0.6630

Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 87/100
4388/4388 - 48s - loss: 0.6327
Epoch 88/100
4388/4388 - 48s - loss: 0.6267
Epoch 89/100
4388/4388 - 48s - loss: 0.6235
Epoch 90/100
4388/4388 - 49s - loss: 0.6275
Epoch 91/100
4388/4388 - 48s - loss: 0.6260
Epoch 92/100
4388/4388 - 48s - loss: 0.6253
Epoch 93/100
4388/4388 - 49s - loss: 0.6231
Epoch 94/100
4388/4388 - 49s - loss: 0.6233
Epoch 95/100
4388/4388 - 48s - loss: 0.6257
Epoch 96/100
4388/4388 - 47s - loss: 0.6224
Epoch 97/100
4388/4388 - 46s - loss: 0.6241
Epoch 98/100
4388/4388 - 46s - loss: 0.6226
Epoch 99/100
4388/4388 - 46s - loss: 0.6202
Epoch 100/100
4388/4388 - 46s - loss: 0.6212
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.5473230658948907
Test f1_avg: 0.4759626786898222
