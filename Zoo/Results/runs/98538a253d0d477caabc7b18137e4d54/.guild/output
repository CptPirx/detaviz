INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 60/2042 [00:00<00:03, 596.10it/s]Padding data:   6%|▌         | 121/2042 [00:00<00:03, 603.99it/s]Padding data:   9%|▉         | 182/2042 [00:00<00:03, 602.91it/s]Padding data:  12%|█▏        | 243/2042 [00:00<00:02, 601.31it/s]Padding data:  15%|█▍        | 304/2042 [00:00<00:02, 603.48it/s]Padding data:  18%|█▊        | 366/2042 [00:00<00:02, 606.24it/s]Padding data:  21%|██        | 427/2042 [00:00<00:02, 606.57it/s]Padding data:  24%|██▍       | 488/2042 [00:00<00:02, 604.39it/s]Padding data:  27%|██▋       | 550/2042 [00:00<00:02, 606.78it/s]Padding data:  30%|███       | 613/2042 [00:01<00:02, 611.94it/s]Padding data:  33%|███▎      | 675/2042 [00:01<00:02, 612.99it/s]Padding data:  36%|███▌      | 737/2042 [00:01<00:02, 610.51it/s]Padding data:  39%|███▉      | 799/2042 [00:01<00:02, 611.36it/s]Padding data:  42%|████▏     | 861/2042 [00:01<00:01, 610.93it/s]Padding data:  45%|████▌     | 923/2042 [00:01<00:01, 611.07it/s]Padding data:  48%|████▊     | 985/2042 [00:01<00:01, 612.14it/s]Padding data:  51%|█████▏    | 1048/2042 [00:01<00:01, 614.67it/s]Padding data:  54%|█████▍    | 1110/2042 [00:01<00:01, 613.61it/s]Padding data:  57%|█████▋    | 1172/2042 [00:01<00:01, 613.97it/s]Padding data:  60%|██████    | 1235/2042 [00:02<00:01, 616.43it/s]Padding data:  64%|██████▎   | 1297/2042 [00:02<00:01, 616.73it/s]Padding data:  67%|██████▋   | 1360/2042 [00:02<00:01, 618.51it/s]Padding data:  70%|██████▉   | 1422/2042 [00:02<00:01, 615.37it/s]Padding data:  73%|███████▎  | 1484/2042 [00:02<00:00, 614.83it/s]Padding data:  76%|███████▌  | 1546/2042 [00:02<00:00, 613.03it/s]Padding data:  79%|███████▊  | 1608/2042 [00:02<00:00, 613.79it/s]Padding data:  82%|████████▏ | 1671/2042 [00:02<00:00, 615.69it/s]Padding data:  85%|████████▍ | 1733/2042 [00:02<00:00, 614.41it/s]Padding data:  88%|████████▊ | 1796/2042 [00:02<00:00, 617.95it/s]Padding data:  91%|█████████ | 1858/2042 [00:03<00:00, 615.47it/s]Padding data:  94%|█████████▍| 1920/2042 [00:03<00:00, 614.30it/s]Padding data:  97%|█████████▋| 1982/2042 [00:03<00:00, 614.33it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 611.95it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 200, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            24900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 32,359
Trainable params: 32,359
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 37s - loss: 0.6581
Epoch 2/100
4383/4383 - 36s - loss: 0.5983
Epoch 3/100
4383/4383 - 36s - loss: 0.5829
Epoch 4/100
4383/4383 - 36s - loss: 0.5723
Epoch 5/100
4383/4383 - 37s - loss: 0.5622
Epoch 6/100
4383/4383 - 36s - loss: 0.5556
Epoch 7/100
4383/4383 - 36s - loss: 0.5480
Epoch 8/100
4383/4383 - 36s - loss: 0.5410
Epoch 9/100
4383/4383 - 36s - loss: 0.5351
Epoch 10/100
4383/4383 - 37s - loss: 0.5270
Epoch 11/100
4383/4383 - 37s - loss: 0.5229
Epoch 12/100
4383/4383 - 36s - loss: 0.5194
Epoch 13/100
4383/4383 - 37s - loss: 0.5153
Epoch 14/100
4383/4383 - 37s - loss: 0.5094
Epoch 15/100
4383/4383 - 37s - loss: 0.5107
Epoch 16/100
4383/4383 - 36s - loss: 0.5043
Epoch 17/100
4383/4383 - 37s - loss: 0.5013
Epoch 18/100
4383/4383 - 37s - loss: 0.5017
Epoch 19/100
4383/4383 - 37s - loss: 0.4999
Epoch 20/100
4383/4383 - 37s - loss: 0.4968
Epoch 21/100
4383/4383 - 37s - loss: 0.4922
Epoch 22/100
4383/4383 - 37s - loss: 0.4914
Epoch 23/100
4383/4383 - 37s - loss: 0.4917
Epoch 24/100
4383/4383 - 37s - loss: 0.4916
Epoch 25/100
4383/4383 - 37s - loss: 0.4868
Epoch 26/100
4383/4383 - 37s - loss: 0.4866
Epoch 27/100
4383/4383 - 37s - loss: 0.4846
Epoch 28/100
4383/4383 - 36s - loss: 0.4819
Epoch 29/100
4383/4383 - 37s - loss: 0.4819
Epoch 30/100
4383/4383 - 37s - loss: 0.4826
Epoch 31/100
4383/4383 - 37s - loss: 0.4772
Epoch 32/100
4383/4383 - 37s - loss: 0.4758
Epoch 33/100
4383/4383 - 37s - loss: 0.4741
Epoch 34/100
4383/4383 - 37s - loss: 0.4728
Epoch 35/100
4383/4383 - 37s - loss: 0.4740
Epoch 36/100
4383/4383 - 36s - loss: 0.4716
Epoch 37/100
4383/4383 - 36s - loss: 0.4716
Epoch 38/100
4383/4383 - 37s - loss: 0.4676
Epoch 39/100
4383/4383 - 36s - loss: 0.4648
Epoch 40/100
4383/4383 - 36s - loss: 0.4678
Epoch 41/100
4383/4383 - 36s - loss: 0.4667
Epoch 42/100
4383/4383 - 36s - loss: 0.4660
Epoch 43/100
4383/4383 - 36s - loss: 0.4640
Epoch 44/100
4383/4383 - 36s - loss: 0.4632
Epoch 45/100
4383/4383 - 36s - loss: 0.4603
Epoch 46/100
4383/4383 - 36s - loss: 0.4604
Epoch 47/100
4383/4383 - 36s - loss: 0.4578
Epoch 48/100
4383/4383 - 36s - loss: 0.4578
Epoch 49/100
4383/4383 - 36s - loss: 0.4560
Epoch 50/100
4383/4383 - 36s - loss: 0.4561
Epoch 51/100
4383/4383 - 36s - loss: 0.4542
Epoch 52/100
4383/4383 - 36s - loss: 0.4545
Epoch 53/100
4383/4383 - 36s - loss: 0.4574
Epoch 54/100
4383/4383 - 36s - loss: 0.4533
Epoch 55/100
4383/4383 - 36s - loss: 0.4548
Epoch 56/100
4383/4383 - 36s - loss: 0.4528
Epoch 57/100
4383/4383 - 36s - loss: 0.4543
Epoch 58/100
4383/4383 - 36s - loss: 0.4525
Epoch 59/100
4383/4383 - 36s - loss: 0.4525
Epoch 60/100
4383/4383 - 36s - loss: 0.4492
Epoch 61/100
4383/4383 - 36s - loss: 0.4516
Epoch 62/100
4383/4383 - 36s - loss: 0.4491
Epoch 63/100
4383/4383 - 36s - loss: 0.4493
Epoch 64/100
4383/4383 - 36s - loss: 0.4484
Epoch 65/100
4383/4383 - 36s - loss: 0.4516
Epoch 66/100
4383/4383 - 36s - loss: 0.4456
Epoch 67/100
4383/4383 - 36s - loss: 0.4457
Epoch 68/100
4383/4383 - 36s - loss: 0.4467
Epoch 69/100
4383/4383 - 36s - loss: 0.4486
Epoch 70/100
4383/4383 - 36s - loss: 0.4459

Epoch 00070: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 71/100
4383/4383 - 36s - loss: 0.4291
Epoch 72/100
4383/4383 - 36s - loss: 0.4257
Epoch 73/100
4383/4383 - 37s - loss: 0.4247
Epoch 74/100
4383/4383 - 36s - loss: 0.4259
Epoch 75/100
4383/4383 - 36s - loss: 0.4260
Epoch 76/100
4383/4383 - 36s - loss: 0.4236
Epoch 77/100
4383/4383 - 36s - loss: 0.4236
Epoch 78/100
4383/4383 - 36s - loss: 0.4228
Epoch 79/100
4383/4383 - 36s - loss: 0.4230
Epoch 80/100
4383/4383 - 36s - loss: 0.4223
Epoch 81/100
4383/4383 - 36s - loss: 0.4227
Epoch 82/100
4383/4383 - 36s - loss: 0.4225
Epoch 83/100
4383/4383 - 36s - loss: 0.4225
Epoch 84/100
4383/4383 - 36s - loss: 0.4179
Epoch 85/100
4383/4383 - 36s - loss: 0.4214
Epoch 86/100
4383/4383 - 36s - loss: 0.4214
Epoch 87/100
4383/4383 - 36s - loss: 0.4221
Epoch 88/100
4383/4383 - 37s - loss: 0.4212

Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 89/100
4383/4383 - 36s - loss: 0.4115
Epoch 90/100
4383/4383 - 36s - loss: 0.4106
Epoch 91/100
4383/4383 - 36s - loss: 0.4102
Epoch 92/100
4383/4383 - 36s - loss: 0.4098
Epoch 93/100
4383/4383 - 36s - loss: 0.4098
Epoch 94/100
4383/4383 - 36s - loss: 0.4089
Epoch 95/100
4383/4383 - 36s - loss: 0.4101
Epoch 96/100
4383/4383 - 36s - loss: 0.4086
Epoch 97/100
4383/4383 - 36s - loss: 0.4090
Epoch 98/100
4383/4383 - 37s - loss: 0.4088
Epoch 99/100
4383/4383 - 36s - loss: 0.4078
Epoch 100/100
4383/4383 - 36s - loss: 0.4100
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.7480119024330312
Test f1_avg: 0.7110678404471908
