INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 36/2042 [00:00<00:05, 350.45it/s]Padding data:   4%|3         | 74/2042 [00:00<00:05, 363.98it/s]Padding data:   6%|5         | 113/2042 [00:00<00:05, 372.82it/s]Padding data:   7%|7         | 152/2042 [00:00<00:04, 378.44it/s]Padding data:   9%|9         | 192/2042 [00:00<00:04, 383.09it/s]Padding data:  11%|#1        | 233/2042 [00:00<00:04, 389.87it/s]Padding data:  13%|#3        | 275/2042 [00:00<00:04, 398.67it/s]Padding data:  15%|#5        | 315/2042 [00:00<00:04, 398.17it/s]Padding data:  17%|#7        | 356/2042 [00:00<00:04, 400.93it/s]Padding data:  19%|#9        | 397/2042 [00:01<00:04, 402.81it/s]Padding data:  21%|##1       | 438/2042 [00:01<00:03, 402.88it/s]Padding data:  23%|##3       | 479/2042 [00:01<00:03, 404.14it/s]Padding data:  25%|##5       | 520/2042 [00:01<00:03, 401.40it/s]Padding data:  27%|##7       | 561/2042 [00:01<00:03, 397.20it/s]Padding data:  29%|##9       | 601/2042 [00:01<00:03, 395.58it/s]Padding data:  31%|###1      | 641/2042 [00:01<00:03, 395.87it/s]Padding data:  33%|###3      | 682/2042 [00:01<00:03, 398.02it/s]Padding data:  35%|###5      | 722/2042 [00:01<00:03, 396.57it/s]Padding data:  37%|###7      | 762/2042 [00:01<00:03, 395.56it/s]Padding data:  39%|###9      | 802/2042 [00:02<00:03, 394.86it/s]Padding data:  41%|####1     | 843/2042 [00:02<00:03, 398.48it/s]Padding data:  43%|####3     | 884/2042 [00:02<00:02, 399.85it/s]Padding data:  45%|####5     | 925/2042 [00:02<00:02, 401.39it/s]Padding data:  47%|####7     | 967/2042 [00:02<00:02, 404.84it/s]Padding data:  49%|####9     | 1008/2042 [00:02<00:02, 403.11it/s]Padding data:  51%|#####1    | 1049/2042 [00:02<00:02, 401.91it/s]Padding data:  53%|#####3    | 1090/2042 [00:02<00:02, 403.43it/s]Padding data:  55%|#####5    | 1131/2042 [00:02<00:02, 403.31it/s]Padding data:  57%|#####7    | 1172/2042 [00:02<00:02, 401.88it/s]Padding data:  59%|#####9    | 1213/2042 [00:03<00:02, 403.41it/s]Padding data:  61%|######1   | 1254/2042 [00:03<00:01, 401.52it/s]Padding data:  63%|######3   | 1295/2042 [00:03<00:01, 403.15it/s]Padding data:  65%|######5   | 1336/2042 [00:03<00:01, 402.43it/s]Padding data:  67%|######7   | 1377/2042 [00:03<00:01, 396.78it/s]Padding data:  69%|######9   | 1417/2042 [00:03<00:01, 395.72it/s]Padding data:  71%|#######1  | 1458/2042 [00:03<00:01, 397.90it/s]Padding data:  73%|#######3  | 1498/2042 [00:03<00:01, 397.66it/s]Padding data:  75%|#######5  | 1539/2042 [00:03<00:01, 400.45it/s]Padding data:  77%|#######7  | 1580/2042 [00:03<00:01, 401.22it/s]Padding data:  79%|#######9  | 1621/2042 [00:04<00:01, 402.94it/s]Padding data:  81%|########1 | 1663/2042 [00:04<00:00, 404.73it/s]Padding data:  83%|########3 | 1704/2042 [00:04<00:00, 402.67it/s]Padding data:  85%|########5 | 1745/2042 [00:04<00:00, 403.96it/s]Padding data:  87%|########7 | 1786/2042 [00:04<00:00, 402.50it/s]Padding data:  89%|########9 | 1827/2042 [00:04<00:00, 401.48it/s]Padding data:  91%|#########1| 1868/2042 [00:04<00:00, 401.95it/s]Padding data:  93%|#########3| 1909/2042 [00:04<00:00, 402.27it/s]Padding data:  95%|#########5| 1950/2042 [00:04<00:00, 402.50it/s]Padding data:  98%|#########7| 1991/2042 [00:04<00:00, 402.66it/s]Padding data: 100%|#########9| 2032/2042 [00:05<00:00, 402.77it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 398.88it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 200, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            24900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 4)                 251       
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0         
=================================================================
Total params: 32,481
Trainable params: 32,481
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4388/4388 - 79s - loss: 1.0107
Epoch 2/100
4388/4388 - 78s - loss: 0.8819
Epoch 3/100
4388/4388 - 75s - loss: 0.8524
Epoch 4/100
4388/4388 - 75s - loss: 0.8297
Epoch 5/100
4388/4388 - 76s - loss: 0.8102
Epoch 6/100
4388/4388 - 75s - loss: 0.7910
Epoch 7/100
4388/4388 - 75s - loss: 0.7768
Epoch 8/100
4388/4388 - 76s - loss: 0.7685
Epoch 9/100
4388/4388 - 75s - loss: 0.7583
Epoch 10/100
4388/4388 - 75s - loss: 0.7516
Epoch 11/100
4388/4388 - 75s - loss: 0.7419
Epoch 12/100
4388/4388 - 75s - loss: 0.7376
Epoch 13/100
4388/4388 - 75s - loss: 0.7351
Epoch 14/100
4388/4388 - 75s - loss: 0.7300
Epoch 15/100
4388/4388 - 75s - loss: 0.7249
Epoch 16/100
4388/4388 - 75s - loss: 0.7224
Epoch 17/100
4388/4388 - 75s - loss: 0.7177
Epoch 18/100
4388/4388 - 75s - loss: 0.7167
Epoch 19/100
4388/4388 - 75s - loss: 0.7119
Epoch 20/100
4388/4388 - 75s - loss: 0.7120
Epoch 21/100
4388/4388 - 75s - loss: 0.7116
Epoch 22/100
4388/4388 - 75s - loss: 0.7071
Epoch 23/100
4388/4388 - 75s - loss: 0.7100
Epoch 24/100
4388/4388 - 75s - loss: 0.7089
Epoch 25/100
4388/4388 - 75s - loss: 0.7035
Epoch 26/100
4388/4388 - 75s - loss: 0.7040
Epoch 27/100
4388/4388 - 75s - loss: 0.7028
Epoch 28/100
4388/4388 - 75s - loss: 0.6993
Epoch 29/100
4388/4388 - 75s - loss: 0.7009
Epoch 30/100
4388/4388 - 75s - loss: 0.7014
Epoch 31/100
4388/4388 - 75s - loss: 0.6963
Epoch 32/100
4388/4388 - 75s - loss: 0.6929
Epoch 33/100
4388/4388 - 75s - loss: 0.6949
Epoch 34/100
4388/4388 - 75s - loss: 0.6930
Epoch 35/100
4388/4388 - 74s - loss: 0.6932
Epoch 36/100
4388/4388 - 75s - loss: 0.6911
Epoch 37/100
4388/4388 - 75s - loss: 0.6899
Epoch 38/100
4388/4388 - 75s - loss: 0.6903
Epoch 39/100
4388/4388 - 75s - loss: 0.6849
Epoch 40/100
4388/4388 - 76s - loss: 0.6881
Epoch 41/100
4388/4388 - 75s - loss: 0.6857
Epoch 42/100
4388/4388 - 75s - loss: 0.6833
Epoch 43/100
4388/4388 - 75s - loss: 0.6847
Epoch 44/100
4388/4388 - 75s - loss: 0.6855
Epoch 45/100
4388/4388 - 75s - loss: 0.6847
Epoch 46/100
4388/4388 - 75s - loss: 0.6810
Epoch 47/100
4388/4388 - 75s - loss: 0.6826
Epoch 48/100
4388/4388 - 75s - loss: 0.6823
Epoch 49/100
4388/4388 - 75s - loss: 0.6819
Epoch 50/100
4388/4388 - 75s - loss: 0.6800
Epoch 51/100
4388/4388 - 75s - loss: 0.6770
Epoch 52/100
4388/4388 - 75s - loss: 0.6772
Epoch 53/100
4388/4388 - 75s - loss: 0.6801
Epoch 54/100
4388/4388 - 75s - loss: 0.6763
Epoch 55/100
4388/4388 - 75s - loss: 0.6753
Epoch 56/100
4388/4388 - 75s - loss: 0.6727
Epoch 57/100
4388/4388 - 75s - loss: 0.6769
Epoch 58/100
4388/4388 - 75s - loss: 0.6711
Epoch 59/100
4388/4388 - 74s - loss: 0.6708
Epoch 60/100
4388/4388 - 75s - loss: 0.6727
Epoch 61/100
4388/4388 - 75s - loss: 0.6676
Epoch 62/100
4388/4388 - 75s - loss: 0.6675
Epoch 63/100
4388/4388 - 75s - loss: 0.6684
Epoch 64/100
4388/4388 - 75s - loss: 0.6713
Epoch 65/100
4388/4388 - 75s - loss: 0.6675
Epoch 66/100
4388/4388 - 75s - loss: 0.6676

Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 67/100
4388/4388 - 75s - loss: 0.6445
Epoch 68/100
4388/4388 - 75s - loss: 0.6408
Epoch 69/100
4388/4388 - 75s - loss: 0.6389
Epoch 70/100
4388/4388 - 75s - loss: 0.6396
Epoch 71/100
4388/4388 - 75s - loss: 0.6379
Epoch 72/100
4388/4388 - 75s - loss: 0.6376
Epoch 73/100
4388/4388 - 78s - loss: 0.6355
Epoch 74/100
4388/4388 - 78s - loss: 0.6347
Epoch 75/100
4388/4388 - 76s - loss: 0.6367
Epoch 76/100
4388/4388 - 75s - loss: 0.6346
Epoch 77/100
4388/4388 - 75s - loss: 0.6340
Epoch 78/100
4388/4388 - 75s - loss: 0.6350
Epoch 79/100
4388/4388 - 75s - loss: 0.6336
Epoch 80/100
4388/4388 - 75s - loss: 0.6337
Epoch 81/100
4388/4388 - 75s - loss: 0.6300
Epoch 82/100
4388/4388 - 75s - loss: 0.6324
Epoch 83/100
4388/4388 - 75s - loss: 0.6318
Epoch 84/100
4388/4388 - 75s - loss: 0.6309
Epoch 85/100
4388/4388 - 75s - loss: 0.6312

Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 86/100
4388/4388 - 75s - loss: 0.6172
Epoch 87/100
4388/4388 - 75s - loss: 0.6163
Epoch 88/100
4388/4388 - 75s - loss: 0.6157
Epoch 89/100
4388/4388 - 75s - loss: 0.6154
Epoch 90/100
4388/4388 - 75s - loss: 0.6142
Epoch 91/100
4388/4388 - 75s - loss: 0.6141
Epoch 92/100
4388/4388 - 75s - loss: 0.6145
Epoch 93/100
4388/4388 - 75s - loss: 0.6131
Epoch 94/100
4388/4388 - 75s - loss: 0.6140
Epoch 95/100
4388/4388 - 75s - loss: 0.6127
Epoch 96/100
4388/4388 - 76s - loss: 0.6108
Epoch 97/100
4388/4388 - 75s - loss: 0.6120
Epoch 98/100
4388/4388 - 74s - loss: 0.6126
Epoch 99/100
4388/4388 - 74s - loss: 0.6117
Epoch 100/100
4388/4388 - 75s - loss: 0.6113

Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.542474453868288
Test f1_avg: 0.48318533087420773
