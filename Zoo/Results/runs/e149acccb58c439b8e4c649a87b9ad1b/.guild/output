INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 35/2042 [00:00<00:05, 344.10it/s]Padding data:   3%|3         | 71/2042 [00:00<00:05, 351.89it/s]Padding data:   5%|5         | 108/2042 [00:00<00:05, 357.30it/s]Padding data:   7%|7         | 145/2042 [00:00<00:05, 359.84it/s]Padding data:   9%|8         | 181/2042 [00:00<00:05, 358.96it/s]Padding data:  11%|#         | 217/2042 [00:00<00:05, 349.06it/s]Padding data:  13%|#2        | 260/2042 [00:00<00:04, 373.26it/s]Padding data:  15%|#4        | 298/2042 [00:00<00:04, 373.35it/s]Padding data:  17%|#6        | 339/2042 [00:00<00:04, 381.41it/s]Padding data:  19%|#8        | 378/2042 [00:01<00:04, 379.73it/s]Padding data:  20%|##        | 416/2042 [00:01<00:04, 373.41it/s]Padding data:  22%|##2       | 456/2042 [00:01<00:04, 380.47it/s]Padding data:  24%|##4       | 498/2042 [00:01<00:03, 389.06it/s]Padding data:  26%|##6       | 539/2042 [00:01<00:03, 392.10it/s]Padding data:  28%|##8       | 580/2042 [00:01<00:03, 396.53it/s]Padding data:  30%|###       | 620/2042 [00:01<00:03, 395.53it/s]Padding data:  32%|###2      | 660/2042 [00:01<00:03, 394.84it/s]Padding data:  34%|###4      | 701/2042 [00:01<00:03, 396.13it/s]Padding data:  36%|###6      | 741/2042 [00:01<00:03, 395.26it/s]Padding data:  38%|###8      | 781/2042 [00:02<00:03, 395.80it/s]Padding data:  40%|####      | 822/2042 [00:02<00:03, 399.15it/s]Padding data:  42%|####2     | 862/2042 [00:02<00:02, 397.36it/s]Padding data:  44%|####4     | 902/2042 [00:02<00:02, 397.28it/s]Padding data:  46%|####6     | 943/2042 [00:02<00:02, 397.84it/s]Padding data:  48%|####8     | 984/2042 [00:02<00:02, 399.40it/s]Padding data:  50%|#####     | 1025/2042 [00:02<00:02, 400.49it/s]Padding data:  52%|#####2    | 1066/2042 [00:02<00:02, 402.44it/s]Padding data:  54%|#####4    | 1107/2042 [00:02<00:02, 403.81it/s]Padding data:  56%|#####6    | 1148/2042 [00:02<00:02, 401.21it/s]Padding data:  58%|#####8    | 1189/2042 [00:03<00:02, 401.76it/s]Padding data:  60%|######    | 1230/2042 [00:03<00:02, 400.96it/s]Padding data:  62%|######2   | 1271/2042 [00:03<00:01, 402.76it/s]Padding data:  64%|######4   | 1312/2042 [00:03<00:01, 397.00it/s]Padding data:  66%|######6   | 1352/2042 [00:03<00:01, 397.04it/s]Padding data:  68%|######8   | 1393/2042 [00:03<00:01, 397.66it/s]Padding data:  70%|#######   | 1433/2042 [00:03<00:01, 397.48it/s]Padding data:  72%|#######2  | 1474/2042 [00:03<00:01, 397.98it/s]Padding data:  74%|#######4  | 1514/2042 [00:03<00:01, 393.11it/s]Padding data:  76%|#######6  | 1554/2042 [00:03<00:01, 391.99it/s]Padding data:  78%|#######8  | 1595/2042 [00:04<00:01, 396.44it/s]Padding data:  80%|########  | 1636/2042 [00:04<00:01, 398.41it/s]Padding data:  82%|########2 | 1676/2042 [00:04<00:00, 398.02it/s]Padding data:  84%|########4 | 1716/2042 [00:04<00:00, 397.75it/s]Padding data:  86%|########5 | 1756/2042 [00:04<00:00, 391.76it/s]Padding data:  88%|########8 | 1797/2042 [00:04<00:00, 395.12it/s]Padding data:  90%|########9 | 1837/2042 [00:04<00:00, 395.71it/s]Padding data:  92%|#########1| 1877/2042 [00:04<00:00, 396.12it/s]Padding data:  94%|#########3| 1917/2042 [00:04<00:00, 391.78it/s]Padding data:  96%|#########5| 1957/2042 [00:05<00:00, 388.80it/s]Padding data:  98%|#########7| 1998/2042 [00:05<00:00, 391.88it/s]Padding data: 100%|#########9| 2039/2042 [00:05<00:00, 396.36it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 391.14it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 500, 60)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 500, 64)      30784       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 500, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 500, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 500, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 500, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 500, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 500, 64)      3904        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 500, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 500, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 500, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 500, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 500, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 500, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 500, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 500, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 500, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 500, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 500, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 500, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 500, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 500, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 500, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 500, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 500, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 500, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 500, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 500, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 500, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 500, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 500, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 500, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 500, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 500, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 500, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 4)            516         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 541,060
Trainable params: 538,500
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4386/4386 - 1238s - loss: 0.9328
Epoch 2/100
4386/4386 - 1348s - loss: 0.8643
Epoch 3/100
4386/4386 - 1332s - loss: 0.8187
Epoch 4/100
4386/4386 - 1350s - loss: 0.7777
Epoch 5/100
4386/4386 - 1335s - loss: 0.7521
Epoch 6/100
4386/4386 - 1348s - loss: 0.7321
Epoch 7/100
4386/4386 - 1358s - loss: 0.7008
Epoch 8/100
4386/4386 - 1349s - loss: 0.6805
Epoch 9/100
4386/4386 - 1348s - loss: 0.6515
Epoch 10/100
4386/4386 - 1357s - loss: 0.6273
Epoch 11/100
4386/4386 - 1354s - loss: 0.6061
Epoch 12/100
4386/4386 - 1358s - loss: 0.5811
Epoch 13/100
