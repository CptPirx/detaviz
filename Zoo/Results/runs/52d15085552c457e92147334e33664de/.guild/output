INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   1%|          | 17/2042 [00:00<00:12, 166.51it/s]Padding data:   2%|▏         | 34/2042 [00:00<00:13, 150.61it/s]Padding data:   2%|▏         | 50/2042 [00:00<00:13, 147.10it/s]Padding data:   3%|▎         | 65/2042 [00:00<00:13, 143.57it/s]Padding data:   4%|▍         | 80/2042 [00:00<00:13, 144.45it/s]Padding data:   5%|▍         | 96/2042 [00:00<00:13, 148.70it/s]Padding data:   5%|▌         | 112/2042 [00:00<00:12, 150.57it/s]Padding data:   6%|▋         | 128/2042 [00:00<00:13, 146.91it/s]Padding data:   7%|▋         | 144/2042 [00:00<00:12, 149.04it/s]Padding data:   8%|▊         | 159/2042 [00:01<00:12, 146.20it/s]Padding data:   9%|▊         | 176/2042 [00:01<00:12, 151.09it/s]Padding data:   9%|▉         | 192/2042 [00:01<00:12, 147.86it/s]Padding data:  10%|█         | 209/2042 [00:01<00:12, 152.22it/s]Padding data:  11%|█         | 225/2042 [00:01<00:11, 151.99it/s]Padding data:  12%|█▏        | 243/2042 [00:01<00:11, 159.28it/s]Padding data:  13%|█▎        | 260/2042 [00:01<00:11, 161.46it/s]Padding data:  14%|█▎        | 279/2042 [00:01<00:10, 167.76it/s]Padding data:  15%|█▍        | 299/2042 [00:01<00:09, 175.03it/s]Padding data:  16%|█▌        | 317/2042 [00:02<00:09, 176.41it/s]Padding data:  16%|█▋        | 336/2042 [00:02<00:09, 177.52it/s]Padding data:  17%|█▋        | 354/2042 [00:02<00:09, 178.01it/s]Padding data:  18%|█▊        | 372/2042 [00:02<00:09, 177.22it/s]Padding data:  19%|█▉        | 390/2042 [00:02<00:09, 175.65it/s]Padding data:  20%|██        | 409/2042 [00:02<00:09, 177.32it/s]Padding data:  21%|██        | 427/2042 [00:02<00:09, 174.21it/s]Padding data:  22%|██▏       | 445/2042 [00:02<00:09, 172.88it/s]Padding data:  23%|██▎       | 463/2042 [00:02<00:09, 168.04it/s]Padding data:  24%|██▎       | 482/2042 [00:02<00:09, 173.25it/s]Padding data:  24%|██▍       | 500/2042 [00:03<00:08, 174.46it/s]Padding data:  25%|██▌       | 518/2042 [00:03<00:08, 173.49it/s]Padding data:  26%|██▋       | 537/2042 [00:03<00:08, 176.23it/s]Padding data:  27%|██▋       | 555/2042 [00:03<00:08, 176.81it/s]Padding data:  28%|██▊       | 573/2042 [00:03<00:08, 175.85it/s]Padding data:  29%|██▉       | 591/2042 [00:03<00:08, 175.37it/s]Padding data:  30%|██▉       | 609/2042 [00:03<00:08, 169.34it/s]Padding data:  31%|███       | 626/2042 [00:03<00:08, 167.99it/s]Padding data:  31%|███▏      | 643/2042 [00:03<00:08, 165.90it/s]Padding data:  32%|███▏      | 661/2042 [00:04<00:08, 167.36it/s]Padding data:  33%|███▎      | 678/2042 [00:04<00:08, 167.99it/s]Padding data:  34%|███▍      | 695/2042 [00:04<00:08, 166.57it/s]Padding data:  35%|███▍      | 712/2042 [00:04<00:08, 165.72it/s]Padding data:  36%|███▌      | 729/2042 [00:04<00:08, 163.21it/s]Padding data:  37%|███▋      | 748/2042 [00:04<00:07, 168.47it/s]Padding data:  38%|███▊      | 766/2042 [00:04<00:07, 171.32it/s]Padding data:  38%|███▊      | 784/2042 [00:04<00:07, 173.16it/s]Padding data:  39%|███▉      | 802/2042 [00:04<00:07, 174.33it/s]Padding data:  40%|████      | 821/2042 [00:04<00:06, 177.43it/s]Padding data:  41%|████      | 840/2042 [00:05<00:06, 178.97it/s]Padding data:  42%|████▏     | 858/2042 [00:05<00:06, 179.22it/s]Padding data:  43%|████▎     | 876/2042 [00:05<00:06, 178.75it/s]Padding data:  44%|████▍     | 894/2042 [00:05<00:06, 178.73it/s]Padding data:  45%|████▍     | 913/2042 [00:05<00:06, 180.10it/s]Padding data:  46%|████▌     | 932/2042 [00:05<00:06, 181.72it/s]Padding data:  47%|████▋     | 951/2042 [00:05<00:05, 183.32it/s]Padding data:  48%|████▊     | 970/2042 [00:05<00:05, 181.86it/s]Padding data:  48%|████▊     | 989/2042 [00:05<00:05, 182.55it/s]Padding data:  49%|████▉     | 1008/2042 [00:05<00:05, 178.38it/s]Padding data:  50%|█████     | 1026/2042 [00:06<00:05, 178.70it/s]Padding data:  51%|█████     | 1044/2042 [00:06<00:05, 175.84it/s]Padding data:  52%|█████▏    | 1062/2042 [00:06<00:05, 170.15it/s]Padding data:  53%|█████▎    | 1080/2042 [00:06<00:05, 167.62it/s]Padding data:  54%|█████▍    | 1098/2042 [00:06<00:05, 170.02it/s]Padding data:  55%|█████▍    | 1116/2042 [00:06<00:05, 172.26it/s]Padding data:  56%|█████▌    | 1135/2042 [00:06<00:05, 175.32it/s]Padding data:  57%|█████▋    | 1154/2042 [00:06<00:04, 177.69it/s]Padding data:  57%|█████▋    | 1173/2042 [00:06<00:04, 179.21it/s]Padding data:  58%|█████▊    | 1191/2042 [00:07<00:04, 177.54it/s]Padding data:  59%|█████▉    | 1210/2042 [00:07<00:04, 179.14it/s]Padding data:  60%|██████    | 1229/2042 [00:07<00:04, 179.76it/s]Padding data:  61%|██████    | 1248/2042 [00:07<00:04, 179.80it/s]Padding data:  62%|██████▏   | 1267/2042 [00:07<00:04, 181.17it/s]Padding data:  63%|██████▎   | 1286/2042 [00:07<00:04, 178.90it/s]Padding data:  64%|██████▍   | 1304/2042 [00:07<00:04, 178.35it/s]Padding data:  65%|██████▍   | 1323/2042 [00:07<00:03, 180.41it/s]Padding data:  66%|██████▌   | 1342/2042 [00:07<00:03, 180.77it/s]Padding data:  67%|██████▋   | 1362/2042 [00:07<00:03, 183.98it/s]Padding data:  68%|██████▊   | 1381/2042 [00:08<00:03, 179.33it/s]Padding data:  69%|██████▊   | 1399/2042 [00:08<00:03, 178.13it/s]Padding data:  69%|██████▉   | 1417/2042 [00:08<00:03, 174.64it/s]Padding data:  70%|███████   | 1435/2042 [00:08<00:03, 169.43it/s]Padding data:  71%|███████   | 1454/2042 [00:08<00:03, 175.11it/s]Padding data:  72%|███████▏  | 1472/2042 [00:08<00:03, 174.51it/s]Padding data:  73%|███████▎  | 1490/2042 [00:08<00:03, 173.99it/s]Padding data:  74%|███████▍  | 1509/2042 [00:08<00:03, 175.64it/s]Padding data:  75%|███████▍  | 1527/2042 [00:08<00:03, 170.34it/s]Padding data:  76%|███████▌  | 1547/2042 [00:09<00:02, 176.32it/s]Padding data:  77%|███████▋  | 1566/2042 [00:09<00:02, 179.65it/s]Padding data:  78%|███████▊  | 1585/2042 [00:09<00:02, 179.54it/s]Padding data:  79%|███████▊  | 1603/2042 [00:09<00:02, 178.51it/s]Padding data:  79%|███████▉  | 1621/2042 [00:09<00:02, 176.24it/s]Padding data:  80%|████████  | 1640/2042 [00:09<00:02, 179.71it/s]Padding data:  81%|████████▏ | 1660/2042 [00:09<00:02, 184.14it/s]Padding data:  82%|████████▏ | 1679/2042 [00:09<00:01, 182.40it/s]Padding data:  83%|████████▎ | 1698/2042 [00:09<00:01, 183.43it/s]Padding data:  84%|████████▍ | 1717/2042 [00:09<00:01, 182.46it/s]Padding data:  85%|████████▌ | 1736/2042 [00:10<00:01, 176.50it/s]Padding data:  86%|████████▌ | 1755/2042 [00:10<00:01, 178.20it/s]Padding data:  87%|████████▋ | 1773/2042 [00:10<00:01, 175.73it/s]Padding data:  88%|████████▊ | 1791/2042 [00:10<00:01, 172.18it/s]Padding data:  89%|████████▊ | 1809/2042 [00:10<00:01, 171.08it/s]Padding data:  89%|████████▉ | 1827/2042 [00:10<00:01, 170.07it/s]Padding data:  90%|█████████ | 1845/2042 [00:10<00:01, 167.25it/s]Padding data:  91%|█████████▏| 1864/2042 [00:10<00:01, 172.96it/s]Padding data:  92%|█████████▏| 1882/2042 [00:10<00:00, 174.63it/s]Padding data:  93%|█████████▎| 1901/2042 [00:11<00:00, 177.79it/s]Padding data:  94%|█████████▍| 1921/2042 [00:11<00:00, 182.29it/s]Padding data:  95%|█████████▌| 1940/2042 [00:11<00:00, 181.20it/s]Padding data:  96%|█████████▌| 1959/2042 [00:11<00:00, 181.84it/s]Padding data:  97%|█████████▋| 1978/2042 [00:11<00:00, 179.02it/s]Padding data:  98%|█████████▊| 1996/2042 [00:11<00:00, 178.01it/s]Padding data:  99%|█████████▊| 2014/2042 [00:11<00:00, 175.15it/s]Padding data: 100%|█████████▉| 2033/2042 [00:11<00:00, 176.66it/s]Padding data: 100%|██████████| 2042/2042 [00:11<00:00, 172.61it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 100, 125)]   0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 100, 64)      64064       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 100, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 100, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 100, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 100, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 100, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 100, 64)      8064        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 100, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 100, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 100, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 100, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 100, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 100, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 100, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 100, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 100, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 100, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 100, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 100, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 100, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 100, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 100, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 100, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 100, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 100, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 100, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 100, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 100, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 100, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 100, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 100, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 100, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 100, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 2)            258         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 578,242
Trainable params: 575,682
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4383/4383 - 302s - loss: 0.6324
Epoch 2/100
4383/4383 - 296s - loss: 0.6218
Epoch 3/100
4383/4383 - 298s - loss: 0.6136
Epoch 4/100
4383/4383 - 297s - loss: 0.6102
Epoch 5/100
4383/4383 - 298s - loss: 0.6063
Epoch 6/100
4383/4383 - 298s - loss: 0.6032
Epoch 7/100
4383/4383 - 298s - loss: 0.5997
Epoch 8/100
4383/4383 - 298s - loss: 0.5952
Epoch 9/100
4383/4383 - 298s - loss: 0.5916
Epoch 10/100
4383/4383 - 299s - loss: 0.5836
Epoch 11/100
4383/4383 - 299s - loss: 0.5795
Epoch 12/100
4383/4383 - 299s - loss: 0.5729
Epoch 13/100
4383/4383 - 298s - loss: 0.5676
Epoch 14/100
4383/4383 - 298s - loss: 0.5603
Epoch 15/100
4383/4383 - 298s - loss: 0.5558
Epoch 16/100
4383/4383 - 299s - loss: 0.5499
Epoch 17/100
4383/4383 - 298s - loss: 0.5472
Epoch 18/100
4383/4383 - 299s - loss: 0.5388
Epoch 19/100
4383/4383 - 299s - loss: 0.5338
Epoch 20/100
4383/4383 - 299s - loss: 0.5312
Epoch 21/100
4383/4383 - 299s - loss: 0.5235
Epoch 22/100
4383/4383 - 298s - loss: 0.5165
Epoch 23/100
4383/4383 - 299s - loss: 0.5118
Epoch 24/100
4383/4383 - 299s - loss: 0.5035
Epoch 25/100
4383/4383 - 299s - loss: 0.5008
Epoch 26/100
4383/4383 - 299s - loss: 0.4900
Epoch 27/100
4383/4383 - 299s - loss: 0.4832
Epoch 28/100
4383/4383 - 297s - loss: 0.4761
Epoch 29/100
4383/4383 - 298s - loss: 0.4668
Epoch 30/100
4383/4383 - 299s - loss: 0.4647
Epoch 31/100
4383/4383 - 299s - loss: 0.4523
Epoch 32/100
4383/4383 - 298s - loss: 0.4491
Epoch 33/100
4383/4383 - 298s - loss: 0.4412
Epoch 34/100
4383/4383 - 298s - loss: 0.4322
Epoch 35/100
4383/4383 - 297s - loss: 0.4237
Epoch 36/100
4383/4383 - 296s - loss: 0.4137
Epoch 37/100
4383/4383 - 297s - loss: 0.4093
Epoch 38/100
4383/4383 - 297s - loss: 0.4030
Epoch 39/100
4383/4383 - 298s - loss: 0.3916
Epoch 40/100
4383/4383 - 297s - loss: 0.3887
Epoch 41/100
4383/4383 - 297s - loss: 0.3746
Epoch 42/100
4383/4383 - 298s - loss: 0.3734
Epoch 43/100
4383/4383 - 297s - loss: 0.3674
Epoch 44/100
4383/4383 - 297s - loss: 0.3603
Epoch 45/100
4383/4383 - 297s - loss: 0.3541
Epoch 46/100
4383/4383 - 298s - loss: 0.3424
Epoch 47/100
4383/4383 - 297s - loss: 0.3401
Epoch 48/100
4383/4383 - 298s - loss: 0.3327
Epoch 49/100
4383/4383 - 298s - loss: 0.3272
Epoch 50/100
4383/4383 - 297s - loss: 0.3247
Epoch 51/100
4383/4383 - 297s - loss: 0.3119
Epoch 52/100
4383/4383 - 297s - loss: 0.3117
Epoch 53/100
4383/4383 - 297s - loss: 0.3068
Epoch 54/100
4383/4383 - 297s - loss: 0.3036
Epoch 55/100
4383/4383 - 298s - loss: 0.2936
Epoch 56/100
4383/4383 - 298s - loss: 0.3020
Epoch 57/100
4383/4383 - 298s - loss: 0.2900
Epoch 58/100
4383/4383 - 298s - loss: 0.2855
Epoch 59/100
4383/4383 - 297s - loss: 0.2896
Epoch 60/100
4383/4383 - 297s - loss: 0.2783
Epoch 61/100
4383/4383 - 297s - loss: 0.2889
Epoch 62/100
4383/4383 - 297s - loss: 0.2766
Epoch 63/100
4383/4383 - 298s - loss: 0.2742
Epoch 64/100
4383/4383 - 297s - loss: 0.2753
Epoch 65/100
4383/4383 - 298s - loss: 0.2701
Epoch 66/100
4383/4383 - 297s - loss: 0.2673
Epoch 67/100
4383/4383 - 298s - loss: 0.2572
Epoch 68/100
4383/4383 - 298s - loss: 0.2681
Epoch 69/100
4383/4383 - 298s - loss: 0.2546
Epoch 70/100
4383/4383 - 298s - loss: 0.2627
Epoch 71/100
4383/4383 - 298s - loss: 0.2541
Epoch 72/100
4383/4383 - 299s - loss: 0.2494
Epoch 73/100
4383/4383 - 296s - loss: 0.2581
Epoch 74/100
4383/4383 - 297s - loss: 0.2524
Epoch 75/100
4383/4383 - 296s - loss: 0.2543
Epoch 76/100
4383/4383 - 296s - loss: 0.2462
Epoch 77/100
4383/4383 - 296s - loss: 0.2432
Epoch 78/100
4383/4383 - 298s - loss: 0.2396
Epoch 79/100
4383/4383 - 298s - loss: 0.2480
Epoch 80/100
4383/4383 - 297s - loss: 0.2432
Epoch 81/100
4383/4383 - 297s - loss: 0.2379
Epoch 82/100
4383/4383 - 297s - loss: 0.2483
Epoch 83/100
4383/4383 - 298s - loss: 0.2412
Epoch 84/100
4383/4383 - 298s - loss: 0.2303
Epoch 85/100
4383/4383 - 298s - loss: 0.2374
Epoch 86/100
4383/4383 - 299s - loss: 0.2347
Epoch 87/100
4383/4383 - 299s - loss: 0.2346
Epoch 88/100
4383/4383 - 298s - loss: 0.2253
Epoch 89/100
4383/4383 - 298s - loss: 0.2311
Epoch 90/100
4383/4383 - 298s - loss: 0.2315
Epoch 91/100
4383/4383 - 298s - loss: 0.2279
Epoch 92/100
4383/4383 - 298s - loss: 0.2270

Epoch 00092: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 93/100
4383/4383 - 298s - loss: 0.1958
Epoch 94/100
4383/4383 - 298s - loss: 0.1907
Epoch 95/100
4383/4383 - 297s - loss: 0.1885
Epoch 96/100
4383/4383 - 297s - loss: 0.1847
Epoch 97/100
4383/4383 - 297s - loss: 0.1854
Epoch 98/100
4383/4383 - 298s - loss: 0.1820
Epoch 99/100
4383/4383 - 297s - loss: 0.1798
Epoch 100/100
4383/4383 - 298s - loss: 0.1780
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.47953641912827516
Test f1_avg: 0.4576002625780642
