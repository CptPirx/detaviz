INFO: [queue] 2021-03-09 18:55:19 Waiting for staged runs
INFO: [queue] 2021-03-09 18:55:49 Starting staged run 7f1e4e71beba4c1ca5875cafaddbcc1b
INFO: [guild] Running trial c9fe1f5b794944e5bc7875fc9a9d7682: TABL:train (attention_constraint=None, attention_regularizer=None, bl_dimensions={0: [120, 5], 1: [60, 2]}, dev=no, dimensionality=60, horizon=1, learning_rate=0.001, n_bl_layers=2, n_tabl_layers=1, optimizer=adam, projection_constraint=None, projection_regularizer=None, tabl_dimensions={0: [4, 1]}, window=100)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 34/2042 [00:00<00:06, 330.98it/s]Padding data:   3%|3         | 68/2042 [00:00<00:05, 334.80it/s]Padding data:   5%|5         | 105/2042 [00:00<00:05, 348.02it/s]Padding data:   7%|7         | 144/2042 [00:00<00:05, 361.98it/s]Padding data:   9%|9         | 184/2042 [00:00<00:04, 373.25it/s]Padding data:  11%|#1        | 226/2042 [00:00<00:04, 388.01it/s]Padding data:  13%|#3        | 270/2042 [00:00<00:04, 403.21it/s]Padding data:  15%|#5        | 313/2042 [00:00<00:04, 408.16it/s]Padding data:  17%|#7        | 355/2042 [00:00<00:04, 409.61it/s]Padding data:  19%|#9        | 397/2042 [00:01<00:04, 410.61it/s]Padding data:  21%|##1       | 439/2042 [00:01<00:03, 401.70it/s]Padding data:  24%|##3       | 480/2042 [00:01<00:03, 392.85it/s]Padding data:  25%|##5       | 520/2042 [00:01<00:03, 391.86it/s]Padding data:  28%|##7       | 565/2042 [00:01<00:03, 405.57it/s]Padding data:  30%|##9       | 607/2042 [00:01<00:03, 408.92it/s]Padding data:  32%|###1      | 649/2042 [00:01<00:03, 410.11it/s]Padding data:  34%|###3      | 692/2042 [00:01<00:03, 412.69it/s]Padding data:  36%|###5      | 735/2042 [00:01<00:03, 414.43it/s]Padding data:  38%|###8      | 778/2042 [00:01<00:03, 416.88it/s]Padding data:  40%|####      | 820/2042 [00:02<00:02, 415.70it/s]Padding data:  42%|####2     | 862/2042 [00:02<00:02, 406.51it/s]Padding data:  44%|####4     | 903/2042 [00:02<00:02, 400.84it/s]Padding data:  46%|####6     | 944/2042 [00:02<00:02, 398.07it/s]Padding data:  48%|####8     | 987/2042 [00:02<00:02, 406.45it/s]Padding data:  50%|#####     | 1029/2042 [00:02<00:02, 408.35it/s]Padding data:  52%|#####2    | 1071/2042 [00:02<00:02, 408.51it/s]Padding data:  55%|#####4    | 1115/2042 [00:02<00:02, 415.65it/s]Padding data:  57%|#####6    | 1158/2042 [00:02<00:02, 418.08it/s]Padding data:  59%|#####8    | 1200/2042 [00:02<00:02, 417.71it/s]Padding data:  61%|######    | 1242/2042 [00:03<00:01, 410.26it/s]Padding data:  63%|######2   | 1284/2042 [00:03<00:01, 405.14it/s]Padding data:  65%|######4   | 1325/2042 [00:03<00:01, 401.01it/s]Padding data:  67%|######6   | 1367/2042 [00:03<00:01, 404.53it/s]Padding data:  69%|######9   | 1411/2042 [00:03<00:01, 412.78it/s]Padding data:  71%|#######1  | 1454/2042 [00:03<00:01, 415.77it/s]Padding data:  73%|#######3  | 1497/2042 [00:03<00:01, 417.55it/s]Padding data:  75%|#######5  | 1539/2042 [00:03<00:01, 417.36it/s]Padding data:  77%|#######7  | 1582/2042 [00:03<00:01, 418.96it/s]Padding data:  80%|#######9  | 1624/2042 [00:04<00:01, 417.13it/s]Padding data:  82%|########1 | 1666/2042 [00:04<00:00, 413.42it/s]Padding data:  84%|########3 | 1708/2042 [00:04<00:00, 408.27it/s]Padding data:  86%|########5 | 1749/2042 [00:04<00:00, 397.51it/s]Padding data:  88%|########7 | 1789/2042 [00:04<00:00, 388.42it/s]Padding data:  90%|########9 | 1829/2042 [00:04<00:00, 390.92it/s]Padding data:  92%|#########1| 1872/2042 [00:04<00:00, 401.42it/s]Padding data:  94%|#########3| 1914/2042 [00:04<00:00, 404.82it/s]Padding data:  96%|#########5| 1957/2042 [00:04<00:00, 408.95it/s]Padding data:  98%|#########7| 2000/2042 [00:04<00:00, 413.04it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 410.57it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 404.34it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            12900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 4)                 251       
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0         
=================================================================
Total params: 20,481
Trainable params: 20,481
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4388/4388 - 48s - loss: 1.1088
Epoch 2/100
4388/4388 - 46s - loss: 0.9299
Epoch 3/100
4388/4388 - 46s - loss: 0.8946
Epoch 4/100
4388/4388 - 46s - loss: 0.8650
Epoch 5/100
4388/4388 - 46s - loss: 0.8453
Epoch 6/100
4388/4388 - 46s - loss: 0.8281
Epoch 7/100
4388/4388 - 46s - loss: 0.8169
Epoch 8/100
4388/4388 - 46s - loss: 0.8037
Epoch 9/100
4388/4388 - 45s - loss: 0.7956
Epoch 10/100
4388/4388 - 46s - loss: 0.7862
Epoch 11/100
4388/4388 - 45s - loss: 0.7788
Epoch 12/100
4388/4388 - 46s - loss: 0.7733
Epoch 13/100
4388/4388 - 45s - loss: 0.7657
Epoch 14/100
4388/4388 - 46s - loss: 0.7616
Epoch 15/100
4388/4388 - 46s - loss: 0.7627
Epoch 16/100
4388/4388 - 46s - loss: 0.7559
Epoch 17/100
4388/4388 - 46s - loss: 0.7520
Epoch 18/100
4388/4388 - 46s - loss: 0.7480
Epoch 19/100
4388/4388 - 46s - loss: 0.7457
Epoch 20/100
4388/4388 - 46s - loss: 0.7420
Epoch 21/100
4388/4388 - 46s - loss: 0.7369
Epoch 22/100
4388/4388 - 46s - loss: 0.7396
Epoch 23/100
4388/4388 - 46s - loss: 0.7338
Epoch 24/100
4388/4388 - 47s - loss: 0.7297
Epoch 25/100
4388/4388 - 46s - loss: 0.7282
Epoch 26/100
4388/4388 - 46s - loss: 0.7246
Epoch 27/100
4388/4388 - 46s - loss: 0.7230
Epoch 28/100
4388/4388 - 46s - loss: 0.7209
Epoch 29/100
4388/4388 - 46s - loss: 0.7195
Epoch 30/100
4388/4388 - 46s - loss: 0.7179
Epoch 31/100
4388/4388 - 48s - loss: 0.7139
Epoch 32/100
4388/4388 - 51s - loss: 0.7114
Epoch 33/100
4388/4388 - 96s - loss: 0.7101
Epoch 34/100
4388/4388 - 49s - loss: 0.7117
Epoch 35/100
4388/4388 - 48s - loss: 0.7072
Epoch 36/100
4388/4388 - 49s - loss: 0.7082
Epoch 37/100
4388/4388 - 50s - loss: 0.7060
Epoch 38/100
4388/4388 - 50s - loss: 0.7029
Epoch 39/100
4388/4388 - 50s - loss: 0.7001
Epoch 40/100
4388/4388 - 49s - loss: 0.6970
Epoch 41/100
4388/4388 - 48s - loss: 0.6984
Epoch 42/100
4388/4388 - 48s - loss: 0.6976
Epoch 43/100
4388/4388 - 47s - loss: 0.6977
Epoch 44/100
4388/4388 - 48s - loss: 0.6924
Epoch 45/100
4388/4388 - 48s - loss: 0.6933
Epoch 46/100
4388/4388 - 49s - loss: 0.6950
Epoch 47/100
4388/4388 - 49s - loss: 0.6921
Epoch 48/100
4388/4388 - 48s - loss: 0.6924
Epoch 49/100
4388/4388 - 48s - loss: 0.6880
Epoch 50/100
4388/4388 - 48s - loss: 0.6890
Epoch 51/100
4388/4388 - 48s - loss: 0.6858
Epoch 52/100
4388/4388 - 48s - loss: 0.6842
Epoch 53/100
4388/4388 - 48s - loss: 0.6833
Epoch 54/100
4388/4388 - 48s - loss: 0.6870
Epoch 55/100
4388/4388 - 48s - loss: 0.6812
Epoch 56/100
4388/4388 - 48s - loss: 0.6813
Epoch 57/100
4388/4388 - 48s - loss: 0.6815
Epoch 58/100
4388/4388 - 48s - loss: 0.6797
Epoch 59/100
4388/4388 - 47s - loss: 0.6806
Epoch 60/100
4388/4388 - 47s - loss: 0.6813
Epoch 61/100
4388/4388 - 48s - loss: 0.6773
Epoch 62/100
4388/4388 - 47s - loss: 0.6770
Epoch 63/100
4388/4388 - 48s - loss: 0.6750
Epoch 64/100
4388/4388 - 49s - loss: 0.6747
Epoch 65/100
4388/4388 - 49s - loss: 0.6752
Epoch 66/100
4388/4388 - 47s - loss: 0.6729
Epoch 67/100
4388/4388 - 48s - loss: 0.6725
Epoch 68/100
4388/4388 - 48s - loss: 0.6722
Epoch 69/100
4388/4388 - 48s - loss: 0.6695
Epoch 70/100
4388/4388 - 48s - loss: 0.6740
Epoch 71/100
4388/4388 - 48s - loss: 0.6690
Epoch 72/100
4388/4388 - 48s - loss: 0.6703
Epoch 73/100
4388/4388 - 48s - loss: 0.6649
Epoch 74/100
4388/4388 - 48s - loss: 0.6699
Epoch 75/100
4388/4388 - 48s - loss: 0.6692
Epoch 76/100
4388/4388 - 49s - loss: 0.6645
Epoch 77/100
4388/4388 - 48s - loss: 0.6688
Epoch 78/100
4388/4388 - 48s - loss: 0.6664
Epoch 79/100
4388/4388 - 48s - loss: 0.6613
Epoch 80/100
4388/4388 - 48s - loss: 0.6647
Epoch 81/100
4388/4388 - 48s - loss: 0.6604
Epoch 82/100
4388/4388 - 48s - loss: 0.6597
Epoch 83/100
4388/4388 - 48s - loss: 0.6650
Epoch 84/100
4388/4388 - 48s - loss: 0.6632
Epoch 85/100
4388/4388 - 48s - loss: 0.6642
Epoch 86/100
4388/4388 - 48s - loss: 0.6630

Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 87/100
4388/4388 - 48s - loss: 0.6327
Epoch 88/100
4388/4388 - 48s - loss: 0.6267
Epoch 89/100
4388/4388 - 48s - loss: 0.6235
Epoch 90/100
4388/4388 - 49s - loss: 0.6275
Epoch 91/100
4388/4388 - 48s - loss: 0.6260
Epoch 92/100
4388/4388 - 48s - loss: 0.6253
Epoch 93/100
4388/4388 - 49s - loss: 0.6231
Epoch 94/100
4388/4388 - 49s - loss: 0.6233
Epoch 95/100
4388/4388 - 48s - loss: 0.6257
Epoch 96/100
4388/4388 - 47s - loss: 0.6224
Epoch 97/100
4388/4388 - 46s - loss: 0.6241
Epoch 98/100
4388/4388 - 46s - loss: 0.6226
Epoch 99/100
4388/4388 - 46s - loss: 0.6202
Epoch 100/100
4388/4388 - 46s - loss: 0.6212
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.5473230658948907
Test f1_avg: 0.4759626786898222
INFO: [guild] Running trial a0d32a056f1f40a289a775a6864a0671: TABL:train (attention_constraint=None, attention_regularizer=None, bl_dimensions={0: [120, 5], 1: [60, 2]}, dev=no, dimensionality=60, horizon=1, learning_rate=0.001, n_bl_layers=2, n_tabl_layers=1, optimizer=adam, projection_constraint=None, projection_regularizer=None, tabl_dimensions={0: [4, 1]}, window=200)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 36/2042 [00:00<00:05, 350.45it/s]Padding data:   4%|3         | 74/2042 [00:00<00:05, 363.98it/s]Padding data:   6%|5         | 113/2042 [00:00<00:05, 372.82it/s]Padding data:   7%|7         | 152/2042 [00:00<00:04, 378.44it/s]Padding data:   9%|9         | 192/2042 [00:00<00:04, 383.09it/s]Padding data:  11%|#1        | 233/2042 [00:00<00:04, 389.87it/s]Padding data:  13%|#3        | 275/2042 [00:00<00:04, 398.67it/s]Padding data:  15%|#5        | 315/2042 [00:00<00:04, 398.17it/s]Padding data:  17%|#7        | 356/2042 [00:00<00:04, 400.93it/s]Padding data:  19%|#9        | 397/2042 [00:01<00:04, 402.81it/s]Padding data:  21%|##1       | 438/2042 [00:01<00:03, 402.88it/s]Padding data:  23%|##3       | 479/2042 [00:01<00:03, 404.14it/s]Padding data:  25%|##5       | 520/2042 [00:01<00:03, 401.40it/s]Padding data:  27%|##7       | 561/2042 [00:01<00:03, 397.20it/s]Padding data:  29%|##9       | 601/2042 [00:01<00:03, 395.58it/s]Padding data:  31%|###1      | 641/2042 [00:01<00:03, 395.87it/s]Padding data:  33%|###3      | 682/2042 [00:01<00:03, 398.02it/s]Padding data:  35%|###5      | 722/2042 [00:01<00:03, 396.57it/s]Padding data:  37%|###7      | 762/2042 [00:01<00:03, 395.56it/s]Padding data:  39%|###9      | 802/2042 [00:02<00:03, 394.86it/s]Padding data:  41%|####1     | 843/2042 [00:02<00:03, 398.48it/s]Padding data:  43%|####3     | 884/2042 [00:02<00:02, 399.85it/s]Padding data:  45%|####5     | 925/2042 [00:02<00:02, 401.39it/s]Padding data:  47%|####7     | 967/2042 [00:02<00:02, 404.84it/s]Padding data:  49%|####9     | 1008/2042 [00:02<00:02, 403.11it/s]Padding data:  51%|#####1    | 1049/2042 [00:02<00:02, 401.91it/s]Padding data:  53%|#####3    | 1090/2042 [00:02<00:02, 403.43it/s]Padding data:  55%|#####5    | 1131/2042 [00:02<00:02, 403.31it/s]Padding data:  57%|#####7    | 1172/2042 [00:02<00:02, 401.88it/s]Padding data:  59%|#####9    | 1213/2042 [00:03<00:02, 403.41it/s]Padding data:  61%|######1   | 1254/2042 [00:03<00:01, 401.52it/s]Padding data:  63%|######3   | 1295/2042 [00:03<00:01, 403.15it/s]Padding data:  65%|######5   | 1336/2042 [00:03<00:01, 402.43it/s]Padding data:  67%|######7   | 1377/2042 [00:03<00:01, 396.78it/s]Padding data:  69%|######9   | 1417/2042 [00:03<00:01, 395.72it/s]Padding data:  71%|#######1  | 1458/2042 [00:03<00:01, 397.90it/s]Padding data:  73%|#######3  | 1498/2042 [00:03<00:01, 397.66it/s]Padding data:  75%|#######5  | 1539/2042 [00:03<00:01, 400.45it/s]Padding data:  77%|#######7  | 1580/2042 [00:03<00:01, 401.22it/s]Padding data:  79%|#######9  | 1621/2042 [00:04<00:01, 402.94it/s]Padding data:  81%|########1 | 1663/2042 [00:04<00:00, 404.73it/s]Padding data:  83%|########3 | 1704/2042 [00:04<00:00, 402.67it/s]Padding data:  85%|########5 | 1745/2042 [00:04<00:00, 403.96it/s]Padding data:  87%|########7 | 1786/2042 [00:04<00:00, 402.50it/s]Padding data:  89%|########9 | 1827/2042 [00:04<00:00, 401.48it/s]Padding data:  91%|#########1| 1868/2042 [00:04<00:00, 401.95it/s]Padding data:  93%|#########3| 1909/2042 [00:04<00:00, 402.27it/s]Padding data:  95%|#########5| 1950/2042 [00:04<00:00, 402.50it/s]Padding data:  98%|#########7| 1991/2042 [00:04<00:00, 402.66it/s]Padding data: 100%|#########9| 2032/2042 [00:05<00:00, 402.77it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 398.88it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 200, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            24900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 4)                 251       
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0         
=================================================================
Total params: 32,481
Trainable params: 32,481
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4388/4388 - 79s - loss: 1.0107
Epoch 2/100
4388/4388 - 78s - loss: 0.8819
Epoch 3/100
4388/4388 - 75s - loss: 0.8524
Epoch 4/100
4388/4388 - 75s - loss: 0.8297
Epoch 5/100
4388/4388 - 76s - loss: 0.8102
Epoch 6/100
4388/4388 - 75s - loss: 0.7910
Epoch 7/100
4388/4388 - 75s - loss: 0.7768
Epoch 8/100
4388/4388 - 76s - loss: 0.7685
Epoch 9/100
4388/4388 - 75s - loss: 0.7583
Epoch 10/100
4388/4388 - 75s - loss: 0.7516
Epoch 11/100
4388/4388 - 75s - loss: 0.7419
Epoch 12/100
4388/4388 - 75s - loss: 0.7376
Epoch 13/100
4388/4388 - 75s - loss: 0.7351
Epoch 14/100
4388/4388 - 75s - loss: 0.7300
Epoch 15/100
4388/4388 - 75s - loss: 0.7249
Epoch 16/100
4388/4388 - 75s - loss: 0.7224
Epoch 17/100
4388/4388 - 75s - loss: 0.7177
Epoch 18/100
4388/4388 - 75s - loss: 0.7167
Epoch 19/100
4388/4388 - 75s - loss: 0.7119
Epoch 20/100
4388/4388 - 75s - loss: 0.7120
Epoch 21/100
4388/4388 - 75s - loss: 0.7116
Epoch 22/100
4388/4388 - 75s - loss: 0.7071
Epoch 23/100
4388/4388 - 75s - loss: 0.7100
Epoch 24/100
4388/4388 - 75s - loss: 0.7089
Epoch 25/100
4388/4388 - 75s - loss: 0.7035
Epoch 26/100
4388/4388 - 75s - loss: 0.7040
Epoch 27/100
4388/4388 - 75s - loss: 0.7028
Epoch 28/100
4388/4388 - 75s - loss: 0.6993
Epoch 29/100
4388/4388 - 75s - loss: 0.7009
Epoch 30/100
4388/4388 - 75s - loss: 0.7014
Epoch 31/100
4388/4388 - 75s - loss: 0.6963
Epoch 32/100
4388/4388 - 75s - loss: 0.6929
Epoch 33/100
4388/4388 - 75s - loss: 0.6949
Epoch 34/100
4388/4388 - 75s - loss: 0.6930
Epoch 35/100
4388/4388 - 74s - loss: 0.6932
Epoch 36/100
4388/4388 - 75s - loss: 0.6911
Epoch 37/100
4388/4388 - 75s - loss: 0.6899
Epoch 38/100
4388/4388 - 75s - loss: 0.6903
Epoch 39/100
4388/4388 - 75s - loss: 0.6849
Epoch 40/100
4388/4388 - 76s - loss: 0.6881
Epoch 41/100
4388/4388 - 75s - loss: 0.6857
Epoch 42/100
4388/4388 - 75s - loss: 0.6833
Epoch 43/100
4388/4388 - 75s - loss: 0.6847
Epoch 44/100
4388/4388 - 75s - loss: 0.6855
Epoch 45/100
4388/4388 - 75s - loss: 0.6847
Epoch 46/100
4388/4388 - 75s - loss: 0.6810
Epoch 47/100
4388/4388 - 75s - loss: 0.6826
Epoch 48/100
4388/4388 - 75s - loss: 0.6823
Epoch 49/100
4388/4388 - 75s - loss: 0.6819
Epoch 50/100
4388/4388 - 75s - loss: 0.6800
Epoch 51/100
4388/4388 - 75s - loss: 0.6770
Epoch 52/100
4388/4388 - 75s - loss: 0.6772
Epoch 53/100
4388/4388 - 75s - loss: 0.6801
Epoch 54/100
4388/4388 - 75s - loss: 0.6763
Epoch 55/100
4388/4388 - 75s - loss: 0.6753
Epoch 56/100
4388/4388 - 75s - loss: 0.6727
Epoch 57/100
4388/4388 - 75s - loss: 0.6769
Epoch 58/100
4388/4388 - 75s - loss: 0.6711
Epoch 59/100
4388/4388 - 74s - loss: 0.6708
Epoch 60/100
4388/4388 - 75s - loss: 0.6727
Epoch 61/100
4388/4388 - 75s - loss: 0.6676
Epoch 62/100
4388/4388 - 75s - loss: 0.6675
Epoch 63/100
4388/4388 - 75s - loss: 0.6684
Epoch 64/100
4388/4388 - 75s - loss: 0.6713
Epoch 65/100
4388/4388 - 75s - loss: 0.6675
Epoch 66/100
4388/4388 - 75s - loss: 0.6676

Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 67/100
4388/4388 - 75s - loss: 0.6445
Epoch 68/100
4388/4388 - 75s - loss: 0.6408
Epoch 69/100
4388/4388 - 75s - loss: 0.6389
Epoch 70/100
4388/4388 - 75s - loss: 0.6396
Epoch 71/100
4388/4388 - 75s - loss: 0.6379
Epoch 72/100
4388/4388 - 75s - loss: 0.6376
Epoch 73/100
4388/4388 - 78s - loss: 0.6355
Epoch 74/100
4388/4388 - 78s - loss: 0.6347
Epoch 75/100
4388/4388 - 76s - loss: 0.6367
Epoch 76/100
4388/4388 - 75s - loss: 0.6346
Epoch 77/100
4388/4388 - 75s - loss: 0.6340
Epoch 78/100
4388/4388 - 75s - loss: 0.6350
Epoch 79/100
4388/4388 - 75s - loss: 0.6336
Epoch 80/100
4388/4388 - 75s - loss: 0.6337
Epoch 81/100
4388/4388 - 75s - loss: 0.6300
Epoch 82/100
4388/4388 - 75s - loss: 0.6324
Epoch 83/100
4388/4388 - 75s - loss: 0.6318
Epoch 84/100
4388/4388 - 75s - loss: 0.6309
Epoch 85/100
4388/4388 - 75s - loss: 0.6312

Epoch 00085: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 86/100
4388/4388 - 75s - loss: 0.6172
Epoch 87/100
4388/4388 - 75s - loss: 0.6163
Epoch 88/100
4388/4388 - 75s - loss: 0.6157
Epoch 89/100
4388/4388 - 75s - loss: 0.6154
Epoch 90/100
4388/4388 - 75s - loss: 0.6142
Epoch 91/100
4388/4388 - 75s - loss: 0.6141
Epoch 92/100
4388/4388 - 75s - loss: 0.6145
Epoch 93/100
4388/4388 - 75s - loss: 0.6131
Epoch 94/100
4388/4388 - 75s - loss: 0.6140
Epoch 95/100
4388/4388 - 75s - loss: 0.6127
Epoch 96/100
4388/4388 - 76s - loss: 0.6108
Epoch 97/100
4388/4388 - 75s - loss: 0.6120
Epoch 98/100
4388/4388 - 74s - loss: 0.6126
Epoch 99/100
4388/4388 - 74s - loss: 0.6117
Epoch 100/100
4388/4388 - 75s - loss: 0.6113

Epoch 00100: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.542474453868288
Test f1_avg: 0.48318533087420773
INFO: [guild] Running trial 9dfa59e991ed4ad19e1e31c8fe82cd81: TABL:train (attention_constraint=None, attention_regularizer=None, bl_dimensions={0: [120, 5], 1: [60, 2]}, dev=no, dimensionality=60, horizon=1, learning_rate=0.001, n_bl_layers=2, n_tabl_layers=1, optimizer=adam, projection_constraint=None, projection_regularizer=None, tabl_dimensions={0: [4, 1]}, window=500)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 38/2042 [00:00<00:05, 377.38it/s]Padding data:   4%|3         | 78/2042 [00:00<00:05, 388.98it/s]Padding data:   6%|5         | 117/2042 [00:00<00:04, 388.16it/s]Padding data:   8%|7         | 156/2042 [00:00<00:04, 387.77it/s]Padding data:  10%|9         | 195/2042 [00:00<00:04, 384.77it/s]Padding data:  12%|#1        | 235/2042 [00:00<00:04, 387.64it/s]Padding data:  14%|#3        | 277/2042 [00:00<00:04, 397.18it/s]Padding data:  16%|#5        | 320/2042 [00:00<00:04, 405.32it/s]Padding data:  18%|#7        | 362/2042 [00:00<00:04, 408.93it/s]Padding data:  20%|#9        | 405/2042 [00:01<00:03, 411.95it/s]Padding data:  22%|##1       | 447/2042 [00:01<00:03, 413.47it/s]Padding data:  24%|##3       | 490/2042 [00:01<00:03, 415.08it/s]Padding data:  26%|##6       | 532/2042 [00:01<00:03, 415.61it/s]Padding data:  28%|##8       | 574/2042 [00:01<00:03, 416.02it/s]Padding data:  30%|###       | 616/2042 [00:01<00:03, 415.05it/s]Padding data:  32%|###2      | 658/2042 [00:01<00:03, 413.18it/s]Padding data:  34%|###4      | 700/2042 [00:01<00:03, 414.31it/s]Padding data:  36%|###6      | 742/2042 [00:01<00:03, 415.09it/s]Padding data:  38%|###8      | 784/2042 [00:01<00:03, 414.46it/s]Padding data:  40%|####      | 827/2042 [00:02<00:02, 416.90it/s]Padding data:  43%|####2     | 869/2042 [00:02<00:02, 416.91it/s]Padding data:  45%|####4     | 911/2042 [00:02<00:02, 416.94it/s]Padding data:  47%|####6     | 953/2042 [00:02<00:02, 415.74it/s]Padding data:  49%|####8     | 995/2042 [00:02<00:02, 413.63it/s]Padding data:  51%|#####     | 1038/2042 [00:02<00:02, 416.21it/s]Padding data:  53%|#####2    | 1080/2042 [00:02<00:02, 415.33it/s]Padding data:  55%|#####4    | 1122/2042 [00:02<00:02, 415.82it/s]Padding data:  57%|#####7    | 1165/2042 [00:02<00:02, 417.89it/s]Padding data:  59%|#####9    | 1207/2042 [00:02<00:01, 417.60it/s]Padding data:  61%|######1   | 1250/2042 [00:03<00:01, 417.90it/s]Padding data:  63%|######3   | 1293/2042 [00:03<00:01, 420.58it/s]Padding data:  65%|######5   | 1336/2042 [00:03<00:01, 420.02it/s]Padding data:  68%|######7   | 1379/2042 [00:03<00:01, 420.79it/s]Padding data:  70%|######9   | 1422/2042 [00:03<00:01, 418.90it/s]Padding data:  72%|#######1  | 1464/2042 [00:03<00:01, 418.31it/s]Padding data:  74%|#######3  | 1506/2042 [00:03<00:01, 417.95it/s]Padding data:  76%|#######5  | 1548/2042 [00:03<00:01, 417.62it/s]Padding data:  78%|#######7  | 1591/2042 [00:03<00:01, 419.14it/s]Padding data:  80%|#######9  | 1633/2042 [00:03<00:00, 418.50it/s]Padding data:  82%|########2 | 1676/2042 [00:04<00:00, 418.52it/s]Padding data:  84%|########4 | 1718/2042 [00:04<00:00, 415.60it/s]Padding data:  86%|########6 | 1760/2042 [00:04<00:00, 416.01it/s]Padding data:  88%|########8 | 1802/2042 [00:04<00:00, 416.28it/s]Padding data:  90%|######### | 1844/2042 [00:04<00:00, 416.48it/s]Padding data:  92%|#########2| 1886/2042 [00:04<00:00, 415.39it/s]Padding data:  94%|#########4| 1929/2042 [00:04<00:00, 417.59it/s]Padding data:  97%|#########6| 1971/2042 [00:04<00:00, 413.73it/s]Padding data:  99%|#########8| 2014/2042 [00:04<00:00, 415.20it/s]Padding data: 100%|##########| 2042/2042 [00:04<00:00, 413.38it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 500, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            60900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 4)                 251       
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0         
=================================================================
Total params: 68,481
Trainable params: 68,481
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4386/4386 - 162s - loss: 1.0436
Epoch 2/100
4386/4386 - 160s - loss: 0.8210
Epoch 3/100
4386/4386 - 160s - loss: 0.7921
Epoch 4/100
4386/4386 - 160s - loss: 0.7711
Epoch 5/100
4386/4386 - 159s - loss: 0.7596
Epoch 6/100
4386/4386 - 159s - loss: 0.7484
Epoch 7/100
4386/4386 - 160s - loss: 0.7414
Epoch 8/100
4386/4386 - 159s - loss: 0.7358
Epoch 9/100
4386/4386 - 159s - loss: 0.7211
Epoch 10/100
4386/4386 - 159s - loss: 0.7175
Epoch 11/100
4386/4386 - 160s - loss: 0.7097
Epoch 12/100
4386/4386 - 160s - loss: 0.6967
Epoch 13/100
4386/4386 - 160s - loss: 0.6913
Epoch 14/100
4386/4386 - 158s - loss: 0.6839
Epoch 15/100
4386/4386 - 159s - loss: 0.6802
Epoch 16/100
4386/4386 - 159s - loss: 0.6762
Epoch 17/100
4386/4386 - 159s - loss: 0.6722
Epoch 18/100
4386/4386 - 159s - loss: 0.6654
Epoch 19/100
4386/4386 - 160s - loss: 0.6635
Epoch 20/100
4386/4386 - 160s - loss: 0.6616
Epoch 21/100
4386/4386 - 159s - loss: 0.6589
Epoch 22/100
4386/4386 - 159s - loss: 0.6571
Epoch 23/100
4386/4386 - 160s - loss: 0.6527
Epoch 24/100
4386/4386 - 159s - loss: 0.6514
Epoch 25/100
4386/4386 - 159s - loss: 0.6433
Epoch 26/100
4386/4386 - 160s - loss: 0.6409
Epoch 27/100
4386/4386 - 160s - loss: 0.6438
Epoch 28/100
4386/4386 - 160s - loss: 0.6461
Epoch 29/100
4386/4386 - 159s - loss: 0.6447
Epoch 30/100
4386/4386 - 159s - loss: 0.6403
Epoch 31/100
4386/4386 - 160s - loss: 0.6381
Epoch 32/100
4386/4386 - 160s - loss: 0.6386
Epoch 33/100
4386/4386 - 160s - loss: 0.6340
Epoch 34/100
4386/4386 - 159s - loss: 0.6316
Epoch 35/100
4386/4386 - 159s - loss: 0.6327
Epoch 36/100
4386/4386 - 159s - loss: 0.6346
Epoch 37/100
4386/4386 - 160s - loss: 0.6320
Epoch 38/100
4386/4386 - 162s - loss: 0.6280
Epoch 39/100
4386/4386 - 161s - loss: 0.6282
Epoch 40/100
4386/4386 - 160s - loss: 0.6234
Epoch 41/100
4386/4386 - 160s - loss: 0.6264
Epoch 42/100
4386/4386 - 160s - loss: 0.6282
Epoch 43/100
4386/4386 - 160s - loss: 0.6259
Epoch 44/100
4386/4386 - 160s - loss: 0.6208
Epoch 45/100
4386/4386 - 160s - loss: 0.6176
Epoch 46/100
4386/4386 - 160s - loss: 0.6175
Epoch 47/100
4386/4386 - 160s - loss: 0.6175
Epoch 48/100
4386/4386 - 159s - loss: 0.6202
Epoch 49/100
4386/4386 - 160s - loss: 0.6192
Epoch 50/100
4386/4386 - 160s - loss: 0.6164
Epoch 51/100
4386/4386 - 160s - loss: 0.6151
Epoch 52/100
4386/4386 - 160s - loss: 0.6139
Epoch 53/100
4386/4386 - 159s - loss: 0.6148
Epoch 54/100
4386/4386 - 159s - loss: 0.6106
Epoch 55/100
4386/4386 - 160s - loss: 0.6080
Epoch 56/100
4386/4386 - 159s - loss: 0.6105
Epoch 57/100
4386/4386 - 159s - loss: 0.6100
Epoch 58/100
4386/4386 - 159s - loss: 0.6091
Epoch 59/100
4386/4386 - 160s - loss: 0.6093

Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 60/100
4386/4386 - 160s - loss: 0.5802
Epoch 61/100
4386/4386 - 160s - loss: 0.5758
Epoch 62/100
4386/4386 - 159s - loss: 0.5754
Epoch 63/100
4386/4386 - 160s - loss: 0.5730
Epoch 64/100
4386/4386 - 159s - loss: 0.5757
Epoch 65/100
4386/4386 - 160s - loss: 0.5733
Epoch 66/100
4386/4386 - 159s - loss: 0.5720
Epoch 67/100
4386/4386 - 160s - loss: 0.5700
Epoch 68/100
4386/4386 - 160s - loss: 0.5717
Epoch 69/100
4386/4386 - 159s - loss: 0.5691
Epoch 70/100
4386/4386 - 159s - loss: 0.5713
Epoch 71/100
4386/4386 - 160s - loss: 0.5693
Epoch 72/100
4386/4386 - 160s - loss: 0.5732
Epoch 73/100
4386/4386 - 160s - loss: 0.5687
Epoch 74/100
4386/4386 - 160s - loss: 0.5697
Epoch 75/100
4386/4386 - 160s - loss: 0.5681
Epoch 76/100
4386/4386 - 159s - loss: 0.5682
Epoch 77/100
4386/4386 - 160s - loss: 0.5658
Epoch 78/100
4386/4386 - 159s - loss: 0.5665
Epoch 79/100
4386/4386 - 159s - loss: 0.5658
Epoch 80/100
4386/4386 - 159s - loss: 0.5650
Epoch 81/100
4386/4386 - 159s - loss: 0.5638
Epoch 82/100
4386/4386 - 159s - loss: 0.5657
Epoch 83/100
4386/4386 - 159s - loss: 0.5632
Epoch 84/100
4386/4386 - 159s - loss: 0.5636
Epoch 85/100
4386/4386 - 159s - loss: 0.5605
Epoch 86/100
4386/4386 - 159s - loss: 0.5634
Epoch 87/100
4386/4386 - 159s - loss: 0.5607
Epoch 88/100
4386/4386 - 159s - loss: 0.5634
Epoch 89/100
4386/4386 - 159s - loss: 0.5625

Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 90/100
4386/4386 - 159s - loss: 0.5472
Epoch 91/100
4386/4386 - 159s - loss: 0.5469
Epoch 92/100
4386/4386 - 160s - loss: 0.5435
Epoch 93/100
4386/4386 - 159s - loss: 0.5430
Epoch 94/100
4386/4386 - 159s - loss: 0.5444
Epoch 95/100
4386/4386 - 160s - loss: 0.5438
Epoch 96/100
4386/4386 - 159s - loss: 0.5427
Epoch 97/100
4386/4386 - 159s - loss: 0.5440
Epoch 98/100
4386/4386 - 159s - loss: 0.5426
Epoch 99/100
4386/4386 - 158s - loss: 0.5432
Epoch 100/100
4386/4386 - 159s - loss: 0.5423
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.6065580566569942
Test f1_avg: 0.5162696206808253
INFO: [queue] 2021-03-10 03:00:52 Starting staged run d3ad3228f4714d7980776a353ed9e7b8
INFO: [guild] Running trial e22ea0b5de0141139284412b855707a8: ResNet:train (dev=no, dimensionality=60, horizon=1, learning_rate=0.001, n_feature_maps=64, optimizer=adam, window=100)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 38/2042 [00:00<00:05, 377.24it/s]Padding data:   4%|3         | 76/2042 [00:00<00:05, 377.24it/s]Padding data:   6%|5         | 116/2042 [00:00<00:04, 386.31it/s]Padding data:   8%|7         | 156/2042 [00:00<00:04, 388.17it/s]Padding data:  10%|9         | 197/2042 [00:00<00:04, 393.55it/s]Padding data:  12%|#1        | 238/2042 [00:00<00:04, 396.78it/s]Padding data:  14%|#3        | 280/2042 [00:00<00:04, 403.34it/s]Padding data:  16%|#5        | 323/2042 [00:00<00:04, 408.24it/s]Padding data:  18%|#7        | 364/2042 [00:00<00:04, 408.48it/s]Padding data:  20%|#9        | 407/2042 [00:01<00:03, 411.63it/s]Padding data:  22%|##2       | 450/2042 [00:01<00:03, 415.02it/s]Padding data:  24%|##4       | 492/2042 [00:01<00:03, 414.36it/s]Padding data:  26%|##6       | 534/2042 [00:01<00:03, 415.14it/s]Padding data:  28%|##8       | 577/2042 [00:01<00:03, 417.43it/s]Padding data:  30%|###       | 619/2042 [00:01<00:03, 414.82it/s]Padding data:  32%|###2      | 661/2042 [00:01<00:03, 413.02it/s]Padding data:  34%|###4      | 703/2042 [00:01<00:03, 411.76it/s]Padding data:  36%|###6      | 745/2042 [00:01<00:03, 411.48it/s]Padding data:  39%|###8      | 788/2042 [00:01<00:03, 413.62it/s]Padding data:  41%|####      | 830/2042 [00:02<00:02, 414.60it/s]Padding data:  43%|####2     | 872/2042 [00:02<00:02, 415.30it/s]Padding data:  45%|####4     | 914/2042 [00:02<00:02, 415.79it/s]Padding data:  47%|####6     | 956/2042 [00:02<00:02, 416.14it/s]Padding data:  49%|####8     | 998/2042 [00:02<00:02, 415.15it/s]Padding data:  51%|#####     | 1040/2042 [00:02<00:02, 413.25it/s]Padding data:  53%|#####2    | 1082/2042 [00:02<00:02, 413.13it/s]Padding data:  55%|#####5    | 1124/2042 [00:02<00:02, 413.05it/s]Padding data:  57%|#####7    | 1166/2042 [00:02<00:02, 414.22it/s]Padding data:  59%|#####9    | 1209/2042 [00:02<00:01, 416.76it/s]Padding data:  61%|######1   | 1251/2042 [00:03<00:01, 414.37it/s]Padding data:  63%|######3   | 1293/2042 [00:03<00:01, 412.71it/s]Padding data:  65%|######5   | 1335/2042 [00:03<00:01, 409.16it/s]Padding data:  67%|######7   | 1376/2042 [00:03<00:01, 401.48it/s]Padding data:  69%|######9   | 1418/2042 [00:03<00:01, 403.67it/s]Padding data:  71%|#######1  | 1459/2042 [00:03<00:01, 403.48it/s]Padding data:  74%|#######3  | 1501/2042 [00:03<00:01, 406.27it/s]Padding data:  76%|#######5  | 1543/2042 [00:03<00:01, 408.24it/s]Padding data:  78%|#######7  | 1585/2042 [00:03<00:01, 410.82it/s]Padding data:  80%|#######9  | 1627/2042 [00:03<00:01, 412.65it/s]Padding data:  82%|########1 | 1670/2042 [00:04<00:00, 415.66it/s]Padding data:  84%|########3 | 1712/2042 [00:04<00:00, 416.05it/s]Padding data:  86%|########5 | 1754/2042 [00:04<00:00, 416.32it/s]Padding data:  88%|########7 | 1796/2042 [00:04<00:00, 415.27it/s]Padding data:  90%|######### | 1838/2042 [00:04<00:00, 413.34it/s]Padding data:  92%|#########2| 1880/2042 [00:04<00:00, 415.00it/s]Padding data:  94%|#########4| 1922/2042 [00:04<00:00, 415.58it/s]Padding data:  96%|#########6| 1964/2042 [00:04<00:00, 414.76it/s]Padding data:  98%|#########8| 2006/2042 [00:04<00:00, 415.42it/s]Padding data: 100%|##########| 2042/2042 [00:04<00:00, 410.63it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 100, 60)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 100, 64)      30784       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 100, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 100, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 100, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 100, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 100, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 100, 64)      3904        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 100, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 100, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 100, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 100, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 100, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 100, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 100, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 100, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 100, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 100, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 100, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 100, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 100, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 100, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 100, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 100, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 100, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 100, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 100, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 100, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 100, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 100, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 100, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 100, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 100, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 100, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 4)            516         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 541,060
Trainable params: 538,500
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4388/4388 - 310s - loss: 0.9679
Epoch 2/100
4388/4388 - 270s - loss: 0.9250
Epoch 3/100
4388/4388 - 269s - loss: 0.8896
Epoch 4/100
4388/4388 - 272s - loss: 0.8424
Epoch 5/100
4388/4388 - 274s - loss: 0.8017
Epoch 6/100
4388/4388 - 283s - loss: 0.7782
Epoch 7/100
4388/4388 - 282s - loss: 0.7534
Epoch 8/100
4388/4388 - 288s - loss: 0.7314
Epoch 9/100
4388/4388 - 296s - loss: 0.7212
Epoch 10/100
4388/4388 - 297s - loss: 0.7069
Epoch 11/100
4388/4388 - 298s - loss: 0.6879
Epoch 12/100
4388/4388 - 297s - loss: 0.6735
Epoch 13/100
4388/4388 - 301s - loss: 0.6573
Epoch 14/100
4388/4388 - 303s - loss: 0.6406
Epoch 15/100
4388/4388 - 300s - loss: 0.6259
Epoch 16/100
4388/4388 - 316s - loss: 0.6070
Epoch 17/100
4388/4388 - 317s - loss: 0.5917
Epoch 18/100
4388/4388 - 321s - loss: 0.5711
Epoch 19/100
4388/4388 - 318s - loss: 0.5523
Epoch 20/100
4388/4388 - 328s - loss: 0.5425
Epoch 21/100
4388/4388 - 332s - loss: 0.5200
Epoch 22/100
4388/4388 - 340s - loss: 0.5047
Epoch 23/100
4388/4388 - 334s - loss: 0.4848
Epoch 24/100
4388/4388 - 338s - loss: 0.4654
Epoch 25/100
4388/4388 - 330s - loss: 0.4539
Epoch 26/100
4388/4388 - 323s - loss: 0.4359
Epoch 27/100
4388/4388 - 322s - loss: 0.4164
Epoch 28/100
4388/4388 - 324s - loss: 0.3976
Epoch 29/100
4388/4388 - 317s - loss: 0.3794
Epoch 30/100
4388/4388 - 321s - loss: 0.3641
Epoch 31/100
4388/4388 - 327s - loss: 0.3475
Epoch 32/100
4388/4388 - 334s - loss: 0.3396
Epoch 33/100
4388/4388 - 337s - loss: 0.3237
Epoch 34/100
4388/4388 - 337s - loss: 0.3129
Epoch 35/100
4388/4388 - 331s - loss: 0.2963
Epoch 36/100
4388/4388 - 333s - loss: 0.2789
Epoch 37/100
4388/4388 - 332s - loss: 0.2769
Epoch 38/100
4388/4388 - 333s - loss: 0.2535
Epoch 39/100
4388/4388 - 327s - loss: 0.2418
Epoch 40/100
4388/4388 - 330s - loss: 0.2334
Epoch 41/100
4388/4388 - 333s - loss: 0.2358
Epoch 42/100
4388/4388 - 333s - loss: 0.2140
Epoch 43/100
4388/4388 - 330s - loss: 0.2041
Epoch 44/100
4388/4388 - 329s - loss: 0.2027
Epoch 45/100
4388/4388 - 331s - loss: 0.1928
Epoch 46/100
4388/4388 - 325s - loss: 0.1853
Epoch 47/100
4388/4388 - 327s - loss: 0.1815
Epoch 48/100
4388/4388 - 325s - loss: 0.1669
Epoch 49/100
4388/4388 - 326s - loss: 0.1670
Epoch 50/100
4388/4388 - 336s - loss: 0.1595
Epoch 51/100
4388/4388 - 339s - loss: 0.1519
Epoch 52/100
4388/4388 - 344s - loss: 0.1529
Epoch 53/100
4388/4388 - 345s - loss: 0.1517
Epoch 54/100
4388/4388 - 343s - loss: 0.1405
Epoch 55/100
4388/4388 - 340s - loss: 0.1353
Epoch 56/100
4388/4388 - 337s - loss: 0.1233
Epoch 57/100
4388/4388 - 341s - loss: 0.1255
Epoch 58/100
4388/4388 - 340s - loss: 0.1243
Epoch 59/100
4388/4388 - 334s - loss: 0.1180
Epoch 60/100
4388/4388 - 320s - loss: 0.1009
Epoch 61/100
4388/4388 - 333s - loss: 0.1144
Epoch 62/100
4388/4388 - 348s - loss: 0.1023
Epoch 63/100
4388/4388 - 336s - loss: 0.1143
Epoch 64/100
4388/4388 - 323s - loss: 0.1098

Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 65/100
4388/4388 - 310s - loss: 0.0438
Epoch 66/100
4388/4388 - 302s - loss: 0.0389
Epoch 67/100
4388/4388 - 303s - loss: 0.0346
Epoch 68/100
4388/4388 - 311s - loss: 0.0274
Epoch 69/100
4388/4388 - 314s - loss: 0.0301
Epoch 70/100
4388/4388 - 308s - loss: 0.0287
Epoch 71/100
4388/4388 - 295s - loss: 0.0243
Epoch 72/100
4388/4388 - 279s - loss: 0.0275
Epoch 73/100
4388/4388 - 292s - loss: 0.0225
Epoch 74/100
4388/4388 - 287s - loss: 0.0229
Epoch 75/100
4388/4388 - 295s - loss: 0.0291
Epoch 76/100
4388/4388 - 283s - loss: 0.0240
Epoch 77/100
4388/4388 - 290s - loss: 0.0172
Epoch 78/100
4388/4388 - 310s - loss: 0.0231
Epoch 79/100
4388/4388 - 317s - loss: 0.0207
Epoch 80/100
4388/4388 - 315s - loss: 0.0250
Epoch 81/100
4388/4388 - 310s - loss: 0.0265

Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 82/100
4388/4388 - 314s - loss: 0.0118
Epoch 83/100
4388/4388 - 315s - loss: 0.0070
Epoch 84/100
4388/4388 - 296s - loss: 0.0075
Epoch 85/100
4388/4388 - 318s - loss: 0.0075
Epoch 86/100
4388/4388 - 314s - loss: 0.0082
Epoch 87/100
4388/4388 - 310s - loss: 0.0067
Epoch 88/100
4388/4388 - 309s - loss: 0.0066
Epoch 89/100
4388/4388 - 294s - loss: 0.0060
Epoch 90/100
4388/4388 - 295s - loss: 0.0059
Epoch 91/100
4388/4388 - 302s - loss: 0.0053
Epoch 92/100
4388/4388 - 289s - loss: 0.0079
Epoch 93/100
4388/4388 - 298s - loss: 0.0054
Epoch 94/100
4388/4388 - 306s - loss: 0.0052
Epoch 95/100
4388/4388 - 306s - loss: 0.0052
Epoch 96/100
4388/4388 - 296s - loss: 0.0060
Epoch 97/100
4388/4388 - 305s - loss: 0.0055
Epoch 98/100
4388/4388 - 318s - loss: 0.0057
Epoch 99/100
4388/4388 - 318s - loss: 0.0047
Epoch 100/100
4388/4388 - 316s - loss: 0.0059
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.25063497086333164
Test f1_avg: 0.24294535475531648
INFO: [guild] Running trial 1ae4598d033444cb89553b89add765bb: ResNet:train (dev=no, dimensionality=60, horizon=1, learning_rate=0.001, n_feature_maps=64, optimizer=adam, window=200)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 33/2042 [00:00<00:06, 321.25it/s]Padding data:   3%|3         | 66/2042 [00:00<00:06, 315.84it/s]Padding data:   5%|4         | 99/2042 [00:00<00:06, 319.69it/s]Padding data:   7%|6         | 133/2042 [00:00<00:05, 326.63it/s]Padding data:   8%|8         | 167/2042 [00:00<00:05, 330.52it/s]Padding data:  10%|9         | 201/2042 [00:00<00:05, 332.89it/s]Padding data:  12%|#1        | 238/2042 [00:00<00:05, 344.09it/s]Padding data:  13%|#3        | 275/2042 [00:00<00:05, 351.47it/s]Padding data:  15%|#5        | 315/2042 [00:00<00:04, 365.71it/s]Padding data:  17%|#7        | 355/2042 [00:01<00:04, 374.24it/s]Padding data:  19%|#9        | 395/2042 [00:01<00:04, 381.22it/s]Padding data:  21%|##1       | 434/2042 [00:01<00:04, 383.02it/s]Padding data:  23%|##3       | 473/2042 [00:01<00:04, 379.73it/s]Padding data:  25%|##5       | 511/2042 [00:01<00:04, 377.87it/s]Padding data:  27%|##6       | 551/2042 [00:01<00:03, 383.61it/s]Padding data:  29%|##8       | 591/2042 [00:01<00:03, 386.50it/s]Padding data:  31%|###       | 630/2042 [00:01<00:03, 386.70it/s]Padding data:  33%|###2      | 669/2042 [00:01<00:03, 385.70it/s]Padding data:  35%|###4      | 708/2042 [00:01<00:03, 382.74it/s]Padding data:  37%|###6      | 747/2042 [00:02<00:03, 382.93it/s]Padding data:  39%|###8      | 788/2042 [00:02<00:03, 387.81it/s]Padding data:  41%|####      | 829/2042 [00:02<00:03, 391.22it/s]Padding data:  43%|####2     | 869/2042 [00:02<00:02, 391.81it/s]Padding data:  45%|####4     | 909/2042 [00:02<00:02, 391.08it/s]Padding data:  47%|####6     | 950/2042 [00:02<00:02, 395.81it/s]Padding data:  48%|####8     | 990/2042 [00:02<00:02, 392.72it/s]Padding data:  50%|#####     | 1030/2042 [00:02<00:02, 384.98it/s]Padding data:  52%|#####2    | 1069/2042 [00:02<00:02, 381.20it/s]Padding data:  54%|#####4    | 1108/2042 [00:02<00:02, 382.95it/s]Padding data:  56%|#####6    | 1147/2042 [00:03<00:02, 384.19it/s]Padding data:  58%|#####8    | 1187/2042 [00:03<00:02, 386.88it/s]Padding data:  60%|######    | 1226/2042 [00:03<00:02, 383.59it/s]Padding data:  62%|######2   | 1267/2042 [00:03<00:01, 388.25it/s]Padding data:  64%|######3   | 1306/2042 [00:03<00:01, 387.93it/s]Padding data:  66%|######5   | 1345/2042 [00:03<00:01, 386.57it/s]Padding data:  68%|######7   | 1385/2042 [00:03<00:01, 388.55it/s]Padding data:  70%|######9   | 1424/2042 [00:03<00:01, 387.00it/s]Padding data:  72%|#######1  | 1463/2042 [00:03<00:01, 383.66it/s]Padding data:  74%|#######3  | 1502/2042 [00:03<00:01, 383.58it/s]Padding data:  76%|#######5  | 1543/2042 [00:04<00:01, 390.54it/s]Padding data:  78%|#######7  | 1584/2042 [00:04<00:01, 395.45it/s]Padding data:  80%|#######9  | 1625/2042 [00:04<00:01, 397.73it/s]Padding data:  82%|########1 | 1665/2042 [00:04<00:00, 390.62it/s]Padding data:  83%|########3 | 1705/2042 [00:04<00:00, 379.23it/s]Padding data:  85%|########5 | 1744/2042 [00:04<00:00, 381.53it/s]Padding data:  87%|########7 | 1783/2042 [00:04<00:00, 379.88it/s]Padding data:  89%|########9 | 1822/2042 [00:04<00:00, 377.61it/s]Padding data:  91%|#########1| 1861/2042 [00:04<00:00, 379.33it/s]Padding data:  93%|#########3| 1900/2042 [00:05<00:00, 381.64it/s]Padding data:  95%|#########4| 1939/2042 [00:05<00:00, 371.31it/s]Padding data:  97%|#########6| 1977/2042 [00:05<00:00, 370.90it/s]Padding data:  99%|#########8| 2016/2042 [00:05<00:00, 374.57it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 377.83it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 200, 60)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 200, 64)      30784       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 200, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 200, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 200, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 200, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 200, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 200, 64)      3904        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 200, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 200, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 200, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 200, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 200, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 200, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 200, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 200, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 200, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 200, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 200, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 200, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 200, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 200, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 200, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 200, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 200, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 200, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 200, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 200, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 200, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 200, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 200, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 200, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 200, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 200, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 200, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 200, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 4)            516         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 541,060
Trainable params: 538,500
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4388/4388 - 496s - loss: 0.9595
Epoch 2/100
4388/4388 - 549s - loss: 0.9061
Epoch 3/100
4388/4388 - 539s - loss: 0.8633
Epoch 4/100
4388/4388 - 527s - loss: 0.8409
Epoch 5/100
4388/4388 - 508s - loss: 0.8125
Epoch 6/100
4388/4388 - 520s - loss: 0.7868
Epoch 7/100
4388/4388 - 506s - loss: 0.7677
Epoch 8/100
4388/4388 - 532s - loss: 0.7473
Epoch 9/100
4388/4388 - 581s - loss: 0.7249
Epoch 10/100
4388/4388 - 589s - loss: 0.7083
Epoch 11/100
4388/4388 - 555s - loss: 0.6909
Epoch 12/100
4388/4388 - 541s - loss: 0.6727
Epoch 13/100
4388/4388 - 582s - loss: 0.6506
Epoch 14/100
4388/4388 - 593s - loss: 0.6337
Epoch 15/100
4388/4388 - 573s - loss: 0.6139
Epoch 16/100
4388/4388 - 560s - loss: 0.5978
Epoch 17/100
4388/4388 - 579s - loss: 0.5702
Epoch 18/100
4388/4388 - 563s - loss: 0.5490
Epoch 19/100
4388/4388 - 546s - loss: 0.5305
Epoch 20/100
4388/4388 - 541s - loss: 0.5137
Epoch 21/100
4388/4388 - 560s - loss: 0.4896
Epoch 22/100
4388/4388 - 554s - loss: 0.4653
Epoch 23/100
4388/4388 - 558s - loss: 0.4531
Epoch 24/100
4388/4388 - 558s - loss: 0.4267
Epoch 25/100
4388/4388 - 556s - loss: 0.4083
Epoch 26/100
4388/4388 - 550s - loss: 0.3899
Epoch 27/100
4388/4388 - 549s - loss: 0.3664
Epoch 28/100
4388/4388 - 571s - loss: 0.3490
Epoch 29/100
4388/4388 - 578s - loss: 0.3329
Epoch 30/100
4388/4388 - 594s - loss: 0.3187
Epoch 31/100
4388/4388 - 579s - loss: 0.2998
Epoch 32/100
4388/4388 - 560s - loss: 0.2844
Epoch 33/100
4388/4388 - 544s - loss: 0.2748
Epoch 34/100
4388/4388 - 550s - loss: 0.2610
Epoch 35/100
4388/4388 - 544s - loss: 0.2386
Epoch 36/100
4388/4388 - 573s - loss: 0.2392
Epoch 37/100
4388/4388 - 556s - loss: 0.2304
Epoch 38/100
4388/4388 - 575s - loss: 0.2186
Epoch 39/100
4388/4388 - 595s - loss: 0.2049
Epoch 40/100
4388/4388 - 593s - loss: 0.1978
Epoch 41/100
4388/4388 - 584s - loss: 0.1931
Epoch 42/100
4388/4388 - 578s - loss: 0.1868
Epoch 43/100
4388/4388 - 553s - loss: 0.1701
Epoch 44/100
4388/4388 - 565s - loss: 0.1684
Epoch 45/100
4388/4388 - 566s - loss: 0.1657
Epoch 46/100
4388/4388 - 576s - loss: 0.1546
Epoch 47/100
4388/4388 - 571s - loss: 0.1440
Epoch 48/100
4388/4388 - 552s - loss: 0.1539
Epoch 49/100
4388/4388 - 554s - loss: 0.1330
Epoch 50/100
4388/4388 - 547s - loss: 0.1374
Epoch 51/100
4388/4388 - 558s - loss: 0.1305
Epoch 52/100
4388/4388 - 556s - loss: 0.1280
Epoch 53/100
4388/4388 - 561s - loss: 0.1187
Epoch 54/100
4388/4388 - 564s - loss: 0.1307
Epoch 55/100
4388/4388 - 575s - loss: 0.1152
Epoch 56/100
4388/4388 - 587s - loss: 0.1121
Epoch 57/100
4388/4388 - 589s - loss: 0.1142
Epoch 58/100
4388/4388 - 576s - loss: 0.1091
Epoch 59/100
4388/4388 - 589s - loss: 0.1022
Epoch 60/100
4388/4388 - 593s - loss: 0.1044
Epoch 61/100
4388/4388 - 589s - loss: 0.0919
Epoch 62/100
4388/4388 - 581s - loss: 0.1018
Epoch 63/100
4388/4388 - 591s - loss: 0.0993
Epoch 64/100
4388/4388 - 595s - loss: 0.0974
Epoch 65/100
4388/4388 - 606s - loss: 0.0987

Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 66/100
4388/4388 - 614s - loss: 0.0464
Epoch 67/100
4388/4388 - 613s - loss: 0.0341
Epoch 68/100
4388/4388 - 616s - loss: 0.0364
Epoch 69/100
4388/4388 - 611s - loss: 0.0310
Epoch 70/100
4388/4388 - 624s - loss: 0.0297
Epoch 71/100
4388/4388 - 618s - loss: 0.0290
Epoch 72/100
4388/4388 - 618s - loss: 0.0283
Epoch 73/100
4388/4388 - 630s - loss: 0.0284
Epoch 74/100
4388/4388 - 629s - loss: 0.0255
Epoch 75/100
4388/4388 - 629s - loss: 0.0272
Epoch 76/100
4388/4388 - 621s - loss: 0.0259
Epoch 77/100
4388/4388 - 628s - loss: 0.0263
Epoch 78/100
4388/4388 - 643s - loss: 0.0220
Epoch 79/100
4388/4388 - 626s - loss: 0.0247
Epoch 80/100
4388/4388 - 625s - loss: 0.0235
Epoch 81/100
4388/4388 - 638s - loss: 0.0211
Epoch 82/100
4388/4388 - 639s - loss: 0.0185
Epoch 83/100
4388/4388 - 634s - loss: 0.0282
Epoch 84/100
4388/4388 - 634s - loss: 0.0214
Epoch 85/100
4388/4388 - 632s - loss: 0.0194
Epoch 86/100
4388/4388 - 629s - loss: 0.0210

Epoch 00086: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 87/100
4388/4388 - 626s - loss: 0.0154
Epoch 88/100
4388/4388 - 631s - loss: 0.0098
Epoch 89/100
4388/4388 - 630s - loss: 0.0090
Epoch 90/100
4388/4388 - 636s - loss: 0.0097
Epoch 91/100
4388/4388 - 645s - loss: 0.0100
Epoch 92/100
4388/4388 - 637s - loss: 0.0091
Epoch 93/100
4388/4388 - 633s - loss: 0.0084
Epoch 94/100
4388/4388 - 636s - loss: 0.0083
Epoch 95/100
4388/4388 - 637s - loss: 0.0086
Epoch 96/100
4388/4388 - 635s - loss: 0.0085
Epoch 97/100
4388/4388 - 645s - loss: 0.0078
Epoch 98/100
4388/4388 - 648s - loss: 0.0081
Epoch 99/100
4388/4388 - 645s - loss: 0.0078
Epoch 100/100
4388/4388 - 645s - loss: 0.0071
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.29399749018881605
Test f1_avg: 0.28918110177960665
INFO: [guild] Running trial e149acccb58c439b8e4c649a87b9ad1b: ResNet:train (dev=no, dimensionality=60, horizon=1, learning_rate=0.001, n_feature_maps=64, optimizer=adam, window=500)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 35/2042 [00:00<00:05, 344.10it/s]Padding data:   3%|3         | 71/2042 [00:00<00:05, 351.89it/s]Padding data:   5%|5         | 108/2042 [00:00<00:05, 357.30it/s]Padding data:   7%|7         | 145/2042 [00:00<00:05, 359.84it/s]Padding data:   9%|8         | 181/2042 [00:00<00:05, 358.96it/s]Padding data:  11%|#         | 217/2042 [00:00<00:05, 349.06it/s]Padding data:  13%|#2        | 260/2042 [00:00<00:04, 373.26it/s]Padding data:  15%|#4        | 298/2042 [00:00<00:04, 373.35it/s]Padding data:  17%|#6        | 339/2042 [00:00<00:04, 381.41it/s]Padding data:  19%|#8        | 378/2042 [00:01<00:04, 379.73it/s]Padding data:  20%|##        | 416/2042 [00:01<00:04, 373.41it/s]Padding data:  22%|##2       | 456/2042 [00:01<00:04, 380.47it/s]Padding data:  24%|##4       | 498/2042 [00:01<00:03, 389.06it/s]Padding data:  26%|##6       | 539/2042 [00:01<00:03, 392.10it/s]Padding data:  28%|##8       | 580/2042 [00:01<00:03, 396.53it/s]Padding data:  30%|###       | 620/2042 [00:01<00:03, 395.53it/s]Padding data:  32%|###2      | 660/2042 [00:01<00:03, 394.84it/s]Padding data:  34%|###4      | 701/2042 [00:01<00:03, 396.13it/s]Padding data:  36%|###6      | 741/2042 [00:01<00:03, 395.26it/s]Padding data:  38%|###8      | 781/2042 [00:02<00:03, 395.80it/s]Padding data:  40%|####      | 822/2042 [00:02<00:03, 399.15it/s]Padding data:  42%|####2     | 862/2042 [00:02<00:02, 397.36it/s]Padding data:  44%|####4     | 902/2042 [00:02<00:02, 397.28it/s]Padding data:  46%|####6     | 943/2042 [00:02<00:02, 397.84it/s]Padding data:  48%|####8     | 984/2042 [00:02<00:02, 399.40it/s]Padding data:  50%|#####     | 1025/2042 [00:02<00:02, 400.49it/s]Padding data:  52%|#####2    | 1066/2042 [00:02<00:02, 402.44it/s]Padding data:  54%|#####4    | 1107/2042 [00:02<00:02, 403.81it/s]Padding data:  56%|#####6    | 1148/2042 [00:02<00:02, 401.21it/s]Padding data:  58%|#####8    | 1189/2042 [00:03<00:02, 401.76it/s]Padding data:  60%|######    | 1230/2042 [00:03<00:02, 400.96it/s]Padding data:  62%|######2   | 1271/2042 [00:03<00:01, 402.76it/s]Padding data:  64%|######4   | 1312/2042 [00:03<00:01, 397.00it/s]Padding data:  66%|######6   | 1352/2042 [00:03<00:01, 397.04it/s]Padding data:  68%|######8   | 1393/2042 [00:03<00:01, 397.66it/s]Padding data:  70%|#######   | 1433/2042 [00:03<00:01, 397.48it/s]Padding data:  72%|#######2  | 1474/2042 [00:03<00:01, 397.98it/s]Padding data:  74%|#######4  | 1514/2042 [00:03<00:01, 393.11it/s]Padding data:  76%|#######6  | 1554/2042 [00:03<00:01, 391.99it/s]Padding data:  78%|#######8  | 1595/2042 [00:04<00:01, 396.44it/s]Padding data:  80%|########  | 1636/2042 [00:04<00:01, 398.41it/s]Padding data:  82%|########2 | 1676/2042 [00:04<00:00, 398.02it/s]Padding data:  84%|########4 | 1716/2042 [00:04<00:00, 397.75it/s]Padding data:  86%|########5 | 1756/2042 [00:04<00:00, 391.76it/s]Padding data:  88%|########8 | 1797/2042 [00:04<00:00, 395.12it/s]Padding data:  90%|########9 | 1837/2042 [00:04<00:00, 395.71it/s]Padding data:  92%|#########1| 1877/2042 [00:04<00:00, 396.12it/s]Padding data:  94%|#########3| 1917/2042 [00:04<00:00, 391.78it/s]Padding data:  96%|#########5| 1957/2042 [00:05<00:00, 388.80it/s]Padding data:  98%|#########7| 1998/2042 [00:05<00:00, 391.88it/s]Padding data: 100%|#########9| 2039/2042 [00:05<00:00, 396.36it/s]Padding data: 100%|##########| 2042/2042 [00:05<00:00, 391.14it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 500, 60)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 500, 64)      30784       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 500, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 500, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 500, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 500, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 500, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 500, 64)      3904        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 500, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 500, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 500, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 500, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 500, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 500, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 500, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 500, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 500, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 500, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 500, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 500, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 500, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 500, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 500, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 500, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 500, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 500, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 500, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 500, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 500, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 500, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 500, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 500, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 500, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 500, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 500, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 500, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 4)            516         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 541,060
Trainable params: 538,500
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4386/4386 - 1238s - loss: 0.9328
Epoch 2/100
4386/4386 - 1348s - loss: 0.8643
Epoch 3/100
4386/4386 - 1332s - loss: 0.8187
Epoch 4/100
4386/4386 - 1350s - loss: 0.7777
Epoch 5/100
4386/4386 - 1335s - loss: 0.7521
Epoch 6/100
4386/4386 - 1348s - loss: 0.7321
Epoch 7/100
4386/4386 - 1358s - loss: 0.7008
Epoch 8/100
4386/4386 - 1349s - loss: 0.6805
Epoch 9/100
4386/4386 - 1348s - loss: 0.6515
Epoch 10/100
4386/4386 - 1357s - loss: 0.6273
Epoch 11/100
4386/4386 - 1354s - loss: 0.6061
Epoch 12/100
4386/4386 - 1358s - loss: 0.5811
Epoch 13/100
