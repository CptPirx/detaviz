INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 38/2042 [00:00<00:05, 377.38it/s]Padding data:   4%|3         | 78/2042 [00:00<00:05, 388.98it/s]Padding data:   6%|5         | 117/2042 [00:00<00:04, 388.16it/s]Padding data:   8%|7         | 156/2042 [00:00<00:04, 387.77it/s]Padding data:  10%|9         | 195/2042 [00:00<00:04, 384.77it/s]Padding data:  12%|#1        | 235/2042 [00:00<00:04, 387.64it/s]Padding data:  14%|#3        | 277/2042 [00:00<00:04, 397.18it/s]Padding data:  16%|#5        | 320/2042 [00:00<00:04, 405.32it/s]Padding data:  18%|#7        | 362/2042 [00:00<00:04, 408.93it/s]Padding data:  20%|#9        | 405/2042 [00:01<00:03, 411.95it/s]Padding data:  22%|##1       | 447/2042 [00:01<00:03, 413.47it/s]Padding data:  24%|##3       | 490/2042 [00:01<00:03, 415.08it/s]Padding data:  26%|##6       | 532/2042 [00:01<00:03, 415.61it/s]Padding data:  28%|##8       | 574/2042 [00:01<00:03, 416.02it/s]Padding data:  30%|###       | 616/2042 [00:01<00:03, 415.05it/s]Padding data:  32%|###2      | 658/2042 [00:01<00:03, 413.18it/s]Padding data:  34%|###4      | 700/2042 [00:01<00:03, 414.31it/s]Padding data:  36%|###6      | 742/2042 [00:01<00:03, 415.09it/s]Padding data:  38%|###8      | 784/2042 [00:01<00:03, 414.46it/s]Padding data:  40%|####      | 827/2042 [00:02<00:02, 416.90it/s]Padding data:  43%|####2     | 869/2042 [00:02<00:02, 416.91it/s]Padding data:  45%|####4     | 911/2042 [00:02<00:02, 416.94it/s]Padding data:  47%|####6     | 953/2042 [00:02<00:02, 415.74it/s]Padding data:  49%|####8     | 995/2042 [00:02<00:02, 413.63it/s]Padding data:  51%|#####     | 1038/2042 [00:02<00:02, 416.21it/s]Padding data:  53%|#####2    | 1080/2042 [00:02<00:02, 415.33it/s]Padding data:  55%|#####4    | 1122/2042 [00:02<00:02, 415.82it/s]Padding data:  57%|#####7    | 1165/2042 [00:02<00:02, 417.89it/s]Padding data:  59%|#####9    | 1207/2042 [00:02<00:01, 417.60it/s]Padding data:  61%|######1   | 1250/2042 [00:03<00:01, 417.90it/s]Padding data:  63%|######3   | 1293/2042 [00:03<00:01, 420.58it/s]Padding data:  65%|######5   | 1336/2042 [00:03<00:01, 420.02it/s]Padding data:  68%|######7   | 1379/2042 [00:03<00:01, 420.79it/s]Padding data:  70%|######9   | 1422/2042 [00:03<00:01, 418.90it/s]Padding data:  72%|#######1  | 1464/2042 [00:03<00:01, 418.31it/s]Padding data:  74%|#######3  | 1506/2042 [00:03<00:01, 417.95it/s]Padding data:  76%|#######5  | 1548/2042 [00:03<00:01, 417.62it/s]Padding data:  78%|#######7  | 1591/2042 [00:03<00:01, 419.14it/s]Padding data:  80%|#######9  | 1633/2042 [00:03<00:00, 418.50it/s]Padding data:  82%|########2 | 1676/2042 [00:04<00:00, 418.52it/s]Padding data:  84%|########4 | 1718/2042 [00:04<00:00, 415.60it/s]Padding data:  86%|########6 | 1760/2042 [00:04<00:00, 416.01it/s]Padding data:  88%|########8 | 1802/2042 [00:04<00:00, 416.28it/s]Padding data:  90%|######### | 1844/2042 [00:04<00:00, 416.48it/s]Padding data:  92%|#########2| 1886/2042 [00:04<00:00, 415.39it/s]Padding data:  94%|#########4| 1929/2042 [00:04<00:00, 417.59it/s]Padding data:  97%|#########6| 1971/2042 [00:04<00:00, 413.73it/s]Padding data:  99%|#########8| 2014/2042 [00:04<00:00, 415.20it/s]Padding data: 100%|##########| 2042/2042 [00:04<00:00, 413.38it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 500, 60)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            60900     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 4)                 251       
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0         
=================================================================
Total params: 68,481
Trainable params: 68,481
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4386/4386 - 162s - loss: 1.0436
Epoch 2/100
4386/4386 - 160s - loss: 0.8210
Epoch 3/100
4386/4386 - 160s - loss: 0.7921
Epoch 4/100
4386/4386 - 160s - loss: 0.7711
Epoch 5/100
4386/4386 - 159s - loss: 0.7596
Epoch 6/100
4386/4386 - 159s - loss: 0.7484
Epoch 7/100
4386/4386 - 160s - loss: 0.7414
Epoch 8/100
4386/4386 - 159s - loss: 0.7358
Epoch 9/100
4386/4386 - 159s - loss: 0.7211
Epoch 10/100
4386/4386 - 159s - loss: 0.7175
Epoch 11/100
4386/4386 - 160s - loss: 0.7097
Epoch 12/100
4386/4386 - 160s - loss: 0.6967
Epoch 13/100
4386/4386 - 160s - loss: 0.6913
Epoch 14/100
4386/4386 - 158s - loss: 0.6839
Epoch 15/100
4386/4386 - 159s - loss: 0.6802
Epoch 16/100
4386/4386 - 159s - loss: 0.6762
Epoch 17/100
4386/4386 - 159s - loss: 0.6722
Epoch 18/100
4386/4386 - 159s - loss: 0.6654
Epoch 19/100
4386/4386 - 160s - loss: 0.6635
Epoch 20/100
4386/4386 - 160s - loss: 0.6616
Epoch 21/100
4386/4386 - 159s - loss: 0.6589
Epoch 22/100
4386/4386 - 159s - loss: 0.6571
Epoch 23/100
4386/4386 - 160s - loss: 0.6527
Epoch 24/100
4386/4386 - 159s - loss: 0.6514
Epoch 25/100
4386/4386 - 159s - loss: 0.6433
Epoch 26/100
4386/4386 - 160s - loss: 0.6409
Epoch 27/100
4386/4386 - 160s - loss: 0.6438
Epoch 28/100
4386/4386 - 160s - loss: 0.6461
Epoch 29/100
4386/4386 - 159s - loss: 0.6447
Epoch 30/100
4386/4386 - 159s - loss: 0.6403
Epoch 31/100
4386/4386 - 160s - loss: 0.6381
Epoch 32/100
4386/4386 - 160s - loss: 0.6386
Epoch 33/100
4386/4386 - 160s - loss: 0.6340
Epoch 34/100
4386/4386 - 159s - loss: 0.6316
Epoch 35/100
4386/4386 - 159s - loss: 0.6327
Epoch 36/100
4386/4386 - 159s - loss: 0.6346
Epoch 37/100
4386/4386 - 160s - loss: 0.6320
Epoch 38/100
4386/4386 - 162s - loss: 0.6280
Epoch 39/100
4386/4386 - 161s - loss: 0.6282
Epoch 40/100
4386/4386 - 160s - loss: 0.6234
Epoch 41/100
4386/4386 - 160s - loss: 0.6264
Epoch 42/100
4386/4386 - 160s - loss: 0.6282
Epoch 43/100
4386/4386 - 160s - loss: 0.6259
Epoch 44/100
4386/4386 - 160s - loss: 0.6208
Epoch 45/100
4386/4386 - 160s - loss: 0.6176
Epoch 46/100
4386/4386 - 160s - loss: 0.6175
Epoch 47/100
4386/4386 - 160s - loss: 0.6175
Epoch 48/100
4386/4386 - 159s - loss: 0.6202
Epoch 49/100
4386/4386 - 160s - loss: 0.6192
Epoch 50/100
4386/4386 - 160s - loss: 0.6164
Epoch 51/100
4386/4386 - 160s - loss: 0.6151
Epoch 52/100
4386/4386 - 160s - loss: 0.6139
Epoch 53/100
4386/4386 - 159s - loss: 0.6148
Epoch 54/100
4386/4386 - 159s - loss: 0.6106
Epoch 55/100
4386/4386 - 160s - loss: 0.6080
Epoch 56/100
4386/4386 - 159s - loss: 0.6105
Epoch 57/100
4386/4386 - 159s - loss: 0.6100
Epoch 58/100
4386/4386 - 159s - loss: 0.6091
Epoch 59/100
4386/4386 - 160s - loss: 0.6093

Epoch 00059: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 60/100
4386/4386 - 160s - loss: 0.5802
Epoch 61/100
4386/4386 - 160s - loss: 0.5758
Epoch 62/100
4386/4386 - 159s - loss: 0.5754
Epoch 63/100
4386/4386 - 160s - loss: 0.5730
Epoch 64/100
4386/4386 - 159s - loss: 0.5757
Epoch 65/100
4386/4386 - 160s - loss: 0.5733
Epoch 66/100
4386/4386 - 159s - loss: 0.5720
Epoch 67/100
4386/4386 - 160s - loss: 0.5700
Epoch 68/100
4386/4386 - 160s - loss: 0.5717
Epoch 69/100
4386/4386 - 159s - loss: 0.5691
Epoch 70/100
4386/4386 - 159s - loss: 0.5713
Epoch 71/100
4386/4386 - 160s - loss: 0.5693
Epoch 72/100
4386/4386 - 160s - loss: 0.5732
Epoch 73/100
4386/4386 - 160s - loss: 0.5687
Epoch 74/100
4386/4386 - 160s - loss: 0.5697
Epoch 75/100
4386/4386 - 160s - loss: 0.5681
Epoch 76/100
4386/4386 - 159s - loss: 0.5682
Epoch 77/100
4386/4386 - 160s - loss: 0.5658
Epoch 78/100
4386/4386 - 159s - loss: 0.5665
Epoch 79/100
4386/4386 - 159s - loss: 0.5658
Epoch 80/100
4386/4386 - 159s - loss: 0.5650
Epoch 81/100
4386/4386 - 159s - loss: 0.5638
Epoch 82/100
4386/4386 - 159s - loss: 0.5657
Epoch 83/100
4386/4386 - 159s - loss: 0.5632
Epoch 84/100
4386/4386 - 159s - loss: 0.5636
Epoch 85/100
4386/4386 - 159s - loss: 0.5605
Epoch 86/100
4386/4386 - 159s - loss: 0.5634
Epoch 87/100
4386/4386 - 159s - loss: 0.5607
Epoch 88/100
4386/4386 - 159s - loss: 0.5634
Epoch 89/100
4386/4386 - 159s - loss: 0.5625

Epoch 00089: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 90/100
4386/4386 - 159s - loss: 0.5472
Epoch 91/100
4386/4386 - 159s - loss: 0.5469
Epoch 92/100
4386/4386 - 160s - loss: 0.5435
Epoch 93/100
4386/4386 - 159s - loss: 0.5430
Epoch 94/100
4386/4386 - 159s - loss: 0.5444
Epoch 95/100
4386/4386 - 160s - loss: 0.5438
Epoch 96/100
4386/4386 - 159s - loss: 0.5427
Epoch 97/100
4386/4386 - 159s - loss: 0.5440
Epoch 98/100
4386/4386 - 159s - loss: 0.5426
Epoch 99/100
4386/4386 - 158s - loss: 0.5432
Epoch 100/100
4386/4386 - 159s - loss: 0.5423
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.6065580566569942
Test f1_avg: 0.5162696206808253
