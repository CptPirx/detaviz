INFO: [guild] Running trial 846818b1b5d244c89a28702604d7ac55: TABL:train (attention_constraint=None, attention_regularizer=None, binarize=yes, dev=no, dimensionality=100, horizon=1, learning_rate=0.001, n_bl_layers=2, n_tabl_layers=1, optimizer=adam, projection_constraint=None, projection_regularizer=None, remote=no, window=100)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 53/2042 [00:00<00:03, 526.58it/s]Padding data:   5%|▌         | 106/2042 [00:00<00:03, 527.96it/s]Padding data:   8%|▊         | 159/2042 [00:00<00:03, 524.79it/s]Padding data:  10%|█         | 212/2042 [00:00<00:03, 526.10it/s]Padding data:  13%|█▎        | 265/2042 [00:00<00:03, 525.49it/s]Padding data:  16%|█▌        | 319/2042 [00:00<00:03, 527.05it/s]Padding data:  18%|█▊        | 372/2042 [00:00<00:03, 527.90it/s]Padding data:  21%|██        | 426/2042 [00:00<00:03, 530.82it/s]Padding data:  24%|██▎       | 480/2042 [00:00<00:02, 533.03it/s]Padding data:  26%|██▌       | 534/2042 [00:01<00:02, 533.80it/s]Padding data:  29%|██▉       | 588/2042 [00:01<00:02, 535.30it/s]Padding data:  31%|███▏      | 643/2042 [00:01<00:02, 537.28it/s]Padding data:  34%|███▍      | 698/2042 [00:01<00:02, 538.85it/s]Padding data:  37%|███▋      | 752/2042 [00:01<00:02, 538.47it/s]Padding data:  40%|███▉      | 807/2042 [00:01<00:02, 539.36it/s]Padding data:  42%|████▏     | 862/2042 [00:01<00:02, 539.94it/s]Padding data:  45%|████▍     | 917/2042 [00:01<00:02, 541.64it/s]Padding data:  48%|████▊     | 972/2042 [00:01<00:01, 541.60it/s]Padding data:  50%|█████     | 1027/2042 [00:01<00:01, 543.65it/s]Padding data:  53%|█████▎    | 1082/2042 [00:02<00:01, 541.46it/s]Padding data:  56%|█████▌    | 1138/2042 [00:02<00:01, 544.46it/s]Padding data:  58%|█████▊    | 1193/2042 [00:02<00:01, 545.87it/s]Padding data:  61%|██████    | 1248/2042 [00:02<00:01, 546.50it/s]Padding data:  64%|██████▍   | 1303/2042 [00:02<00:01, 546.04it/s]Padding data:  67%|██████▋   | 1358/2042 [00:02<00:01, 545.02it/s]Padding data:  69%|██████▉   | 1413/2042 [00:02<00:01, 545.09it/s]Padding data:  72%|███████▏  | 1468/2042 [00:02<00:01, 544.30it/s]Padding data:  75%|███████▍  | 1524/2042 [00:02<00:00, 546.63it/s]Padding data:  77%|███████▋  | 1579/2042 [00:02<00:00, 543.68it/s]Padding data:  80%|████████  | 1634/2042 [00:03<00:00, 545.08it/s]Padding data:  83%|████████▎ | 1690/2042 [00:03<00:00, 547.52it/s]Padding data:  85%|████████▌ | 1745/2042 [00:03<00:00, 546.14it/s]Padding data:  88%|████████▊ | 1800/2042 [00:03<00:00, 545.53it/s]Padding data:  91%|█████████ | 1855/2042 [00:03<00:00, 544.54it/s]Padding data:  94%|█████████▎| 1911/2042 [00:03<00:00, 546.58it/s]Padding data:  96%|█████████▋| 1966/2042 [00:03<00:00, 546.59it/s]Padding data:  99%|█████████▉| 2021/2042 [00:03<00:00, 545.59it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 540.52it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 100)]        0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            13100     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,559
Trainable params: 20,559
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 37s - loss: 0.6377
Epoch 2/100
4383/4383 - 36s - loss: 0.5917
Epoch 3/100
4383/4383 - 36s - loss: 0.5775
Epoch 4/100
4383/4383 - 36s - loss: 0.5665
Epoch 5/100
4383/4383 - 36s - loss: 0.5553
Epoch 6/100
4383/4383 - 36s - loss: 0.5473
Epoch 7/100
4383/4383 - 36s - loss: 0.5412
Epoch 8/100
4383/4383 - 37s - loss: 0.5357
Epoch 9/100
4383/4383 - 36s - loss: 0.5300
Epoch 10/100
4383/4383 - 36s - loss: 0.5277
Epoch 11/100
4383/4383 - 36s - loss: 0.5228
Epoch 12/100
4383/4383 - 36s - loss: 0.5211
Epoch 13/100
4383/4383 - 36s - loss: 0.5171
Epoch 14/100
4383/4383 - 36s - loss: 0.5146
Epoch 15/100
4383/4383 - 36s - loss: 0.5129
Epoch 16/100
4383/4383 - 36s - loss: 0.5108
Epoch 17/100
4383/4383 - 36s - loss: 0.5087
Epoch 18/100
4383/4383 - 36s - loss: 0.5072
Epoch 19/100
4383/4383 - 36s - loss: 0.5030
Epoch 20/100
4383/4383 - 36s - loss: 0.5044
Epoch 21/100
4383/4383 - 36s - loss: 0.5023
Epoch 22/100
4383/4383 - 35s - loss: 0.5024
Epoch 23/100
4383/4383 - 35s - loss: 0.4990
Epoch 24/100
4383/4383 - 36s - loss: 0.4997
Epoch 25/100
4383/4383 - 36s - loss: 0.4986
Epoch 26/100
4383/4383 - 36s - loss: 0.4956
Epoch 27/100
4383/4383 - 36s - loss: 0.4964
Epoch 28/100
4383/4383 - 36s - loss: 0.4941
Epoch 29/100
4383/4383 - 37s - loss: 0.4937
Epoch 30/100
4383/4383 - 36s - loss: 0.4910
Epoch 31/100
4383/4383 - 36s - loss: 0.4914
Epoch 32/100
4383/4383 - 36s - loss: 0.4887
Epoch 33/100
4383/4383 - 36s - loss: 0.4878
Epoch 34/100
4383/4383 - 36s - loss: 0.4902
Epoch 35/100
4383/4383 - 36s - loss: 0.4872
Epoch 36/100
4383/4383 - 36s - loss: 0.4869
Epoch 37/100
4383/4383 - 36s - loss: 0.4872
Epoch 38/100
4383/4383 - 36s - loss: 0.4842
Epoch 39/100
4383/4383 - 36s - loss: 0.4850
Epoch 40/100
4383/4383 - 37s - loss: 0.4858
Epoch 41/100
4383/4383 - 36s - loss: 0.4818
Epoch 42/100
4383/4383 - 36s - loss: 0.4808
Epoch 43/100
4383/4383 - 36s - loss: 0.4805
Epoch 44/100
4383/4383 - 36s - loss: 0.4789
Epoch 45/100
4383/4383 - 36s - loss: 0.4796
Epoch 46/100
4383/4383 - 36s - loss: 0.4773
Epoch 47/100
4383/4383 - 36s - loss: 0.4800
Epoch 48/100
4383/4383 - 36s - loss: 0.4756
Epoch 49/100
4383/4383 - 36s - loss: 0.4756
Epoch 50/100
4383/4383 - 36s - loss: 0.4766
Epoch 51/100
4383/4383 - 37s - loss: 0.4743
Epoch 52/100
4383/4383 - 36s - loss: 0.4736
Epoch 53/100
4383/4383 - 36s - loss: 0.4743
Epoch 54/100
4383/4383 - 36s - loss: 0.4773
Epoch 55/100
4383/4383 - 36s - loss: 0.4738
Epoch 56/100
4383/4383 - 36s - loss: 0.4723
Epoch 57/100
4383/4383 - 36s - loss: 0.4700
Epoch 58/100
4383/4383 - 36s - loss: 0.4717
Epoch 59/100
4383/4383 - 36s - loss: 0.4699
Epoch 60/100
4383/4383 - 36s - loss: 0.4702
Epoch 61/100
4383/4383 - 36s - loss: 0.4699
Epoch 62/100
4383/4383 - 36s - loss: 0.4686
Epoch 63/100
4383/4383 - 36s - loss: 0.4689
Epoch 64/100
4383/4383 - 36s - loss: 0.4683
Epoch 65/100
4383/4383 - 36s - loss: 0.4700
Epoch 66/100
4383/4383 - 36s - loss: 0.4664
Epoch 67/100
4383/4383 - 36s - loss: 0.4694
Epoch 68/100
4383/4383 - 36s - loss: 0.4658
Epoch 69/100
4383/4383 - 36s - loss: 0.4682
Epoch 70/100
4383/4383 - 36s - loss: 0.4674
Epoch 71/100
4383/4383 - 36s - loss: 0.4673
Epoch 72/100
4383/4383 - 36s - loss: 0.4651
Epoch 73/100
4383/4383 - 36s - loss: 0.4659
Epoch 74/100
4383/4383 - 36s - loss: 0.4653
Epoch 75/100
4383/4383 - 36s - loss: 0.4678
Epoch 76/100
4383/4383 - 36s - loss: 0.4646
Epoch 77/100
4383/4383 - 36s - loss: 0.4638
Epoch 78/100
4383/4383 - 36s - loss: 0.4646
Epoch 79/100
4383/4383 - 36s - loss: 0.4621
Epoch 80/100
4383/4383 - 36s - loss: 0.4621
Epoch 81/100
4383/4383 - 36s - loss: 0.4624
Epoch 82/100
4383/4383 - 36s - loss: 0.4638
Epoch 83/100
4383/4383 - 36s - loss: 0.4613
Epoch 84/100
4383/4383 - 36s - loss: 0.4615
Epoch 85/100
4383/4383 - 36s - loss: 0.4616
Epoch 86/100
4383/4383 - 36s - loss: 0.4611
Epoch 87/100
4383/4383 - 36s - loss: 0.4610
Epoch 88/100
4383/4383 - 36s - loss: 0.4609
Epoch 89/100
4383/4383 - 36s - loss: 0.4608
Epoch 90/100
4383/4383 - 36s - loss: 0.4586
Epoch 91/100
4383/4383 - 36s - loss: 0.4611
Epoch 92/100
4383/4383 - 36s - loss: 0.4610
Epoch 93/100
4383/4383 - 36s - loss: 0.4589
Epoch 94/100
4383/4383 - 36s - loss: 0.4579
Epoch 95/100
4383/4383 - 36s - loss: 0.4601
Epoch 96/100
4383/4383 - 36s - loss: 0.4600
Epoch 97/100
4383/4383 - 36s - loss: 0.4578
Epoch 98/100
4383/4383 - 36s - loss: 0.4607

Epoch 00098: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 99/100
4383/4383 - 36s - loss: 0.4435
Epoch 100/100
4383/4383 - 36s - loss: 0.4416
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.701910056659205
Test f1_avg: 0.6629217337821125
INFO: [guild] Running trial 33270cda1cdb41e0a823b298b8f007bb: TABL:train (attention_constraint=None, attention_regularizer=None, binarize=yes, dev=no, dimensionality=80, horizon=1, learning_rate=0.001, n_bl_layers=2, n_tabl_layers=1, optimizer=adam, projection_constraint=None, projection_regularizer=None, remote=no, window=100)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 56/2042 [00:00<00:03, 551.99it/s]Padding data:   6%|▌         | 113/2042 [00:00<00:03, 557.79it/s]Padding data:   8%|▊         | 170/2042 [00:00<00:03, 561.66it/s]Padding data:  11%|█         | 227/2042 [00:00<00:03, 562.56it/s]Padding data:  14%|█▍        | 284/2042 [00:00<00:03, 561.95it/s]Padding data:  17%|█▋        | 342/2042 [00:00<00:02, 567.03it/s]Padding data:  20%|█▉        | 401/2042 [00:00<00:02, 572.40it/s]Padding data:  22%|██▏       | 459/2042 [00:00<00:02, 573.78it/s]Padding data:  25%|██▌       | 518/2042 [00:00<00:02, 576.71it/s]Padding data:  28%|██▊       | 576/2042 [00:01<00:02, 576.49it/s]Padding data:  31%|███       | 635/2042 [00:01<00:02, 578.02it/s]Padding data:  34%|███▍      | 693/2042 [00:01<00:02, 577.52it/s]Padding data:  37%|███▋      | 751/2042 [00:01<00:02, 575.31it/s]Padding data:  40%|███▉      | 809/2042 [00:01<00:02, 575.80it/s]Padding data:  42%|████▏     | 867/2042 [00:01<00:02, 573.81it/s]Padding data:  45%|████▌     | 925/2042 [00:01<00:01, 571.46it/s]Padding data:  48%|████▊     | 983/2042 [00:01<00:01, 572.81it/s]Padding data:  51%|█████     | 1041/2042 [00:01<00:01, 574.57it/s]Padding data:  54%|█████▍    | 1100/2042 [00:01<00:01, 577.01it/s]Padding data:  57%|█████▋    | 1159/2042 [00:02<00:01, 579.31it/s]Padding data:  60%|█████▉    | 1218/2042 [00:02<00:01, 580.09it/s]Padding data:  63%|██████▎   | 1277/2042 [00:02<00:01, 581.11it/s]Padding data:  65%|██████▌   | 1336/2042 [00:02<00:01, 580.34it/s]Padding data:  68%|██████▊   | 1395/2042 [00:02<00:01, 580.88it/s]Padding data:  71%|███████   | 1454/2042 [00:02<00:01, 582.64it/s]Padding data:  74%|███████▍  | 1513/2042 [00:02<00:00, 581.87it/s]Padding data:  77%|███████▋  | 1572/2042 [00:02<00:00, 583.06it/s]Padding data:  80%|███████▉  | 1632/2042 [00:02<00:00, 585.19it/s]Padding data:  83%|████████▎ | 1691/2042 [00:02<00:00, 584.61it/s]Padding data:  86%|████████▌ | 1750/2042 [00:03<00:00, 584.76it/s]Padding data:  89%|████████▊ | 1809/2042 [00:03<00:00, 582.69it/s]Padding data:  91%|█████████▏| 1868/2042 [00:03<00:00, 582.88it/s]Padding data:  94%|█████████▍| 1927/2042 [00:03<00:00, 580.64it/s]Padding data:  97%|█████████▋| 1986/2042 [00:03<00:00, 582.69it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 577.38it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 80)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            13000     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,459
Trainable params: 20,459
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 31s - loss: 0.6405
Epoch 2/100
4383/4383 - 30s - loss: 0.5993
Epoch 3/100
4383/4383 - 30s - loss: 0.5853
Epoch 4/100
4383/4383 - 30s - loss: 0.5799
Epoch 5/100
4383/4383 - 30s - loss: 0.5742
Epoch 6/100
4383/4383 - 31s - loss: 0.5644
Epoch 7/100
4383/4383 - 30s - loss: 0.5548
Epoch 8/100
4383/4383 - 30s - loss: 0.5519
Epoch 9/100
4383/4383 - 30s - loss: 0.5483
Epoch 10/100
4383/4383 - 30s - loss: 0.5451
Epoch 11/100
4383/4383 - 30s - loss: 0.5426
Epoch 12/100
4383/4383 - 30s - loss: 0.5394
Epoch 13/100
4383/4383 - 31s - loss: 0.5363
Epoch 14/100
4383/4383 - 30s - loss: 0.5344
Epoch 15/100
4383/4383 - 30s - loss: 0.5309
Epoch 16/100
4383/4383 - 30s - loss: 0.5259
Epoch 17/100
4383/4383 - 30s - loss: 0.5270
Epoch 18/100
4383/4383 - 30s - loss: 0.5243
Epoch 19/100
4383/4383 - 30s - loss: 0.5193
Epoch 20/100
4383/4383 - 30s - loss: 0.5177
Epoch 21/100
4383/4383 - 30s - loss: 0.5157
Epoch 22/100
4383/4383 - 30s - loss: 0.5135
Epoch 23/100
4383/4383 - 30s - loss: 0.5120
Epoch 24/100
4383/4383 - 31s - loss: 0.5101
Epoch 25/100
4383/4383 - 31s - loss: 0.5066
Epoch 26/100
4383/4383 - 31s - loss: 0.5067
Epoch 27/100
4383/4383 - 31s - loss: 0.5053
Epoch 28/100
4383/4383 - 31s - loss: 0.5039
Epoch 29/100
4383/4383 - 30s - loss: 0.5026
Epoch 30/100
4383/4383 - 30s - loss: 0.5025
Epoch 31/100
4383/4383 - 31s - loss: 0.4993
Epoch 32/100
4383/4383 - 31s - loss: 0.4967
Epoch 33/100
4383/4383 - 31s - loss: 0.4997
Epoch 34/100
4383/4383 - 31s - loss: 0.4965
Epoch 35/100
4383/4383 - 31s - loss: 0.4940
Epoch 36/100
4383/4383 - 31s - loss: 0.4935
Epoch 37/100
4383/4383 - 31s - loss: 0.4898
Epoch 38/100
4383/4383 - 31s - loss: 0.4914
Epoch 39/100
4383/4383 - 31s - loss: 0.4880
Epoch 40/100
4383/4383 - 31s - loss: 0.4895
Epoch 41/100
4383/4383 - 31s - loss: 0.4902
Epoch 42/100
4383/4383 - 31s - loss: 0.4878
Epoch 43/100
4383/4383 - 31s - loss: 0.4846
Epoch 44/100
4383/4383 - 31s - loss: 0.4835
Epoch 45/100
4383/4383 - 31s - loss: 0.4833
Epoch 46/100
4383/4383 - 31s - loss: 0.4832
Epoch 47/100
4383/4383 - 31s - loss: 0.4834
Epoch 48/100
4383/4383 - 31s - loss: 0.4834
Epoch 49/100
4383/4383 - 31s - loss: 0.4809
Epoch 50/100
4383/4383 - 30s - loss: 0.4781
Epoch 51/100
4383/4383 - 30s - loss: 0.4821
Epoch 52/100
4383/4383 - 31s - loss: 0.4803
Epoch 53/100
4383/4383 - 30s - loss: 0.4807
Epoch 54/100
4383/4383 - 31s - loss: 0.4780

Epoch 00054: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 55/100
4383/4383 - 30s - loss: 0.4648
Epoch 56/100
4383/4383 - 31s - loss: 0.4622
Epoch 57/100
4383/4383 - 31s - loss: 0.4600
Epoch 58/100
4383/4383 - 31s - loss: 0.4599
Epoch 59/100
4383/4383 - 31s - loss: 0.4604
Epoch 60/100
4383/4383 - 31s - loss: 0.4587
Epoch 61/100
4383/4383 - 31s - loss: 0.4598
Epoch 62/100
4383/4383 - 30s - loss: 0.4583
Epoch 63/100
4383/4383 - 30s - loss: 0.4593
Epoch 64/100
4383/4383 - 30s - loss: 0.4578
Epoch 65/100
4383/4383 - 31s - loss: 0.4579
Epoch 66/100
4383/4383 - 31s - loss: 0.4569
Epoch 67/100
4383/4383 - 31s - loss: 0.4568
Epoch 68/100
4383/4383 - 31s - loss: 0.4561
Epoch 69/100
4383/4383 - 31s - loss: 0.4567
Epoch 70/100
4383/4383 - 31s - loss: 0.4553
Epoch 71/100
4383/4383 - 31s - loss: 0.4541
Epoch 72/100
4383/4383 - 30s - loss: 0.4558
Epoch 73/100
4383/4383 - 31s - loss: 0.4540
Epoch 74/100
4383/4383 - 31s - loss: 0.4551
Epoch 75/100
4383/4383 - 31s - loss: 0.4544
Epoch 76/100
4383/4383 - 31s - loss: 0.4540
Epoch 77/100
4383/4383 - 30s - loss: 0.4533
Epoch 78/100
4383/4383 - 30s - loss: 0.4538
Epoch 79/100
4383/4383 - 30s - loss: 0.4527
Epoch 80/100
4383/4383 - 30s - loss: 0.4521
Epoch 81/100
4383/4383 - 30s - loss: 0.4522
Epoch 82/100
4383/4383 - 30s - loss: 0.4519
Epoch 83/100
4383/4383 - 30s - loss: 0.4535
Epoch 84/100
4383/4383 - 30s - loss: 0.4500
Epoch 85/100
4383/4383 - 31s - loss: 0.4512
Epoch 86/100
4383/4383 - 31s - loss: 0.4529
Epoch 87/100
4383/4383 - 31s - loss: 0.4511
Epoch 88/100
4383/4383 - 31s - loss: 0.4512

Epoch 00088: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 89/100
4383/4383 - 31s - loss: 0.4427
Epoch 90/100
4383/4383 - 31s - loss: 0.4421
Epoch 91/100
4383/4383 - 31s - loss: 0.4417
Epoch 92/100
4383/4383 - 31s - loss: 0.4414
Epoch 93/100
4383/4383 - 31s - loss: 0.4409
Epoch 94/100
4383/4383 - 30s - loss: 0.4398
Epoch 95/100
4383/4383 - 31s - loss: 0.4411
Epoch 96/100
4383/4383 - 31s - loss: 0.4400
Epoch 97/100
4383/4383 - 30s - loss: 0.4396
Epoch 98/100
4383/4383 - 31s - loss: 0.4397
Epoch 99/100
4383/4383 - 31s - loss: 0.4408
Epoch 100/100
4383/4383 - 31s - loss: 0.4387
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.7228534326001341
Test f1_avg: 0.6827159618862798
INFO: [guild] Running trial b3a4d23a97084a009aa9f03be6d386ef: TABL:train (attention_constraint=None, attention_regularizer=None, binarize=yes, dev=no, dimensionality=40, horizon=1, learning_rate=0.001, n_bl_layers=2, n_tabl_layers=1, optimizer=adam, projection_constraint=None, projection_regularizer=None, remote=no, window=100)
INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   3%|▎         | 66/2042 [00:00<00:03, 655.30it/s]Padding data:   6%|▋         | 132/2042 [00:00<00:02, 657.20it/s]Padding data:  10%|▉         | 198/2042 [00:00<00:02, 654.83it/s]Padding data:  13%|█▎        | 265/2042 [00:00<00:02, 657.52it/s]Padding data:  16%|█▌        | 331/2042 [00:00<00:02, 657.18it/s]Padding data:  20%|█▉        | 400/2042 [00:00<00:02, 664.92it/s]Padding data:  23%|██▎       | 467/2042 [00:00<00:02, 666.00it/s]Padding data:  26%|██▌       | 534/2042 [00:00<00:02, 666.38it/s]Padding data:  29%|██▉       | 601/2042 [00:00<00:02, 665.08it/s]Padding data:  33%|███▎      | 668/2042 [00:01<00:02, 664.14it/s]Padding data:  36%|███▌      | 735/2042 [00:01<00:01, 661.82it/s]Padding data:  39%|███▉      | 802/2042 [00:01<00:01, 661.74it/s]Padding data:  43%|████▎     | 869/2042 [00:01<00:01, 661.65it/s]Padding data:  46%|████▌     | 938/2042 [00:01<00:01, 667.27it/s]Padding data:  49%|████▉     | 1007/2042 [00:01<00:01, 672.62it/s]Padding data:  53%|█████▎    | 1076/2042 [00:01<00:01, 676.10it/s]Padding data:  56%|█████▌    | 1144/2042 [00:01<00:01, 669.73it/s]Padding data:  59%|█████▉    | 1212/2042 [00:01<00:01, 670.51it/s]Padding data:  63%|██████▎   | 1280/2042 [00:01<00:01, 670.26it/s]Padding data:  66%|██████▌   | 1348/2042 [00:02<00:01, 667.37it/s]Padding data:  69%|██████▉   | 1416/2042 [00:02<00:00, 670.47it/s]Padding data:  73%|███████▎  | 1484/2042 [00:02<00:00, 672.70it/s]Padding data:  76%|███████▌  | 1553/2042 [00:02<00:00, 675.28it/s]Padding data:  79%|███████▉  | 1622/2042 [00:02<00:00, 677.24it/s]Padding data:  83%|████████▎ | 1692/2042 [00:02<00:00, 682.25it/s]Padding data:  86%|████████▌ | 1761/2042 [00:02<00:00, 673.78it/s]Padding data:  90%|████████▉ | 1829/2042 [00:02<00:00, 674.25it/s]Padding data:  93%|█████████▎| 1898/2042 [00:02<00:00, 676.27it/s]Padding data:  96%|█████████▋| 1966/2042 [00:02<00:00, 675.93it/s]Padding data: 100%|█████████▉| 2034/2042 [00:03<00:00, 676.29it/s]Padding data: 100%|██████████| 2042/2042 [00:03<00:00, 669.14it/s]
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 100, 40)]         0         
_________________________________________________________________
bl_1 (BL)                    (None, 120, 5)            12800     
_________________________________________________________________
activation_1 (Activation)    (None, 120, 5)            0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 120, 5)            0         
_________________________________________________________________
bl_2 (BL)                    (None, 60, 2)             7330      
_________________________________________________________________
activation_2 (Activation)    (None, 60, 2)             0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 60, 2)             0         
_________________________________________________________________
tabl (TABL)                  (None, 2)                 129       
_________________________________________________________________
activation_3 (Activation)    (None, 2)                 0         
=================================================================
Total params: 20,259
Trainable params: 20,259
Non-trainable params: 0
_________________________________________________________________
Epoch 1/100
4383/4383 - 21s - loss: 0.6643
Epoch 2/100
4383/4383 - 20s - loss: 0.6103
Epoch 3/100
4383/4383 - 20s - loss: 0.5995
Epoch 4/100
4383/4383 - 20s - loss: 0.5950
Epoch 5/100
4383/4383 - 20s - loss: 0.5921
Epoch 6/100
4383/4383 - 20s - loss: 0.5888
Epoch 7/100
4383/4383 - 20s - loss: 0.5846
Epoch 8/100
4383/4383 - 20s - loss: 0.5825
Epoch 9/100
4383/4383 - 20s - loss: 0.5786
Epoch 10/100
4383/4383 - 20s - loss: 0.5783
Epoch 11/100
4383/4383 - 20s - loss: 0.5757
Epoch 12/100
4383/4383 - 20s - loss: 0.5743
Epoch 13/100
4383/4383 - 20s - loss: 0.5749
Epoch 14/100
4383/4383 - 20s - loss: 0.5730
Epoch 15/100
4383/4383 - 20s - loss: 0.5687
Epoch 16/100
4383/4383 - 20s - loss: 0.5676
Epoch 17/100
4383/4383 - 20s - loss: 0.5668
Epoch 18/100
4383/4383 - 20s - loss: 0.5617
Epoch 19/100
4383/4383 - 20s - loss: 0.5605
Epoch 20/100
4383/4383 - 20s - loss: 0.5592
Epoch 21/100
4383/4383 - 20s - loss: 0.5557
Epoch 22/100
4383/4383 - 20s - loss: 0.5550
Epoch 23/100
4383/4383 - 20s - loss: 0.5534
Epoch 24/100
4383/4383 - 20s - loss: 0.5520
Epoch 25/100
4383/4383 - 20s - loss: 0.5522
Epoch 26/100
4383/4383 - 20s - loss: 0.5504
Epoch 27/100
4383/4383 - 20s - loss: 0.5489
Epoch 28/100
4383/4383 - 20s - loss: 0.5474
Epoch 29/100
4383/4383 - 20s - loss: 0.5475
Epoch 30/100
4383/4383 - 20s - loss: 0.5472
Epoch 31/100
4383/4383 - 20s - loss: 0.5465
Epoch 32/100
4383/4383 - 20s - loss: 0.5453
Epoch 33/100
4383/4383 - 20s - loss: 0.5434
Epoch 34/100
4383/4383 - 20s - loss: 0.5443
Epoch 35/100
4383/4383 - 20s - loss: 0.5453
Epoch 36/100
4383/4383 - 21s - loss: 0.5423
Epoch 37/100
4383/4383 - 20s - loss: 0.5453
Epoch 38/100
4383/4383 - 20s - loss: 0.5422
Epoch 39/100
4383/4383 - 20s - loss: 0.5421
Epoch 40/100
4383/4383 - 20s - loss: 0.5403
Epoch 41/100
4383/4383 - 20s - loss: 0.5404
Epoch 42/100
4383/4383 - 20s - loss: 0.5385
Epoch 43/100
4383/4383 - 20s - loss: 0.5392
Epoch 44/100
4383/4383 - 20s - loss: 0.5403
Epoch 45/100
4383/4383 - 20s - loss: 0.5387
Epoch 46/100
4383/4383 - 20s - loss: 0.5375
Epoch 47/100
4383/4383 - 20s - loss: 0.5352
Epoch 48/100
4383/4383 - 20s - loss: 0.5395
Epoch 49/100
4383/4383 - 20s - loss: 0.5357
Epoch 50/100
4383/4383 - 20s - loss: 0.5362
Epoch 51/100
4383/4383 - 20s - loss: 0.5362

Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 52/100
4383/4383 - 20s - loss: 0.5238
Epoch 53/100
4383/4383 - 20s - loss: 0.5232
Epoch 54/100
4383/4383 - 20s - loss: 0.5222
Epoch 55/100
4383/4383 - 20s - loss: 0.5225
Epoch 56/100
4383/4383 - 20s - loss: 0.5207
Epoch 57/100
4383/4383 - 20s - loss: 0.5215
Epoch 58/100
4383/4383 - 20s - loss: 0.5210
Epoch 59/100
4383/4383 - 20s - loss: 0.5202
Epoch 60/100
4383/4383 - 20s - loss: 0.5196
Epoch 61/100
4383/4383 - 20s - loss: 0.5180
Epoch 62/100
4383/4383 - 20s - loss: 0.5187
Epoch 63/100
4383/4383 - 20s - loss: 0.5191
Epoch 64/100
4383/4383 - 20s - loss: 0.5176
Epoch 65/100
4383/4383 - 20s - loss: 0.5173
Epoch 66/100
4383/4383 - 20s - loss: 0.5168
Epoch 67/100
4383/4383 - 20s - loss: 0.5153
Epoch 68/100
4383/4383 - 20s - loss: 0.5153
Epoch 69/100
4383/4383 - 20s - loss: 0.5153
Epoch 70/100
4383/4383 - 20s - loss: 0.5155
Epoch 71/100
4383/4383 - 20s - loss: 0.5154

Epoch 00071: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 72/100
4383/4383 - 20s - loss: 0.5074
Epoch 73/100
4383/4383 - 20s - loss: 0.5066
Epoch 74/100
4383/4383 - 22s - loss: 0.5072
Epoch 75/100
4383/4383 - 20s - loss: 0.5048
Epoch 76/100
4383/4383 - 20s - loss: 0.5048
Epoch 77/100
4383/4383 - 20s - loss: 0.5060
Epoch 78/100
4383/4383 - 20s - loss: 0.5050
Epoch 79/100
4383/4383 - 20s - loss: 0.5036
Epoch 80/100
4383/4383 - 20s - loss: 0.5035
Epoch 81/100
4383/4383 - 20s - loss: 0.5034
Epoch 82/100
4383/4383 - 20s - loss: 0.5037
Epoch 83/100
4383/4383 - 20s - loss: 0.5043
Epoch 84/100
4383/4383 - 20s - loss: 0.5033
Epoch 85/100
4383/4383 - 20s - loss: 0.5031
Epoch 86/100
4383/4383 - 20s - loss: 0.5026
Epoch 87/100
4383/4383 - 21s - loss: 0.5035
Epoch 88/100
4383/4383 - 21s - loss: 0.5030
Epoch 89/100
4383/4383 - 20s - loss: 0.5022
Epoch 90/100
4383/4383 - 22s - loss: 0.5026
Epoch 91/100
4383/4383 - 20s - loss: 0.5018
Epoch 92/100
4383/4383 - 20s - loss: 0.5003
Epoch 93/100
4383/4383 - 20s - loss: 0.5003
Epoch 94/100
4383/4383 - 20s - loss: 0.5007
Epoch 95/100
4383/4383 - 20s - loss: 0.5009
Epoch 96/100
4383/4383 - 20s - loss: 0.5010

Epoch 00096: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.
Epoch 97/100
4383/4383 - 20s - loss: 0.4963
Epoch 98/100
4383/4383 - 20s - loss: 0.4962
Epoch 99/100
4383/4383 - 20s - loss: 0.4963
Epoch 100/100
4383/4383 - 20s - loss: 0.4951
INFO: [tensorflow] Assets written to: model/assets
Train f1_avg: 0.6787192896725567
Test f1_avg: 0.6474772573283911
