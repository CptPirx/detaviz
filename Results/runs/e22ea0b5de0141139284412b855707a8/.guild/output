INFO: [numexpr.utils] NumExpr defaulting to 8 threads.
Loaded data
Filtering samples
There are 2042 samples in total.
The types and counts of different labels : 
 {0: 1420, 1: 221, 2: 183, 3: 218}
The types and counts of different labels as percentage of the total data : 
 {0: 0.7, 1: 0.11, 2: 0.09, 3: 0.11}
Reducing dimensionality
Padding data:   0%|          | 0/2042 [00:00<?, ?it/s]Padding data:   2%|1         | 38/2042 [00:00<00:05, 377.24it/s]Padding data:   4%|3         | 76/2042 [00:00<00:05, 377.24it/s]Padding data:   6%|5         | 116/2042 [00:00<00:04, 386.31it/s]Padding data:   8%|7         | 156/2042 [00:00<00:04, 388.17it/s]Padding data:  10%|9         | 197/2042 [00:00<00:04, 393.55it/s]Padding data:  12%|#1        | 238/2042 [00:00<00:04, 396.78it/s]Padding data:  14%|#3        | 280/2042 [00:00<00:04, 403.34it/s]Padding data:  16%|#5        | 323/2042 [00:00<00:04, 408.24it/s]Padding data:  18%|#7        | 364/2042 [00:00<00:04, 408.48it/s]Padding data:  20%|#9        | 407/2042 [00:01<00:03, 411.63it/s]Padding data:  22%|##2       | 450/2042 [00:01<00:03, 415.02it/s]Padding data:  24%|##4       | 492/2042 [00:01<00:03, 414.36it/s]Padding data:  26%|##6       | 534/2042 [00:01<00:03, 415.14it/s]Padding data:  28%|##8       | 577/2042 [00:01<00:03, 417.43it/s]Padding data:  30%|###       | 619/2042 [00:01<00:03, 414.82it/s]Padding data:  32%|###2      | 661/2042 [00:01<00:03, 413.02it/s]Padding data:  34%|###4      | 703/2042 [00:01<00:03, 411.76it/s]Padding data:  36%|###6      | 745/2042 [00:01<00:03, 411.48it/s]Padding data:  39%|###8      | 788/2042 [00:01<00:03, 413.62it/s]Padding data:  41%|####      | 830/2042 [00:02<00:02, 414.60it/s]Padding data:  43%|####2     | 872/2042 [00:02<00:02, 415.30it/s]Padding data:  45%|####4     | 914/2042 [00:02<00:02, 415.79it/s]Padding data:  47%|####6     | 956/2042 [00:02<00:02, 416.14it/s]Padding data:  49%|####8     | 998/2042 [00:02<00:02, 415.15it/s]Padding data:  51%|#####     | 1040/2042 [00:02<00:02, 413.25it/s]Padding data:  53%|#####2    | 1082/2042 [00:02<00:02, 413.13it/s]Padding data:  55%|#####5    | 1124/2042 [00:02<00:02, 413.05it/s]Padding data:  57%|#####7    | 1166/2042 [00:02<00:02, 414.22it/s]Padding data:  59%|#####9    | 1209/2042 [00:02<00:01, 416.76it/s]Padding data:  61%|######1   | 1251/2042 [00:03<00:01, 414.37it/s]Padding data:  63%|######3   | 1293/2042 [00:03<00:01, 412.71it/s]Padding data:  65%|######5   | 1335/2042 [00:03<00:01, 409.16it/s]Padding data:  67%|######7   | 1376/2042 [00:03<00:01, 401.48it/s]Padding data:  69%|######9   | 1418/2042 [00:03<00:01, 403.67it/s]Padding data:  71%|#######1  | 1459/2042 [00:03<00:01, 403.48it/s]Padding data:  74%|#######3  | 1501/2042 [00:03<00:01, 406.27it/s]Padding data:  76%|#######5  | 1543/2042 [00:03<00:01, 408.24it/s]Padding data:  78%|#######7  | 1585/2042 [00:03<00:01, 410.82it/s]Padding data:  80%|#######9  | 1627/2042 [00:03<00:01, 412.65it/s]Padding data:  82%|########1 | 1670/2042 [00:04<00:00, 415.66it/s]Padding data:  84%|########3 | 1712/2042 [00:04<00:00, 416.05it/s]Padding data:  86%|########5 | 1754/2042 [00:04<00:00, 416.32it/s]Padding data:  88%|########7 | 1796/2042 [00:04<00:00, 415.27it/s]Padding data:  90%|######### | 1838/2042 [00:04<00:00, 413.34it/s]Padding data:  92%|#########2| 1880/2042 [00:04<00:00, 415.00it/s]Padding data:  94%|#########4| 1922/2042 [00:04<00:00, 415.58it/s]Padding data:  96%|#########6| 1964/2042 [00:04<00:00, 414.76it/s]Padding data:  98%|#########8| 2006/2042 [00:04<00:00, 415.42it/s]Padding data: 100%|##########| 2042/2042 [00:04<00:00, 410.63it/s]
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 100, 60)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, 100, 64)      30784       input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 100, 64)      256         conv1d[0][0]                     
__________________________________________________________________________________________________
activation (Activation)         (None, 100, 64)      0           batch_normalization[0][0]        
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, 100, 64)      20544       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 100, 64)      256         conv1d_1[0][0]                   
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 100, 64)      0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, 100, 64)      3904        input_1[0][0]                    
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, 100, 64)      12352       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 100, 64)      256         conv1d_3[0][0]                   
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 100, 64)      256         conv1d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 100, 64)      0           batch_normalization_3[0][0]      
                                                                 batch_normalization_2[0][0]      
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 100, 64)      0           add[0][0]                        
__________________________________________________________________________________________________
conv1d_4 (Conv1D)               (None, 100, 128)     65664       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 100, 128)     512         conv1d_4[0][0]                   
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 100, 128)     0           batch_normalization_4[0][0]      
__________________________________________________________________________________________________
conv1d_5 (Conv1D)               (None, 100, 128)     82048       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 100, 128)     512         conv1d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 100, 128)     0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv1d_7 (Conv1D)               (None, 100, 128)     8320        activation_2[0][0]               
__________________________________________________________________________________________________
conv1d_6 (Conv1D)               (None, 100, 128)     49280       activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 100, 128)     512         conv1d_7[0][0]                   
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 100, 128)     512         conv1d_6[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 100, 128)     0           batch_normalization_7[0][0]      
                                                                 batch_normalization_6[0][0]      
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 100, 128)     0           add_1[0][0]                      
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 100, 128)     131200      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 100, 128)     512         conv1d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 100, 128)     0           batch_normalization_8[0][0]      
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 100, 128)     82048       activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 100, 128)     512         conv1d_9[0][0]                   
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 100, 128)     0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 100, 128)     49280       activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 100, 128)     512         activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 100, 128)     512         conv1d_10[0][0]                  
__________________________________________________________________________________________________
add_2 (Add)                     (None, 100, 128)     0           batch_normalization_11[0][0]     
                                                                 batch_normalization_10[0][0]     
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 100, 128)     0           add_2[0][0]                      
__________________________________________________________________________________________________
global_average_pooling1d (Globa (None, 128)          0           activation_8[0][0]               
__________________________________________________________________________________________________
dense (Dense)                   (None, 4)            516         global_average_pooling1d[0][0]   
==================================================================================================
Total params: 541,060
Trainable params: 538,500
Non-trainable params: 2,560
__________________________________________________________________________________________________
Epoch 1/100
4388/4388 - 310s - loss: 0.9679
Epoch 2/100
4388/4388 - 270s - loss: 0.9250
Epoch 3/100
4388/4388 - 269s - loss: 0.8896
Epoch 4/100
4388/4388 - 272s - loss: 0.8424
Epoch 5/100
4388/4388 - 274s - loss: 0.8017
Epoch 6/100
4388/4388 - 283s - loss: 0.7782
Epoch 7/100
4388/4388 - 282s - loss: 0.7534
Epoch 8/100
4388/4388 - 288s - loss: 0.7314
Epoch 9/100
4388/4388 - 296s - loss: 0.7212
Epoch 10/100
4388/4388 - 297s - loss: 0.7069
Epoch 11/100
4388/4388 - 298s - loss: 0.6879
Epoch 12/100
4388/4388 - 297s - loss: 0.6735
Epoch 13/100
4388/4388 - 301s - loss: 0.6573
Epoch 14/100
4388/4388 - 303s - loss: 0.6406
Epoch 15/100
4388/4388 - 300s - loss: 0.6259
Epoch 16/100
4388/4388 - 316s - loss: 0.6070
Epoch 17/100
4388/4388 - 317s - loss: 0.5917
Epoch 18/100
4388/4388 - 321s - loss: 0.5711
Epoch 19/100
4388/4388 - 318s - loss: 0.5523
Epoch 20/100
4388/4388 - 328s - loss: 0.5425
Epoch 21/100
4388/4388 - 332s - loss: 0.5200
Epoch 22/100
4388/4388 - 340s - loss: 0.5047
Epoch 23/100
4388/4388 - 334s - loss: 0.4848
Epoch 24/100
4388/4388 - 338s - loss: 0.4654
Epoch 25/100
4388/4388 - 330s - loss: 0.4539
Epoch 26/100
4388/4388 - 323s - loss: 0.4359
Epoch 27/100
4388/4388 - 322s - loss: 0.4164
Epoch 28/100
4388/4388 - 324s - loss: 0.3976
Epoch 29/100
4388/4388 - 317s - loss: 0.3794
Epoch 30/100
4388/4388 - 321s - loss: 0.3641
Epoch 31/100
4388/4388 - 327s - loss: 0.3475
Epoch 32/100
4388/4388 - 334s - loss: 0.3396
Epoch 33/100
4388/4388 - 337s - loss: 0.3237
Epoch 34/100
4388/4388 - 337s - loss: 0.3129
Epoch 35/100
4388/4388 - 331s - loss: 0.2963
Epoch 36/100
4388/4388 - 333s - loss: 0.2789
Epoch 37/100
4388/4388 - 332s - loss: 0.2769
Epoch 38/100
4388/4388 - 333s - loss: 0.2535
Epoch 39/100
4388/4388 - 327s - loss: 0.2418
Epoch 40/100
4388/4388 - 330s - loss: 0.2334
Epoch 41/100
4388/4388 - 333s - loss: 0.2358
Epoch 42/100
4388/4388 - 333s - loss: 0.2140
Epoch 43/100
4388/4388 - 330s - loss: 0.2041
Epoch 44/100
4388/4388 - 329s - loss: 0.2027
Epoch 45/100
4388/4388 - 331s - loss: 0.1928
Epoch 46/100
4388/4388 - 325s - loss: 0.1853
Epoch 47/100
4388/4388 - 327s - loss: 0.1815
Epoch 48/100
4388/4388 - 325s - loss: 0.1669
Epoch 49/100
4388/4388 - 326s - loss: 0.1670
Epoch 50/100
4388/4388 - 336s - loss: 0.1595
Epoch 51/100
4388/4388 - 339s - loss: 0.1519
Epoch 52/100
4388/4388 - 344s - loss: 0.1529
Epoch 53/100
4388/4388 - 345s - loss: 0.1517
Epoch 54/100
4388/4388 - 343s - loss: 0.1405
Epoch 55/100
4388/4388 - 340s - loss: 0.1353
Epoch 56/100
4388/4388 - 337s - loss: 0.1233
Epoch 57/100
4388/4388 - 341s - loss: 0.1255
Epoch 58/100
4388/4388 - 340s - loss: 0.1243
Epoch 59/100
4388/4388 - 334s - loss: 0.1180
Epoch 60/100
4388/4388 - 320s - loss: 0.1009
Epoch 61/100
4388/4388 - 333s - loss: 0.1144
Epoch 62/100
4388/4388 - 348s - loss: 0.1023
Epoch 63/100
4388/4388 - 336s - loss: 0.1143
Epoch 64/100
4388/4388 - 323s - loss: 0.1098

Epoch 00064: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
Epoch 65/100
4388/4388 - 310s - loss: 0.0438
Epoch 66/100
4388/4388 - 302s - loss: 0.0389
Epoch 67/100
4388/4388 - 303s - loss: 0.0346
Epoch 68/100
4388/4388 - 311s - loss: 0.0274
Epoch 69/100
4388/4388 - 314s - loss: 0.0301
Epoch 70/100
4388/4388 - 308s - loss: 0.0287
Epoch 71/100
4388/4388 - 295s - loss: 0.0243
Epoch 72/100
4388/4388 - 279s - loss: 0.0275
Epoch 73/100
4388/4388 - 292s - loss: 0.0225
Epoch 74/100
4388/4388 - 287s - loss: 0.0229
Epoch 75/100
4388/4388 - 295s - loss: 0.0291
Epoch 76/100
4388/4388 - 283s - loss: 0.0240
Epoch 77/100
4388/4388 - 290s - loss: 0.0172
Epoch 78/100
4388/4388 - 310s - loss: 0.0231
Epoch 79/100
4388/4388 - 317s - loss: 0.0207
Epoch 80/100
4388/4388 - 315s - loss: 0.0250
Epoch 81/100
4388/4388 - 310s - loss: 0.0265

Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
Epoch 82/100
4388/4388 - 314s - loss: 0.0118
Epoch 83/100
4388/4388 - 315s - loss: 0.0070
Epoch 84/100
4388/4388 - 296s - loss: 0.0075
Epoch 85/100
4388/4388 - 318s - loss: 0.0075
Epoch 86/100
4388/4388 - 314s - loss: 0.0082
Epoch 87/100
4388/4388 - 310s - loss: 0.0067
Epoch 88/100
4388/4388 - 309s - loss: 0.0066
Epoch 89/100
4388/4388 - 294s - loss: 0.0060
Epoch 90/100
4388/4388 - 295s - loss: 0.0059
Epoch 91/100
4388/4388 - 302s - loss: 0.0053
Epoch 92/100
4388/4388 - 289s - loss: 0.0079
Epoch 93/100
4388/4388 - 298s - loss: 0.0054
Epoch 94/100
4388/4388 - 306s - loss: 0.0052
Epoch 95/100
4388/4388 - 306s - loss: 0.0052
Epoch 96/100
4388/4388 - 296s - loss: 0.0060
Epoch 97/100
4388/4388 - 305s - loss: 0.0055
Epoch 98/100
4388/4388 - 318s - loss: 0.0057
Epoch 99/100
4388/4388 - 318s - loss: 0.0047
Epoch 100/100
4388/4388 - 316s - loss: 0.0059
INFO: [tensorflow] Assets written to: model\assets
Train f1_avg: 0.25063497086333164
Test f1_avg: 0.24294535475531648
